{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN only, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# Helper functions\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'), nn.BatchNorm1d(32), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'), nn.BatchNorm1d(64), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'), nn.BatchNorm1d(128), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim//8)*128, 1024), nn.LeakyReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256), nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "# Step 1: Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Step 2: k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Step 2: Filter sequences by genus frequency\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Main Experiment Loop\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    data_by_class = defaultdict(list)\n",
    "    with open(filtered_vector_file, 'r') as f:\n",
    "        for line in f:\n",
    "            label = line.split('\\t')[0][1:]\n",
    "            data_by_class[label].append(line.strip())\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(len(train_vectors[0]), len(label_encoder.classes_)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# Helper functions\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# New ResidualBlock and CNNModel\n",
    "# ----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=7, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.downsample = downsample  # To match dimensions if needed\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_length, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.in_channels = 32  # Reduced from 64\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.dropout = nn.Dropout(0.3)  # Add dropout\n",
    "\n",
    "        # Residual layers with fewer blocks\n",
    "        self.layer1 = self._make_layer(32, layers=1)\n",
    "        self.layer2 = self._make_layer(64, layers=1, stride=2)\n",
    "        self.layer3 = self._make_layer(128, layers=1, stride=2)\n",
    "        self.layer4 = self._make_layer(256, layers=1, stride=2)\n",
    "\n",
    "        # Adaptive pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, layers, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "            )\n",
    "        layers_list = []\n",
    "        layers_list.append(ResidualBlock(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "        self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension\n",
    "        x = self.dropout(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing steps remain unchanged\n",
    "# ----------------------------\n",
    "\n",
    "# Step 1: Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Step 2: k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Step 2: Filter sequences by genus frequency\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# ----------------------------\n",
    "# Main Experiment Loop using Residual CNNModel\n",
    "# ----------------------------\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    data_by_class = defaultdict(list)\n",
    "    with open(filtered_vector_file, 'r') as f:\n",
    "        for line in f:\n",
    "            label = line.split('\\t')[0][1:]\n",
    "            data_by_class[label].append(line.strip())\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    # Initialize the model using the new CNNModel\n",
    "    input_length = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(input_length, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    # Training loop with testing using Residual CNNModel\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_correct_train = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted_train = torch.max(outputs, 1)\n",
    "            total_correct_train += (predicted_train == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = total_correct_train / total_train_samples * 100\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Evaluate on test dataset\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNext, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# ----------------------------\n",
    "# Helper functions for k-mer processing\n",
    "# ----------------------------\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "# ----------------------------\n",
    "# Data Preprocessing: Clean FASTA headers and k-mer vectorization\n",
    "# ----------------------------\n",
    "# Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Filter sequences by genus frequency (keeping only those with frequency >= 10)\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# ----------------------------\n",
    "# Define ConvNeXt components: DropPath, ConvNeXtBlock1D and CNNModel\n",
    "# ----------------------------\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample\"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        output = x.div(keep_prob) * random_tensor\n",
    "        return output\n",
    "\n",
    "class ConvNeXtBlock1D(nn.Module):\n",
    "    def __init__(self, dim, drop_path=0.0, layer_scale_init_value=1e-6):\n",
    "        super(ConvNeXtBlock1D, self).__init__()\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, length]\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 1)  # Change to [batch_size, length, channels] for LayerNorm\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = self.drop_path(x)\n",
    "        x = x + input.permute(0, 2, 1)\n",
    "        x = x.permute(0, 2, 1)  # Back to [batch_size, channels, length]\n",
    "        return x\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_length, num_classes, depths=[3, 3, 9, 3], dims=[64, 128, 256, 512], drop_path_rate=0.0):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Stem Layer: initial downsampling\n",
    "        self.downsample_layers = nn.ModuleList()\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv1d(1, dims[0], kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm1d(dims[0])\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "\n",
    "        # Downsampling Layers for subsequent stages\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                nn.BatchNorm1d(dims[i]),\n",
    "                nn.Conv1d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        # Compute drop path rates for each block\n",
    "        dp_rates = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "\n",
    "        # Stages: stack ConvNeXt blocks per stage\n",
    "        self.stages = nn.ModuleList()\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[ConvNeXtBlock1D(dim=dims[i], drop_path=dp_rates[cur + j]) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        # Final normalization and classification head\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6)\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: [batch_size, 1, length]\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        x = x.mean(-1)  # Global average pooling over length dimension\n",
    "        x = self.norm(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Main Experiment Loop: Run the training/testing 10 times using the ConvNeXt-based model\n",
    "# ----------------------------\n",
    "best_accuracies = []\n",
    "\n",
    "# Read filtered k-mer vectors and group by genus\n",
    "data_by_class = defaultdict(list)\n",
    "with open(filtered_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        label = line.split('\\t')[0][1:]\n",
    "        data_by_class[label].append(line.strip())\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Create train and test splits: For each genus, choose one sample for testing and the rest for training.\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    input_length = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(input_length, num_classes, drop_path_rate=0.1).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "    max_accuracy = 0\n",
    "\n",
    "    # Training loop for this run\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_correct_train = 0\n",
    "        total_train_samples = 0\n",
    "\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            _, predicted_train = torch.max(outputs, 1)\n",
    "            total_correct_train += (predicted_train == labels).sum().item()\n",
    "            total_train_samples += labels.size(0)\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = total_correct_train / total_train_samples * 100\n",
    "        print(f'Run {run+1}/{num_runs}, Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Evaluate on test dataset\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_samples * 100\n",
    "        print(f'Run {run+1}/{num_runs}, Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "    print(f'Run {run+1} Best Test Accuracy: {max_accuracy:.2f}%')\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'\\nAverage Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
