{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 trans fusion, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/10 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 3.4437 | Test Acc: 6.17%\n",
      "Epoch 2/100 | Loss: 2.5918 | Test Acc: 16.05%\n",
      "Epoch 3/100 | Loss: 2.2695 | Test Acc: 23.46%\n",
      "Epoch 4/100 | Loss: 1.9817 | Test Acc: 20.99%\n",
      "Epoch 5/100 | Loss: 1.7952 | Test Acc: 27.16%\n",
      "Epoch 6/100 | Loss: 1.6623 | Test Acc: 27.16%\n",
      "Epoch 7/100 | Loss: 1.6584 | Test Acc: 30.86%\n",
      "Epoch 8/100 | Loss: 1.5764 | Test Acc: 39.51%\n",
      "Epoch 9/100 | Loss: 1.3764 | Test Acc: 45.68%\n",
      "Epoch 10/100 | Loss: 1.2214 | Test Acc: 51.85%\n",
      "Epoch 11/100 | Loss: 1.2532 | Test Acc: 48.15%\n",
      "Epoch 12/100 | Loss: 1.1386 | Test Acc: 48.15%\n",
      "Epoch 13/100 | Loss: 1.0929 | Test Acc: 49.38%\n",
      "Epoch 14/100 | Loss: 1.0260 | Test Acc: 44.44%\n",
      "Epoch 15/100 | Loss: 0.9988 | Test Acc: 48.15%\n",
      "Epoch 16/100 | Loss: 1.0364 | Test Acc: 51.85%\n",
      "Epoch 17/100 | Loss: 1.0139 | Test Acc: 55.56%\n",
      "Epoch 18/100 | Loss: 0.9909 | Test Acc: 50.62%\n",
      "Epoch 19/100 | Loss: 0.9880 | Test Acc: 44.44%\n",
      "Epoch 20/100 | Loss: 0.9533 | Test Acc: 55.56%\n",
      "Epoch 21/100 | Loss: 0.9225 | Test Acc: 58.02%\n",
      "Epoch 22/100 | Loss: 0.9713 | Test Acc: 53.09%\n",
      "Epoch 23/100 | Loss: 0.9241 | Test Acc: 46.91%\n",
      "Epoch 24/100 | Loss: 0.9512 | Test Acc: 49.38%\n",
      "Epoch 25/100 | Loss: 0.9083 | Test Acc: 58.02%\n",
      "Epoch 26/100 | Loss: 0.7886 | Test Acc: 61.73%\n",
      "Epoch 27/100 | Loss: 0.8824 | Test Acc: 53.09%\n",
      "Epoch 28/100 | Loss: 0.8642 | Test Acc: 56.79%\n",
      "Epoch 29/100 | Loss: 0.8937 | Test Acc: 60.49%\n",
      "Epoch 30/100 | Loss: 0.8205 | Test Acc: 56.79%\n",
      "Epoch 31/100 | Loss: 0.8341 | Test Acc: 59.26%\n",
      "Epoch 32/100 | Loss: 0.7772 | Test Acc: 54.32%\n",
      "Epoch 33/100 | Loss: 0.8140 | Test Acc: 58.02%\n",
      "Epoch 34/100 | Loss: 0.7755 | Test Acc: 58.02%\n",
      "Epoch 35/100 | Loss: 0.8068 | Test Acc: 54.32%\n",
      "Epoch 36/100 | Loss: 0.7918 | Test Acc: 56.79%\n",
      "Epoch 37/100 | Loss: 0.7872 | Test Acc: 55.56%\n",
      "Epoch 38/100 | Loss: 0.8322 | Test Acc: 54.32%\n",
      "Epoch 39/100 | Loss: 0.7868 | Test Acc: 59.26%\n",
      "Epoch 40/100 | Loss: 0.6850 | Test Acc: 55.56%\n",
      "Epoch 41/100 | Loss: 0.6865 | Test Acc: 65.43%\n",
      "Epoch 42/100 | Loss: 0.7298 | Test Acc: 54.32%\n",
      "Epoch 43/100 | Loss: 0.8090 | Test Acc: 60.49%\n",
      "Epoch 44/100 | Loss: 0.7225 | Test Acc: 59.26%\n",
      "Epoch 45/100 | Loss: 0.7708 | Test Acc: 53.09%\n",
      "Epoch 46/100 | Loss: 0.6875 | Test Acc: 62.96%\n",
      "Epoch 47/100 | Loss: 0.6647 | Test Acc: 60.49%\n",
      "Epoch 48/100 | Loss: 0.7374 | Test Acc: 56.79%\n",
      "Epoch 49/100 | Loss: 0.7310 | Test Acc: 49.38%\n",
      "Epoch 50/100 | Loss: 0.7051 | Test Acc: 58.02%\n",
      "Epoch 51/100 | Loss: 0.6738 | Test Acc: 55.56%\n",
      "Epoch 52/100 | Loss: 0.7197 | Test Acc: 51.85%\n",
      "Epoch 53/100 | Loss: 0.6148 | Test Acc: 59.26%\n",
      "Epoch 54/100 | Loss: 0.6662 | Test Acc: 59.26%\n",
      "Epoch 55/100 | Loss: 0.5873 | Test Acc: 62.96%\n",
      "Epoch 56/100 | Loss: 0.5984 | Test Acc: 59.26%\n",
      "Epoch 57/100 | Loss: 0.6774 | Test Acc: 55.56%\n",
      "Epoch 58/100 | Loss: 0.6666 | Test Acc: 59.26%\n",
      "Epoch 59/100 | Loss: 0.6754 | Test Acc: 49.38%\n",
      "Epoch 60/100 | Loss: 0.6178 | Test Acc: 54.32%\n",
      "Epoch 61/100 | Loss: 0.6370 | Test Acc: 58.02%\n",
      "Epoch 62/100 | Loss: 0.7569 | Test Acc: 59.26%\n",
      "Epoch 63/100 | Loss: 0.6808 | Test Acc: 54.32%\n",
      "Epoch 64/100 | Loss: 0.5977 | Test Acc: 64.20%\n",
      "Epoch 65/100 | Loss: 0.5688 | Test Acc: 64.20%\n",
      "Epoch 66/100 | Loss: 0.5843 | Test Acc: 61.73%\n",
      "Epoch 67/100 | Loss: 0.6026 | Test Acc: 51.85%\n",
      "Epoch 68/100 | Loss: 0.6351 | Test Acc: 54.32%\n",
      "Epoch 69/100 | Loss: 0.5847 | Test Acc: 53.09%\n",
      "Epoch 70/100 | Loss: 0.5184 | Test Acc: 58.02%\n",
      "Epoch 71/100 | Loss: 0.5995 | Test Acc: 55.56%\n",
      "Epoch 72/100 | Loss: 0.5860 | Test Acc: 55.56%\n",
      "Epoch 73/100 | Loss: 0.6211 | Test Acc: 51.85%\n",
      "Epoch 74/100 | Loss: 0.5801 | Test Acc: 64.20%\n",
      "Epoch 75/100 | Loss: 0.6420 | Test Acc: 51.85%\n",
      "Epoch 76/100 | Loss: 0.6007 | Test Acc: 58.02%\n",
      "Epoch 77/100 | Loss: 0.5928 | Test Acc: 58.02%\n",
      "Epoch 78/100 | Loss: 0.6582 | Test Acc: 51.85%\n",
      "Epoch 79/100 | Loss: 0.5753 | Test Acc: 54.32%\n",
      "Epoch 80/100 | Loss: 0.5677 | Test Acc: 56.79%\n",
      "Epoch 81/100 | Loss: 0.6023 | Test Acc: 56.79%\n",
      "Epoch 82/100 | Loss: 0.5732 | Test Acc: 55.56%\n",
      "Epoch 83/100 | Loss: 0.4854 | Test Acc: 60.49%\n",
      "Epoch 84/100 | Loss: 0.4853 | Test Acc: 61.73%\n",
      "Epoch 85/100 | Loss: 0.4867 | Test Acc: 60.49%\n",
      "Epoch 86/100 | Loss: 0.5177 | Test Acc: 59.26%\n",
      "Epoch 87/100 | Loss: 0.5487 | Test Acc: 59.26%\n",
      "Epoch 88/100 | Loss: 0.5442 | Test Acc: 54.32%\n",
      "Epoch 89/100 | Loss: 0.5234 | Test Acc: 56.79%\n",
      "Epoch 90/100 | Loss: 0.4634 | Test Acc: 59.26%\n",
      "Epoch 91/100 | Loss: 0.6543 | Test Acc: 54.32%\n",
      "Epoch 92/100 | Loss: 0.5764 | Test Acc: 53.09%\n",
      "Epoch 93/100 | Loss: 0.5248 | Test Acc: 64.20%\n",
      "Epoch 94/100 | Loss: 0.4791 | Test Acc: 61.73%\n",
      "Epoch 95/100 | Loss: 0.4654 | Test Acc: 58.02%\n",
      "Epoch 96/100 | Loss: 0.4428 | Test Acc: 62.96%\n",
      "Epoch 97/100 | Loss: 0.4944 | Test Acc: 58.02%\n",
      "Epoch 98/100 | Loss: 0.4789 | Test Acc: 58.02%\n",
      "Epoch 99/100 | Loss: 0.5694 | Test Acc: 65.43%\n",
      "Epoch 100/100 | Loss: 0.5316 | Test Acc: 64.20%\n",
      "Run 1 Best Test Accuracy: 65.43%\n",
      "\n",
      "=== Run 2/10 ===\n",
      "Epoch 1/100 | Loss: 3.4134 | Test Acc: 11.11%\n",
      "Epoch 2/100 | Loss: 2.6158 | Test Acc: 13.58%\n",
      "Epoch 3/100 | Loss: 2.3292 | Test Acc: 22.22%\n",
      "Epoch 4/100 | Loss: 2.1168 | Test Acc: 23.46%\n",
      "Epoch 5/100 | Loss: 2.1076 | Test Acc: 22.22%\n",
      "Epoch 6/100 | Loss: 2.0167 | Test Acc: 33.33%\n",
      "Epoch 7/100 | Loss: 1.9880 | Test Acc: 33.33%\n",
      "Epoch 8/100 | Loss: 1.8080 | Test Acc: 29.63%\n",
      "Epoch 9/100 | Loss: 1.7449 | Test Acc: 37.04%\n",
      "Epoch 10/100 | Loss: 1.6793 | Test Acc: 44.44%\n",
      "Epoch 11/100 | Loss: 1.7409 | Test Acc: 33.33%\n",
      "Epoch 12/100 | Loss: 1.6262 | Test Acc: 41.98%\n",
      "Epoch 13/100 | Loss: 1.5762 | Test Acc: 38.27%\n",
      "Epoch 14/100 | Loss: 1.5739 | Test Acc: 44.44%\n",
      "Epoch 15/100 | Loss: 1.5002 | Test Acc: 43.21%\n",
      "Epoch 16/100 | Loss: 1.4545 | Test Acc: 37.04%\n",
      "Epoch 17/100 | Loss: 1.5598 | Test Acc: 45.68%\n",
      "Epoch 18/100 | Loss: 1.5432 | Test Acc: 37.04%\n",
      "Epoch 19/100 | Loss: 1.5746 | Test Acc: 28.40%\n",
      "Epoch 20/100 | Loss: 1.5536 | Test Acc: 41.98%\n",
      "Epoch 21/100 | Loss: 1.4429 | Test Acc: 41.98%\n",
      "Epoch 22/100 | Loss: 1.3607 | Test Acc: 48.15%\n",
      "Epoch 23/100 | Loss: 1.3701 | Test Acc: 41.98%\n",
      "Epoch 24/100 | Loss: 1.3333 | Test Acc: 39.51%\n",
      "Epoch 25/100 | Loss: 1.3915 | Test Acc: 49.38%\n",
      "Epoch 26/100 | Loss: 1.2703 | Test Acc: 41.98%\n",
      "Epoch 27/100 | Loss: 1.2682 | Test Acc: 39.51%\n",
      "Epoch 28/100 | Loss: 1.2350 | Test Acc: 53.09%\n",
      "Epoch 29/100 | Loss: 1.2973 | Test Acc: 50.62%\n",
      "Epoch 30/100 | Loss: 1.3262 | Test Acc: 44.44%\n",
      "Epoch 31/100 | Loss: 1.3326 | Test Acc: 48.15%\n",
      "Epoch 32/100 | Loss: 1.2811 | Test Acc: 44.44%\n",
      "Epoch 33/100 | Loss: 1.2938 | Test Acc: 53.09%\n",
      "Epoch 34/100 | Loss: 1.1457 | Test Acc: 54.32%\n",
      "Epoch 35/100 | Loss: 1.1475 | Test Acc: 53.09%\n",
      "Epoch 36/100 | Loss: 1.1610 | Test Acc: 59.26%\n",
      "Epoch 37/100 | Loss: 1.1386 | Test Acc: 51.85%\n",
      "Epoch 38/100 | Loss: 1.2355 | Test Acc: 51.85%\n",
      "Epoch 39/100 | Loss: 1.1169 | Test Acc: 56.79%\n",
      "Epoch 40/100 | Loss: 1.0699 | Test Acc: 58.02%\n",
      "Epoch 41/100 | Loss: 1.0486 | Test Acc: 55.56%\n",
      "Epoch 42/100 | Loss: 1.0188 | Test Acc: 53.09%\n",
      "Epoch 43/100 | Loss: 0.9630 | Test Acc: 56.79%\n",
      "Epoch 44/100 | Loss: 0.9508 | Test Acc: 54.32%\n",
      "Epoch 45/100 | Loss: 0.9352 | Test Acc: 53.09%\n",
      "Epoch 46/100 | Loss: 0.9080 | Test Acc: 61.73%\n",
      "Epoch 47/100 | Loss: 0.9529 | Test Acc: 58.02%\n",
      "Epoch 48/100 | Loss: 0.8784 | Test Acc: 56.79%\n",
      "Epoch 49/100 | Loss: 0.8745 | Test Acc: 58.02%\n",
      "Epoch 50/100 | Loss: 0.8663 | Test Acc: 40.74%\n",
      "Epoch 51/100 | Loss: 0.8718 | Test Acc: 60.49%\n",
      "Epoch 52/100 | Loss: 0.9680 | Test Acc: 56.79%\n",
      "Epoch 53/100 | Loss: 0.9404 | Test Acc: 55.56%\n",
      "Epoch 54/100 | Loss: 0.9104 | Test Acc: 58.02%\n",
      "Epoch 55/100 | Loss: 0.9210 | Test Acc: 54.32%\n",
      "Epoch 56/100 | Loss: 0.9229 | Test Acc: 54.32%\n",
      "Epoch 57/100 | Loss: 0.9246 | Test Acc: 53.09%\n",
      "Epoch 58/100 | Loss: 0.8691 | Test Acc: 62.96%\n",
      "Epoch 59/100 | Loss: 0.8476 | Test Acc: 62.96%\n",
      "Epoch 60/100 | Loss: 0.8065 | Test Acc: 53.09%\n",
      "Epoch 61/100 | Loss: 0.9264 | Test Acc: 56.79%\n",
      "Epoch 62/100 | Loss: 0.8378 | Test Acc: 64.20%\n",
      "Epoch 63/100 | Loss: 0.8544 | Test Acc: 51.85%\n",
      "Epoch 64/100 | Loss: 0.9179 | Test Acc: 55.56%\n",
      "Epoch 65/100 | Loss: 0.8213 | Test Acc: 55.56%\n",
      "Epoch 66/100 | Loss: 0.7931 | Test Acc: 54.32%\n",
      "Epoch 67/100 | Loss: 0.8170 | Test Acc: 55.56%\n",
      "Epoch 68/100 | Loss: 0.8048 | Test Acc: 62.96%\n",
      "Epoch 69/100 | Loss: 0.7695 | Test Acc: 58.02%\n",
      "Epoch 70/100 | Loss: 0.8369 | Test Acc: 64.20%\n",
      "Epoch 71/100 | Loss: 0.7678 | Test Acc: 59.26%\n",
      "Epoch 72/100 | Loss: 0.7546 | Test Acc: 55.56%\n",
      "Epoch 73/100 | Loss: 0.7704 | Test Acc: 54.32%\n",
      "Epoch 74/100 | Loss: 0.7438 | Test Acc: 49.38%\n",
      "Epoch 75/100 | Loss: 0.9399 | Test Acc: 64.20%\n",
      "Epoch 76/100 | Loss: 0.7593 | Test Acc: 62.96%\n",
      "Epoch 77/100 | Loss: 0.7178 | Test Acc: 61.73%\n",
      "Epoch 78/100 | Loss: 0.7119 | Test Acc: 55.56%\n",
      "Epoch 79/100 | Loss: 0.7395 | Test Acc: 59.26%\n",
      "Epoch 80/100 | Loss: 0.6908 | Test Acc: 65.43%\n",
      "Epoch 81/100 | Loss: 0.6978 | Test Acc: 60.49%\n",
      "Epoch 82/100 | Loss: 0.7496 | Test Acc: 65.43%\n",
      "Epoch 83/100 | Loss: 0.7242 | Test Acc: 64.20%\n",
      "Epoch 84/100 | Loss: 0.6549 | Test Acc: 59.26%\n",
      "Epoch 85/100 | Loss: 0.6800 | Test Acc: 64.20%\n",
      "Epoch 86/100 | Loss: 0.7402 | Test Acc: 56.79%\n",
      "Epoch 87/100 | Loss: 0.7014 | Test Acc: 62.96%\n",
      "Epoch 88/100 | Loss: 0.7103 | Test Acc: 62.96%\n",
      "Epoch 89/100 | Loss: 0.6940 | Test Acc: 55.56%\n",
      "Epoch 90/100 | Loss: 0.7307 | Test Acc: 60.49%\n",
      "Epoch 91/100 | Loss: 0.6467 | Test Acc: 58.02%\n",
      "Epoch 92/100 | Loss: 0.6672 | Test Acc: 56.79%\n",
      "Epoch 93/100 | Loss: 0.7461 | Test Acc: 54.32%\n",
      "Epoch 94/100 | Loss: 0.6799 | Test Acc: 58.02%\n",
      "Epoch 95/100 | Loss: 0.6613 | Test Acc: 61.73%\n",
      "Epoch 96/100 | Loss: 0.6533 | Test Acc: 59.26%\n",
      "Epoch 97/100 | Loss: 0.6503 | Test Acc: 54.32%\n",
      "Epoch 98/100 | Loss: 0.6567 | Test Acc: 54.32%\n",
      "Epoch 99/100 | Loss: 0.6250 | Test Acc: 61.73%\n",
      "Epoch 100/100 | Loss: 0.6161 | Test Acc: 65.43%\n",
      "Run 2 Best Test Accuracy: 65.43%\n",
      "\n",
      "=== Run 3/10 ===\n",
      "Epoch 1/100 | Loss: 3.8147 | Test Acc: 3.70%\n",
      "Epoch 2/100 | Loss: 3.2017 | Test Acc: 8.64%\n",
      "Epoch 3/100 | Loss: 2.9434 | Test Acc: 14.81%\n",
      "Epoch 4/100 | Loss: 2.9726 | Test Acc: 13.58%\n",
      "Epoch 5/100 | Loss: 2.7731 | Test Acc: 12.35%\n",
      "Epoch 6/100 | Loss: 2.6071 | Test Acc: 17.28%\n",
      "Epoch 7/100 | Loss: 2.4442 | Test Acc: 16.05%\n",
      "Epoch 8/100 | Loss: 2.3066 | Test Acc: 20.99%\n",
      "Epoch 9/100 | Loss: 2.2499 | Test Acc: 19.75%\n",
      "Epoch 10/100 | Loss: 2.2306 | Test Acc: 23.46%\n",
      "Epoch 11/100 | Loss: 2.1718 | Test Acc: 25.93%\n",
      "Epoch 12/100 | Loss: 2.0262 | Test Acc: 25.93%\n",
      "Epoch 13/100 | Loss: 1.9746 | Test Acc: 20.99%\n",
      "Epoch 14/100 | Loss: 1.9001 | Test Acc: 24.69%\n",
      "Epoch 15/100 | Loss: 1.8791 | Test Acc: 27.16%\n",
      "Epoch 16/100 | Loss: 1.8876 | Test Acc: 32.10%\n",
      "Epoch 17/100 | Loss: 1.8414 | Test Acc: 34.57%\n",
      "Epoch 18/100 | Loss: 1.8894 | Test Acc: 22.22%\n",
      "Epoch 19/100 | Loss: 1.9048 | Test Acc: 28.40%\n",
      "Epoch 20/100 | Loss: 1.9077 | Test Acc: 30.86%\n",
      "Epoch 21/100 | Loss: 1.9364 | Test Acc: 23.46%\n",
      "Epoch 22/100 | Loss: 1.8587 | Test Acc: 29.63%\n",
      "Epoch 23/100 | Loss: 1.7351 | Test Acc: 28.40%\n",
      "Epoch 24/100 | Loss: 1.7524 | Test Acc: 35.80%\n",
      "Epoch 25/100 | Loss: 1.8008 | Test Acc: 19.75%\n",
      "Epoch 26/100 | Loss: 1.7895 | Test Acc: 33.33%\n",
      "Epoch 27/100 | Loss: 1.6665 | Test Acc: 37.04%\n",
      "Epoch 28/100 | Loss: 1.6768 | Test Acc: 34.57%\n",
      "Epoch 29/100 | Loss: 1.6773 | Test Acc: 35.80%\n",
      "Epoch 30/100 | Loss: 1.6661 | Test Acc: 30.86%\n",
      "Epoch 31/100 | Loss: 1.6595 | Test Acc: 35.80%\n",
      "Epoch 32/100 | Loss: 1.7093 | Test Acc: 34.57%\n",
      "Epoch 33/100 | Loss: 1.6861 | Test Acc: 37.04%\n",
      "Epoch 34/100 | Loss: 1.5411 | Test Acc: 35.80%\n",
      "Epoch 35/100 | Loss: 1.6085 | Test Acc: 33.33%\n",
      "Epoch 36/100 | Loss: 1.6720 | Test Acc: 34.57%\n",
      "Epoch 37/100 | Loss: 1.5969 | Test Acc: 33.33%\n",
      "Epoch 38/100 | Loss: 1.4947 | Test Acc: 38.27%\n",
      "Epoch 39/100 | Loss: 1.5288 | Test Acc: 32.10%\n",
      "Epoch 40/100 | Loss: 1.4773 | Test Acc: 30.86%\n",
      "Epoch 41/100 | Loss: 1.5708 | Test Acc: 33.33%\n",
      "Epoch 42/100 | Loss: 1.4630 | Test Acc: 39.51%\n",
      "Epoch 43/100 | Loss: 1.5903 | Test Acc: 32.10%\n",
      "Epoch 44/100 | Loss: 1.5278 | Test Acc: 34.57%\n",
      "Epoch 45/100 | Loss: 1.5371 | Test Acc: 34.57%\n",
      "Epoch 46/100 | Loss: 1.4513 | Test Acc: 39.51%\n",
      "Epoch 47/100 | Loss: 1.4308 | Test Acc: 35.80%\n",
      "Epoch 48/100 | Loss: 1.4585 | Test Acc: 38.27%\n",
      "Epoch 49/100 | Loss: 1.4210 | Test Acc: 35.80%\n",
      "Epoch 50/100 | Loss: 1.3675 | Test Acc: 41.98%\n",
      "Epoch 51/100 | Loss: 1.4818 | Test Acc: 33.33%\n",
      "Epoch 52/100 | Loss: 1.4732 | Test Acc: 32.10%\n",
      "Epoch 53/100 | Loss: 1.5309 | Test Acc: 32.10%\n",
      "Epoch 54/100 | Loss: 1.5199 | Test Acc: 33.33%\n",
      "Epoch 55/100 | Loss: 1.4214 | Test Acc: 37.04%\n",
      "Epoch 56/100 | Loss: 1.3934 | Test Acc: 37.04%\n",
      "Epoch 57/100 | Loss: 1.4504 | Test Acc: 37.04%\n",
      "Epoch 58/100 | Loss: 1.4306 | Test Acc: 38.27%\n",
      "Epoch 59/100 | Loss: 1.4763 | Test Acc: 38.27%\n",
      "Epoch 60/100 | Loss: 1.4209 | Test Acc: 40.74%\n",
      "Epoch 61/100 | Loss: 1.4150 | Test Acc: 35.80%\n",
      "Epoch 62/100 | Loss: 1.3857 | Test Acc: 30.86%\n",
      "Epoch 63/100 | Loss: 1.3713 | Test Acc: 41.98%\n",
      "Epoch 64/100 | Loss: 1.3946 | Test Acc: 43.21%\n",
      "Epoch 65/100 | Loss: 1.3741 | Test Acc: 41.98%\n",
      "Epoch 66/100 | Loss: 1.4431 | Test Acc: 35.80%\n",
      "Epoch 67/100 | Loss: 1.3906 | Test Acc: 38.27%\n",
      "Epoch 68/100 | Loss: 1.4318 | Test Acc: 34.57%\n",
      "Epoch 69/100 | Loss: 1.3797 | Test Acc: 39.51%\n",
      "Epoch 70/100 | Loss: 1.4372 | Test Acc: 34.57%\n",
      "Epoch 71/100 | Loss: 1.3125 | Test Acc: 39.51%\n",
      "Epoch 72/100 | Loss: 1.3928 | Test Acc: 35.80%\n",
      "Epoch 73/100 | Loss: 1.3216 | Test Acc: 43.21%\n",
      "Epoch 74/100 | Loss: 1.2776 | Test Acc: 38.27%\n",
      "Epoch 75/100 | Loss: 1.2704 | Test Acc: 37.04%\n",
      "Epoch 76/100 | Loss: 1.2565 | Test Acc: 39.51%\n",
      "Epoch 77/100 | Loss: 1.2521 | Test Acc: 33.33%\n",
      "Epoch 78/100 | Loss: 1.2223 | Test Acc: 38.27%\n",
      "Epoch 79/100 | Loss: 1.2760 | Test Acc: 38.27%\n",
      "Epoch 80/100 | Loss: 1.2265 | Test Acc: 43.21%\n",
      "Epoch 81/100 | Loss: 1.3377 | Test Acc: 41.98%\n",
      "Epoch 82/100 | Loss: 1.3170 | Test Acc: 40.74%\n",
      "Epoch 83/100 | Loss: 1.2302 | Test Acc: 43.21%\n",
      "Epoch 84/100 | Loss: 1.2576 | Test Acc: 34.57%\n",
      "Epoch 85/100 | Loss: 1.2478 | Test Acc: 46.91%\n",
      "Epoch 86/100 | Loss: 1.2302 | Test Acc: 43.21%\n",
      "Epoch 87/100 | Loss: 1.2439 | Test Acc: 39.51%\n",
      "Epoch 88/100 | Loss: 1.1897 | Test Acc: 38.27%\n",
      "Epoch 89/100 | Loss: 1.2162 | Test Acc: 37.04%\n",
      "Epoch 90/100 | Loss: 1.2207 | Test Acc: 38.27%\n",
      "Epoch 91/100 | Loss: 1.2008 | Test Acc: 40.74%\n",
      "Epoch 92/100 | Loss: 1.2392 | Test Acc: 45.68%\n",
      "Epoch 93/100 | Loss: 1.2634 | Test Acc: 38.27%\n",
      "Epoch 94/100 | Loss: 1.2530 | Test Acc: 43.21%\n",
      "Epoch 95/100 | Loss: 1.2046 | Test Acc: 43.21%\n",
      "Epoch 96/100 | Loss: 1.2368 | Test Acc: 45.68%\n",
      "Epoch 97/100 | Loss: 1.2193 | Test Acc: 39.51%\n",
      "Epoch 98/100 | Loss: 1.2114 | Test Acc: 34.57%\n",
      "Epoch 99/100 | Loss: 1.2438 | Test Acc: 40.74%\n",
      "Epoch 100/100 | Loss: 1.1833 | Test Acc: 44.44%\n",
      "Run 3 Best Test Accuracy: 46.91%\n",
      "\n",
      "=== Run 4/10 ===\n",
      "Epoch 1/100 | Loss: 3.4393 | Test Acc: 7.41%\n",
      "Epoch 2/100 | Loss: 2.7178 | Test Acc: 13.58%\n",
      "Epoch 3/100 | Loss: 2.3831 | Test Acc: 17.28%\n",
      "Epoch 4/100 | Loss: 2.1748 | Test Acc: 23.46%\n",
      "Epoch 5/100 | Loss: 1.9724 | Test Acc: 32.10%\n",
      "Epoch 6/100 | Loss: 2.0170 | Test Acc: 32.10%\n",
      "Epoch 7/100 | Loss: 1.8210 | Test Acc: 29.63%\n",
      "Epoch 8/100 | Loss: 1.7960 | Test Acc: 33.33%\n",
      "Epoch 9/100 | Loss: 1.6510 | Test Acc: 30.86%\n",
      "Epoch 10/100 | Loss: 1.6489 | Test Acc: 30.86%\n",
      "Epoch 11/100 | Loss: 1.6071 | Test Acc: 35.80%\n",
      "Epoch 12/100 | Loss: 1.5751 | Test Acc: 28.40%\n",
      "Epoch 13/100 | Loss: 1.6238 | Test Acc: 37.04%\n",
      "Epoch 14/100 | Loss: 1.4210 | Test Acc: 43.21%\n",
      "Epoch 15/100 | Loss: 1.3988 | Test Acc: 33.33%\n",
      "Epoch 16/100 | Loss: 1.4138 | Test Acc: 43.21%\n",
      "Epoch 17/100 | Loss: 1.3308 | Test Acc: 45.68%\n",
      "Epoch 18/100 | Loss: 1.3240 | Test Acc: 39.51%\n",
      "Epoch 19/100 | Loss: 1.2936 | Test Acc: 37.04%\n",
      "Epoch 20/100 | Loss: 1.2677 | Test Acc: 48.15%\n",
      "Epoch 21/100 | Loss: 1.2582 | Test Acc: 40.74%\n",
      "Epoch 22/100 | Loss: 1.2289 | Test Acc: 41.98%\n",
      "Epoch 23/100 | Loss: 1.1761 | Test Acc: 44.44%\n",
      "Epoch 24/100 | Loss: 1.1705 | Test Acc: 45.68%\n",
      "Epoch 25/100 | Loss: 1.1794 | Test Acc: 49.38%\n",
      "Epoch 26/100 | Loss: 1.0958 | Test Acc: 50.62%\n",
      "Epoch 27/100 | Loss: 1.0476 | Test Acc: 41.98%\n",
      "Epoch 28/100 | Loss: 1.0222 | Test Acc: 49.38%\n",
      "Epoch 29/100 | Loss: 0.9924 | Test Acc: 48.15%\n",
      "Epoch 30/100 | Loss: 0.9635 | Test Acc: 53.09%\n",
      "Epoch 31/100 | Loss: 1.0151 | Test Acc: 50.62%\n",
      "Epoch 32/100 | Loss: 1.0118 | Test Acc: 46.91%\n",
      "Epoch 33/100 | Loss: 1.0454 | Test Acc: 49.38%\n",
      "Epoch 34/100 | Loss: 0.9595 | Test Acc: 51.85%\n",
      "Epoch 35/100 | Loss: 0.9747 | Test Acc: 45.68%\n",
      "Epoch 36/100 | Loss: 0.9589 | Test Acc: 45.68%\n",
      "Epoch 37/100 | Loss: 1.0063 | Test Acc: 54.32%\n",
      "Epoch 38/100 | Loss: 0.9424 | Test Acc: 48.15%\n",
      "Epoch 39/100 | Loss: 0.9975 | Test Acc: 50.62%\n",
      "Epoch 40/100 | Loss: 0.9328 | Test Acc: 48.15%\n",
      "Epoch 41/100 | Loss: 0.9781 | Test Acc: 49.38%\n",
      "Epoch 42/100 | Loss: 1.0477 | Test Acc: 53.09%\n",
      "Epoch 43/100 | Loss: 0.9380 | Test Acc: 44.44%\n",
      "Epoch 44/100 | Loss: 0.9454 | Test Acc: 56.79%\n",
      "Epoch 45/100 | Loss: 1.0406 | Test Acc: 43.21%\n",
      "Epoch 46/100 | Loss: 0.9643 | Test Acc: 48.15%\n",
      "Epoch 47/100 | Loss: 1.0111 | Test Acc: 48.15%\n",
      "Epoch 48/100 | Loss: 0.9728 | Test Acc: 46.91%\n",
      "Epoch 49/100 | Loss: 0.9339 | Test Acc: 41.98%\n",
      "Epoch 50/100 | Loss: 0.8965 | Test Acc: 50.62%\n",
      "Epoch 51/100 | Loss: 0.9704 | Test Acc: 51.85%\n",
      "Epoch 52/100 | Loss: 0.9580 | Test Acc: 53.09%\n",
      "Epoch 53/100 | Loss: 0.9493 | Test Acc: 59.26%\n",
      "Epoch 54/100 | Loss: 0.9965 | Test Acc: 51.85%\n",
      "Epoch 55/100 | Loss: 0.9644 | Test Acc: 55.56%\n",
      "Epoch 56/100 | Loss: 0.9351 | Test Acc: 54.32%\n",
      "Epoch 57/100 | Loss: 0.8978 | Test Acc: 58.02%\n",
      "Epoch 58/100 | Loss: 0.8697 | Test Acc: 51.85%\n",
      "Epoch 59/100 | Loss: 0.8396 | Test Acc: 55.56%\n",
      "Epoch 60/100 | Loss: 0.8174 | Test Acc: 58.02%\n",
      "Epoch 61/100 | Loss: 0.8516 | Test Acc: 54.32%\n",
      "Epoch 62/100 | Loss: 0.9421 | Test Acc: 55.56%\n",
      "Epoch 63/100 | Loss: 0.8624 | Test Acc: 55.56%\n",
      "Epoch 64/100 | Loss: 0.8248 | Test Acc: 54.32%\n",
      "Epoch 65/100 | Loss: 0.7948 | Test Acc: 56.79%\n",
      "Epoch 66/100 | Loss: 0.9109 | Test Acc: 44.44%\n",
      "Epoch 67/100 | Loss: 0.9008 | Test Acc: 54.32%\n",
      "Epoch 68/100 | Loss: 0.8780 | Test Acc: 50.62%\n",
      "Epoch 69/100 | Loss: 0.8809 | Test Acc: 53.09%\n",
      "Epoch 70/100 | Loss: 0.9104 | Test Acc: 50.62%\n",
      "Epoch 71/100 | Loss: 1.0265 | Test Acc: 43.21%\n",
      "Epoch 72/100 | Loss: 0.9681 | Test Acc: 49.38%\n",
      "Epoch 73/100 | Loss: 0.8280 | Test Acc: 55.56%\n",
      "Epoch 74/100 | Loss: 0.8616 | Test Acc: 53.09%\n",
      "Epoch 75/100 | Loss: 0.8359 | Test Acc: 53.09%\n",
      "Epoch 76/100 | Loss: 0.8650 | Test Acc: 58.02%\n",
      "Epoch 77/100 | Loss: 0.8006 | Test Acc: 51.85%\n",
      "Epoch 78/100 | Loss: 0.7746 | Test Acc: 55.56%\n",
      "Epoch 79/100 | Loss: 0.8062 | Test Acc: 49.38%\n",
      "Epoch 80/100 | Loss: 0.8282 | Test Acc: 51.85%\n",
      "Epoch 81/100 | Loss: 0.8020 | Test Acc: 56.79%\n",
      "Epoch 82/100 | Loss: 0.7316 | Test Acc: 55.56%\n",
      "Epoch 83/100 | Loss: 0.7565 | Test Acc: 55.56%\n",
      "Epoch 84/100 | Loss: 0.8565 | Test Acc: 48.15%\n",
      "Epoch 85/100 | Loss: 0.7809 | Test Acc: 54.32%\n",
      "Epoch 86/100 | Loss: 0.7383 | Test Acc: 56.79%\n",
      "Epoch 87/100 | Loss: 0.7356 | Test Acc: 53.09%\n",
      "Epoch 88/100 | Loss: 0.7375 | Test Acc: 55.56%\n",
      "Epoch 89/100 | Loss: 0.7250 | Test Acc: 49.38%\n",
      "Epoch 90/100 | Loss: 0.8499 | Test Acc: 46.91%\n",
      "Epoch 91/100 | Loss: 0.8380 | Test Acc: 55.56%\n",
      "Epoch 92/100 | Loss: 0.7206 | Test Acc: 53.09%\n",
      "Epoch 93/100 | Loss: 0.7414 | Test Acc: 48.15%\n",
      "Epoch 94/100 | Loss: 0.6775 | Test Acc: 51.85%\n",
      "Epoch 95/100 | Loss: 0.7075 | Test Acc: 50.62%\n",
      "Epoch 96/100 | Loss: 0.6864 | Test Acc: 53.09%\n",
      "Epoch 97/100 | Loss: 0.6933 | Test Acc: 56.79%\n",
      "Epoch 98/100 | Loss: 0.8084 | Test Acc: 49.38%\n",
      "Epoch 99/100 | Loss: 0.8302 | Test Acc: 53.09%\n",
      "Epoch 100/100 | Loss: 0.8231 | Test Acc: 50.62%\n",
      "Run 4 Best Test Accuracy: 59.26%\n",
      "\n",
      "=== Run 5/10 ===\n",
      "Epoch 1/100 | Loss: 3.4592 | Test Acc: 4.94%\n",
      "Epoch 2/100 | Loss: 2.6667 | Test Acc: 8.64%\n",
      "Epoch 3/100 | Loss: 2.3728 | Test Acc: 12.35%\n",
      "Epoch 4/100 | Loss: 2.1219 | Test Acc: 19.75%\n",
      "Epoch 5/100 | Loss: 1.9154 | Test Acc: 27.16%\n",
      "Epoch 6/100 | Loss: 1.7527 | Test Acc: 33.33%\n",
      "Epoch 7/100 | Loss: 1.5724 | Test Acc: 30.86%\n",
      "Epoch 8/100 | Loss: 1.4956 | Test Acc: 32.10%\n",
      "Epoch 9/100 | Loss: 1.4097 | Test Acc: 33.33%\n",
      "Epoch 10/100 | Loss: 1.3774 | Test Acc: 49.38%\n",
      "Epoch 11/100 | Loss: 1.3178 | Test Acc: 50.62%\n",
      "Epoch 12/100 | Loss: 1.2542 | Test Acc: 44.44%\n",
      "Epoch 13/100 | Loss: 1.2558 | Test Acc: 45.68%\n",
      "Epoch 14/100 | Loss: 1.3190 | Test Acc: 48.15%\n",
      "Epoch 15/100 | Loss: 1.1611 | Test Acc: 48.15%\n",
      "Epoch 16/100 | Loss: 1.2099 | Test Acc: 49.38%\n",
      "Epoch 17/100 | Loss: 1.1550 | Test Acc: 38.27%\n",
      "Epoch 18/100 | Loss: 1.2037 | Test Acc: 49.38%\n",
      "Epoch 19/100 | Loss: 1.1681 | Test Acc: 54.32%\n",
      "Epoch 20/100 | Loss: 1.1481 | Test Acc: 54.32%\n",
      "Epoch 21/100 | Loss: 1.0801 | Test Acc: 60.49%\n",
      "Epoch 22/100 | Loss: 1.0613 | Test Acc: 50.62%\n",
      "Epoch 23/100 | Loss: 0.9985 | Test Acc: 53.09%\n",
      "Epoch 24/100 | Loss: 1.0511 | Test Acc: 46.91%\n",
      "Epoch 25/100 | Loss: 1.0624 | Test Acc: 40.74%\n",
      "Epoch 26/100 | Loss: 1.0292 | Test Acc: 55.56%\n",
      "Epoch 27/100 | Loss: 0.9413 | Test Acc: 50.62%\n",
      "Epoch 28/100 | Loss: 0.9823 | Test Acc: 56.79%\n",
      "Epoch 29/100 | Loss: 0.9563 | Test Acc: 50.62%\n",
      "Epoch 30/100 | Loss: 0.9331 | Test Acc: 49.38%\n",
      "Epoch 31/100 | Loss: 1.0163 | Test Acc: 58.02%\n",
      "Epoch 32/100 | Loss: 1.0051 | Test Acc: 56.79%\n",
      "Epoch 33/100 | Loss: 0.9331 | Test Acc: 58.02%\n",
      "Epoch 34/100 | Loss: 0.9354 | Test Acc: 58.02%\n",
      "Epoch 35/100 | Loss: 0.9214 | Test Acc: 59.26%\n",
      "Epoch 36/100 | Loss: 1.0690 | Test Acc: 48.15%\n",
      "Epoch 37/100 | Loss: 0.9527 | Test Acc: 60.49%\n",
      "Epoch 38/100 | Loss: 0.8833 | Test Acc: 61.73%\n",
      "Epoch 39/100 | Loss: 0.8445 | Test Acc: 56.79%\n",
      "Epoch 40/100 | Loss: 0.8400 | Test Acc: 61.73%\n",
      "Epoch 41/100 | Loss: 0.8583 | Test Acc: 53.09%\n",
      "Epoch 42/100 | Loss: 0.8773 | Test Acc: 60.49%\n",
      "Epoch 43/100 | Loss: 0.8367 | Test Acc: 62.96%\n",
      "Epoch 44/100 | Loss: 0.8130 | Test Acc: 61.73%\n",
      "Epoch 45/100 | Loss: 0.7843 | Test Acc: 58.02%\n",
      "Epoch 46/100 | Loss: 0.7706 | Test Acc: 56.79%\n",
      "Epoch 47/100 | Loss: 0.8318 | Test Acc: 66.67%\n",
      "Epoch 48/100 | Loss: 0.7422 | Test Acc: 55.56%\n",
      "Epoch 49/100 | Loss: 0.7067 | Test Acc: 56.79%\n",
      "Epoch 50/100 | Loss: 0.7354 | Test Acc: 54.32%\n",
      "Epoch 51/100 | Loss: 0.7909 | Test Acc: 61.73%\n",
      "Epoch 52/100 | Loss: 0.7696 | Test Acc: 59.26%\n",
      "Epoch 53/100 | Loss: 0.7292 | Test Acc: 67.90%\n",
      "Epoch 54/100 | Loss: 0.6991 | Test Acc: 56.79%\n",
      "Epoch 55/100 | Loss: 0.8394 | Test Acc: 45.68%\n",
      "Epoch 56/100 | Loss: 0.7592 | Test Acc: 67.90%\n",
      "Epoch 57/100 | Loss: 0.7098 | Test Acc: 65.43%\n",
      "Epoch 58/100 | Loss: 0.6569 | Test Acc: 64.20%\n",
      "Epoch 59/100 | Loss: 0.6155 | Test Acc: 64.20%\n",
      "Epoch 60/100 | Loss: 0.7081 | Test Acc: 61.73%\n",
      "Epoch 61/100 | Loss: 0.7206 | Test Acc: 62.96%\n",
      "Epoch 62/100 | Loss: 0.7389 | Test Acc: 58.02%\n",
      "Epoch 63/100 | Loss: 0.6929 | Test Acc: 61.73%\n",
      "Epoch 64/100 | Loss: 0.6427 | Test Acc: 66.67%\n",
      "Epoch 65/100 | Loss: 0.6069 | Test Acc: 55.56%\n",
      "Epoch 66/100 | Loss: 0.6380 | Test Acc: 64.20%\n",
      "Epoch 67/100 | Loss: 0.6783 | Test Acc: 58.02%\n",
      "Epoch 68/100 | Loss: 0.6942 | Test Acc: 54.32%\n",
      "Epoch 69/100 | Loss: 0.6866 | Test Acc: 62.96%\n",
      "Epoch 70/100 | Loss: 0.7131 | Test Acc: 60.49%\n",
      "Epoch 71/100 | Loss: 0.7981 | Test Acc: 61.73%\n",
      "Epoch 72/100 | Loss: 0.6882 | Test Acc: 56.79%\n",
      "Epoch 73/100 | Loss: 0.6576 | Test Acc: 62.96%\n",
      "Epoch 74/100 | Loss: 0.6918 | Test Acc: 62.96%\n",
      "Epoch 75/100 | Loss: 0.6239 | Test Acc: 62.96%\n",
      "Epoch 76/100 | Loss: 0.6211 | Test Acc: 66.67%\n",
      "Epoch 77/100 | Loss: 0.6565 | Test Acc: 54.32%\n",
      "Epoch 78/100 | Loss: 0.6021 | Test Acc: 54.32%\n",
      "Epoch 79/100 | Loss: 0.6392 | Test Acc: 61.73%\n",
      "Epoch 80/100 | Loss: 0.5938 | Test Acc: 62.96%\n",
      "Epoch 81/100 | Loss: 0.5639 | Test Acc: 60.49%\n",
      "Epoch 82/100 | Loss: 0.6188 | Test Acc: 55.56%\n",
      "Epoch 83/100 | Loss: 0.5666 | Test Acc: 54.32%\n",
      "Epoch 84/100 | Loss: 0.6035 | Test Acc: 58.02%\n",
      "Epoch 85/100 | Loss: 0.6242 | Test Acc: 64.20%\n",
      "Epoch 86/100 | Loss: 0.5479 | Test Acc: 51.85%\n",
      "Epoch 87/100 | Loss: 0.5491 | Test Acc: 59.26%\n",
      "Epoch 88/100 | Loss: 0.5738 | Test Acc: 60.49%\n",
      "Epoch 89/100 | Loss: 0.5358 | Test Acc: 69.14%\n",
      "Epoch 90/100 | Loss: 0.6704 | Test Acc: 56.79%\n",
      "Epoch 91/100 | Loss: 0.6366 | Test Acc: 64.20%\n",
      "Epoch 92/100 | Loss: 0.5882 | Test Acc: 55.56%\n",
      "Epoch 93/100 | Loss: 0.5694 | Test Acc: 59.26%\n",
      "Epoch 94/100 | Loss: 0.5062 | Test Acc: 60.49%\n",
      "Epoch 95/100 | Loss: 0.5392 | Test Acc: 53.09%\n",
      "Epoch 96/100 | Loss: 0.6019 | Test Acc: 64.20%\n",
      "Epoch 97/100 | Loss: 0.6156 | Test Acc: 55.56%\n",
      "Epoch 98/100 | Loss: 0.6027 | Test Acc: 64.20%\n",
      "Epoch 99/100 | Loss: 0.5759 | Test Acc: 60.49%\n",
      "Epoch 100/100 | Loss: 0.5666 | Test Acc: 64.20%\n",
      "Run 5 Best Test Accuracy: 69.14%\n",
      "\n",
      "=== Run 6/10 ===\n",
      "Epoch 1/100 | Loss: 3.8301 | Test Acc: 3.70%\n",
      "Epoch 2/100 | Loss: 3.2186 | Test Acc: 6.17%\n",
      "Epoch 3/100 | Loss: 2.8737 | Test Acc: 8.64%\n",
      "Epoch 4/100 | Loss: 2.8390 | Test Acc: 9.88%\n",
      "Epoch 5/100 | Loss: 2.8755 | Test Acc: 9.88%\n",
      "Epoch 6/100 | Loss: 2.9986 | Test Acc: 6.17%\n",
      "Epoch 7/100 | Loss: 2.9081 | Test Acc: 4.94%\n",
      "Epoch 8/100 | Loss: 2.7862 | Test Acc: 12.35%\n",
      "Epoch 9/100 | Loss: 2.7693 | Test Acc: 8.64%\n",
      "Epoch 10/100 | Loss: 2.7758 | Test Acc: 4.94%\n",
      "Epoch 11/100 | Loss: 2.7008 | Test Acc: 8.64%\n",
      "Epoch 12/100 | Loss: 2.7435 | Test Acc: 13.58%\n",
      "Epoch 13/100 | Loss: 2.7177 | Test Acc: 16.05%\n",
      "Epoch 14/100 | Loss: 2.5945 | Test Acc: 13.58%\n",
      "Epoch 15/100 | Loss: 2.5091 | Test Acc: 13.58%\n",
      "Epoch 16/100 | Loss: 2.5322 | Test Acc: 12.35%\n",
      "Epoch 17/100 | Loss: 2.4842 | Test Acc: 17.28%\n",
      "Epoch 18/100 | Loss: 2.7343 | Test Acc: 12.35%\n",
      "Epoch 19/100 | Loss: 2.6313 | Test Acc: 12.35%\n",
      "Epoch 20/100 | Loss: 2.5080 | Test Acc: 18.52%\n",
      "Epoch 21/100 | Loss: 2.4567 | Test Acc: 16.05%\n",
      "Epoch 22/100 | Loss: 2.4314 | Test Acc: 19.75%\n",
      "Epoch 23/100 | Loss: 2.4038 | Test Acc: 13.58%\n",
      "Epoch 24/100 | Loss: 2.3627 | Test Acc: 19.75%\n",
      "Epoch 25/100 | Loss: 2.3664 | Test Acc: 13.58%\n",
      "Epoch 26/100 | Loss: 2.3605 | Test Acc: 16.05%\n",
      "Epoch 27/100 | Loss: 2.4307 | Test Acc: 11.11%\n",
      "Epoch 28/100 | Loss: 2.3891 | Test Acc: 18.52%\n",
      "Epoch 29/100 | Loss: 2.3380 | Test Acc: 22.22%\n",
      "Epoch 30/100 | Loss: 2.3445 | Test Acc: 19.75%\n",
      "Epoch 31/100 | Loss: 2.3088 | Test Acc: 17.28%\n",
      "Epoch 32/100 | Loss: 2.2988 | Test Acc: 24.69%\n",
      "Epoch 33/100 | Loss: 2.3138 | Test Acc: 19.75%\n",
      "Epoch 34/100 | Loss: 2.2796 | Test Acc: 19.75%\n",
      "Epoch 35/100 | Loss: 2.2216 | Test Acc: 24.69%\n",
      "Epoch 36/100 | Loss: 2.1941 | Test Acc: 18.52%\n",
      "Epoch 37/100 | Loss: 2.2193 | Test Acc: 22.22%\n",
      "Epoch 38/100 | Loss: 2.1662 | Test Acc: 25.93%\n",
      "Epoch 39/100 | Loss: 2.1219 | Test Acc: 23.46%\n",
      "Epoch 40/100 | Loss: 2.2039 | Test Acc: 20.99%\n",
      "Epoch 41/100 | Loss: 2.3152 | Test Acc: 14.81%\n",
      "Epoch 42/100 | Loss: 2.2970 | Test Acc: 14.81%\n",
      "Epoch 43/100 | Loss: 2.2385 | Test Acc: 19.75%\n",
      "Epoch 44/100 | Loss: 2.1187 | Test Acc: 22.22%\n",
      "Epoch 45/100 | Loss: 2.1859 | Test Acc: 20.99%\n",
      "Epoch 46/100 | Loss: 2.0775 | Test Acc: 24.69%\n",
      "Epoch 47/100 | Loss: 1.9234 | Test Acc: 32.10%\n",
      "Epoch 48/100 | Loss: 1.9027 | Test Acc: 20.99%\n",
      "Epoch 49/100 | Loss: 1.9488 | Test Acc: 27.16%\n",
      "Epoch 50/100 | Loss: 1.8234 | Test Acc: 29.63%\n",
      "Epoch 51/100 | Loss: 1.7925 | Test Acc: 35.80%\n",
      "Epoch 52/100 | Loss: 1.8000 | Test Acc: 25.93%\n",
      "Epoch 53/100 | Loss: 1.8402 | Test Acc: 32.10%\n",
      "Epoch 54/100 | Loss: 1.7799 | Test Acc: 32.10%\n",
      "Epoch 55/100 | Loss: 1.7233 | Test Acc: 37.04%\n",
      "Epoch 56/100 | Loss: 1.6960 | Test Acc: 27.16%\n",
      "Epoch 57/100 | Loss: 1.6367 | Test Acc: 38.27%\n",
      "Epoch 58/100 | Loss: 1.6175 | Test Acc: 37.04%\n",
      "Epoch 59/100 | Loss: 1.6034 | Test Acc: 37.04%\n",
      "Epoch 60/100 | Loss: 1.6963 | Test Acc: 37.04%\n",
      "Epoch 61/100 | Loss: 1.6554 | Test Acc: 29.63%\n",
      "Epoch 62/100 | Loss: 1.7690 | Test Acc: 27.16%\n",
      "Epoch 63/100 | Loss: 1.6312 | Test Acc: 37.04%\n",
      "Epoch 64/100 | Loss: 1.5973 | Test Acc: 34.57%\n",
      "Epoch 65/100 | Loss: 1.6123 | Test Acc: 28.40%\n",
      "Epoch 66/100 | Loss: 1.6174 | Test Acc: 39.51%\n",
      "Epoch 67/100 | Loss: 1.6273 | Test Acc: 29.63%\n",
      "Epoch 68/100 | Loss: 1.6062 | Test Acc: 32.10%\n",
      "Epoch 69/100 | Loss: 1.5207 | Test Acc: 40.74%\n",
      "Epoch 70/100 | Loss: 1.6681 | Test Acc: 38.27%\n",
      "Epoch 71/100 | Loss: 1.6985 | Test Acc: 33.33%\n",
      "Epoch 72/100 | Loss: 1.6863 | Test Acc: 27.16%\n",
      "Epoch 73/100 | Loss: 1.6455 | Test Acc: 34.57%\n",
      "Epoch 74/100 | Loss: 1.6103 | Test Acc: 30.86%\n",
      "Epoch 75/100 | Loss: 1.6072 | Test Acc: 33.33%\n",
      "Epoch 76/100 | Loss: 1.6026 | Test Acc: 39.51%\n",
      "Epoch 77/100 | Loss: 1.5175 | Test Acc: 38.27%\n",
      "Epoch 78/100 | Loss: 1.4936 | Test Acc: 43.21%\n",
      "Epoch 79/100 | Loss: 1.4726 | Test Acc: 37.04%\n",
      "Epoch 80/100 | Loss: 1.5034 | Test Acc: 33.33%\n",
      "Epoch 81/100 | Loss: 1.4041 | Test Acc: 39.51%\n",
      "Epoch 82/100 | Loss: 1.4772 | Test Acc: 38.27%\n",
      "Epoch 83/100 | Loss: 1.4682 | Test Acc: 28.40%\n",
      "Epoch 84/100 | Loss: 1.4467 | Test Acc: 40.74%\n",
      "Epoch 85/100 | Loss: 1.4235 | Test Acc: 35.80%\n",
      "Epoch 86/100 | Loss: 1.5600 | Test Acc: 33.33%\n",
      "Epoch 87/100 | Loss: 1.4667 | Test Acc: 34.57%\n",
      "Epoch 88/100 | Loss: 1.4458 | Test Acc: 29.63%\n",
      "Epoch 89/100 | Loss: 1.4372 | Test Acc: 38.27%\n",
      "Epoch 90/100 | Loss: 1.4442 | Test Acc: 43.21%\n",
      "Epoch 91/100 | Loss: 1.5911 | Test Acc: 39.51%\n",
      "Epoch 92/100 | Loss: 1.5108 | Test Acc: 39.51%\n",
      "Epoch 93/100 | Loss: 1.4303 | Test Acc: 38.27%\n",
      "Epoch 94/100 | Loss: 1.3503 | Test Acc: 43.21%\n",
      "Epoch 95/100 | Loss: 1.3344 | Test Acc: 41.98%\n",
      "Epoch 96/100 | Loss: 1.3500 | Test Acc: 41.98%\n",
      "Epoch 97/100 | Loss: 1.3676 | Test Acc: 34.57%\n",
      "Epoch 98/100 | Loss: 1.3935 | Test Acc: 29.63%\n",
      "Epoch 99/100 | Loss: 1.5080 | Test Acc: 29.63%\n",
      "Epoch 100/100 | Loss: 1.3797 | Test Acc: 39.51%\n",
      "Run 6 Best Test Accuracy: 43.21%\n",
      "\n",
      "=== Run 7/10 ===\n",
      "Epoch 1/100 | Loss: 3.7054 | Test Acc: 11.11%\n",
      "Epoch 2/100 | Loss: 2.6255 | Test Acc: 19.75%\n",
      "Epoch 3/100 | Loss: 2.1835 | Test Acc: 17.28%\n",
      "Epoch 4/100 | Loss: 2.0726 | Test Acc: 23.46%\n",
      "Epoch 5/100 | Loss: 1.8936 | Test Acc: 30.86%\n",
      "Epoch 6/100 | Loss: 1.8241 | Test Acc: 29.63%\n",
      "Epoch 7/100 | Loss: 1.6203 | Test Acc: 32.10%\n",
      "Epoch 8/100 | Loss: 1.6200 | Test Acc: 38.27%\n",
      "Epoch 9/100 | Loss: 1.4999 | Test Acc: 41.98%\n",
      "Epoch 10/100 | Loss: 1.4635 | Test Acc: 38.27%\n",
      "Epoch 11/100 | Loss: 1.4451 | Test Acc: 39.51%\n",
      "Epoch 12/100 | Loss: 1.4315 | Test Acc: 40.74%\n",
      "Epoch 13/100 | Loss: 1.4995 | Test Acc: 40.74%\n",
      "Epoch 14/100 | Loss: 1.4434 | Test Acc: 53.09%\n",
      "Epoch 15/100 | Loss: 1.3174 | Test Acc: 45.68%\n",
      "Epoch 16/100 | Loss: 1.2994 | Test Acc: 41.98%\n",
      "Epoch 17/100 | Loss: 1.6415 | Test Acc: 33.33%\n",
      "Epoch 18/100 | Loss: 1.4789 | Test Acc: 45.68%\n",
      "Epoch 19/100 | Loss: 1.4581 | Test Acc: 41.98%\n",
      "Epoch 20/100 | Loss: 1.4252 | Test Acc: 43.21%\n",
      "Epoch 21/100 | Loss: 1.3558 | Test Acc: 44.44%\n",
      "Epoch 22/100 | Loss: 1.2801 | Test Acc: 48.15%\n",
      "Epoch 23/100 | Loss: 1.2251 | Test Acc: 48.15%\n",
      "Epoch 24/100 | Loss: 1.2441 | Test Acc: 48.15%\n",
      "Epoch 25/100 | Loss: 1.2322 | Test Acc: 48.15%\n",
      "Epoch 26/100 | Loss: 1.1601 | Test Acc: 45.68%\n",
      "Epoch 27/100 | Loss: 1.1373 | Test Acc: 46.91%\n",
      "Epoch 28/100 | Loss: 1.0974 | Test Acc: 43.21%\n",
      "Epoch 29/100 | Loss: 1.1048 | Test Acc: 50.62%\n",
      "Epoch 30/100 | Loss: 1.2162 | Test Acc: 44.44%\n",
      "Epoch 31/100 | Loss: 1.2018 | Test Acc: 46.91%\n",
      "Epoch 32/100 | Loss: 1.1673 | Test Acc: 53.09%\n",
      "Epoch 33/100 | Loss: 1.0448 | Test Acc: 50.62%\n",
      "Epoch 34/100 | Loss: 1.0308 | Test Acc: 55.56%\n",
      "Epoch 35/100 | Loss: 1.0883 | Test Acc: 51.85%\n",
      "Epoch 36/100 | Loss: 1.0747 | Test Acc: 54.32%\n",
      "Epoch 37/100 | Loss: 0.9989 | Test Acc: 59.26%\n",
      "Epoch 38/100 | Loss: 0.9893 | Test Acc: 53.09%\n",
      "Epoch 39/100 | Loss: 0.9768 | Test Acc: 54.32%\n",
      "Epoch 40/100 | Loss: 0.9789 | Test Acc: 44.44%\n",
      "Epoch 41/100 | Loss: 0.9554 | Test Acc: 53.09%\n",
      "Epoch 42/100 | Loss: 0.9032 | Test Acc: 51.85%\n",
      "Epoch 43/100 | Loss: 0.9267 | Test Acc: 58.02%\n",
      "Epoch 44/100 | Loss: 0.8897 | Test Acc: 58.02%\n",
      "Epoch 45/100 | Loss: 0.9297 | Test Acc: 51.85%\n",
      "Epoch 46/100 | Loss: 0.9080 | Test Acc: 53.09%\n",
      "Epoch 47/100 | Loss: 0.8710 | Test Acc: 55.56%\n",
      "Epoch 48/100 | Loss: 0.8811 | Test Acc: 53.09%\n",
      "Epoch 49/100 | Loss: 0.8628 | Test Acc: 58.02%\n",
      "Epoch 50/100 | Loss: 0.8321 | Test Acc: 66.67%\n",
      "Epoch 51/100 | Loss: 0.8072 | Test Acc: 55.56%\n",
      "Epoch 52/100 | Loss: 0.8020 | Test Acc: 58.02%\n",
      "Epoch 53/100 | Loss: 0.8181 | Test Acc: 58.02%\n",
      "Epoch 54/100 | Loss: 0.8402 | Test Acc: 55.56%\n",
      "Epoch 55/100 | Loss: 0.8668 | Test Acc: 55.56%\n",
      "Epoch 56/100 | Loss: 0.8035 | Test Acc: 53.09%\n",
      "Epoch 57/100 | Loss: 0.7916 | Test Acc: 59.26%\n",
      "Epoch 58/100 | Loss: 0.7638 | Test Acc: 58.02%\n",
      "Epoch 59/100 | Loss: 0.7502 | Test Acc: 61.73%\n",
      "Epoch 60/100 | Loss: 0.7841 | Test Acc: 55.56%\n",
      "Epoch 61/100 | Loss: 0.8574 | Test Acc: 61.73%\n",
      "Epoch 62/100 | Loss: 0.7935 | Test Acc: 56.79%\n",
      "Epoch 63/100 | Loss: 0.7462 | Test Acc: 46.91%\n",
      "Epoch 64/100 | Loss: 0.7743 | Test Acc: 56.79%\n",
      "Epoch 65/100 | Loss: 0.7857 | Test Acc: 51.85%\n",
      "Epoch 66/100 | Loss: 0.9270 | Test Acc: 53.09%\n",
      "Epoch 67/100 | Loss: 0.8561 | Test Acc: 64.20%\n",
      "Epoch 68/100 | Loss: 0.7630 | Test Acc: 64.20%\n",
      "Epoch 69/100 | Loss: 0.7241 | Test Acc: 55.56%\n",
      "Epoch 70/100 | Loss: 0.7464 | Test Acc: 64.20%\n",
      "Epoch 71/100 | Loss: 0.7898 | Test Acc: 53.09%\n",
      "Epoch 72/100 | Loss: 0.8821 | Test Acc: 53.09%\n",
      "Epoch 73/100 | Loss: 0.8634 | Test Acc: 54.32%\n",
      "Epoch 74/100 | Loss: 0.8554 | Test Acc: 60.49%\n",
      "Epoch 75/100 | Loss: 0.7749 | Test Acc: 59.26%\n",
      "Epoch 76/100 | Loss: 0.7771 | Test Acc: 55.56%\n",
      "Epoch 77/100 | Loss: 0.7959 | Test Acc: 53.09%\n",
      "Epoch 78/100 | Loss: 0.7328 | Test Acc: 55.56%\n",
      "Epoch 79/100 | Loss: 0.7346 | Test Acc: 46.91%\n",
      "Epoch 80/100 | Loss: 0.7473 | Test Acc: 55.56%\n",
      "Epoch 81/100 | Loss: 0.8336 | Test Acc: 55.56%\n",
      "Epoch 82/100 | Loss: 0.7580 | Test Acc: 58.02%\n",
      "Epoch 83/100 | Loss: 0.7809 | Test Acc: 55.56%\n",
      "Epoch 84/100 | Loss: 0.8475 | Test Acc: 55.56%\n",
      "Epoch 85/100 | Loss: 0.7894 | Test Acc: 54.32%\n",
      "Epoch 86/100 | Loss: 0.7837 | Test Acc: 58.02%\n",
      "Epoch 87/100 | Loss: 0.8055 | Test Acc: 55.56%\n",
      "Epoch 88/100 | Loss: 0.7391 | Test Acc: 54.32%\n",
      "Epoch 89/100 | Loss: 0.7344 | Test Acc: 53.09%\n",
      "Epoch 90/100 | Loss: 0.8180 | Test Acc: 53.09%\n",
      "Epoch 91/100 | Loss: 0.7136 | Test Acc: 53.09%\n",
      "Epoch 92/100 | Loss: 0.6792 | Test Acc: 49.38%\n",
      "Epoch 93/100 | Loss: 0.6896 | Test Acc: 44.44%\n",
      "Epoch 94/100 | Loss: 0.7056 | Test Acc: 56.79%\n",
      "Epoch 95/100 | Loss: 0.6550 | Test Acc: 58.02%\n",
      "Epoch 96/100 | Loss: 0.6797 | Test Acc: 53.09%\n",
      "Epoch 97/100 | Loss: 0.6423 | Test Acc: 58.02%\n",
      "Epoch 98/100 | Loss: 0.6543 | Test Acc: 55.56%\n",
      "Epoch 99/100 | Loss: 0.6567 | Test Acc: 48.15%\n",
      "Epoch 100/100 | Loss: 0.7186 | Test Acc: 53.09%\n",
      "Run 7 Best Test Accuracy: 66.67%\n",
      "\n",
      "=== Run 8/10 ===\n",
      "Epoch 1/100 | Loss: 3.5516 | Test Acc: 13.58%\n",
      "Epoch 2/100 | Loss: 2.7559 | Test Acc: 9.88%\n",
      "Epoch 3/100 | Loss: 2.4882 | Test Acc: 17.28%\n",
      "Epoch 4/100 | Loss: 2.2745 | Test Acc: 17.28%\n",
      "Epoch 5/100 | Loss: 2.2086 | Test Acc: 20.99%\n",
      "Epoch 6/100 | Loss: 2.0044 | Test Acc: 24.69%\n",
      "Epoch 7/100 | Loss: 1.9881 | Test Acc: 22.22%\n",
      "Epoch 8/100 | Loss: 1.8463 | Test Acc: 29.63%\n",
      "Epoch 9/100 | Loss: 1.8292 | Test Acc: 27.16%\n",
      "Epoch 10/100 | Loss: 1.7645 | Test Acc: 34.57%\n",
      "Epoch 11/100 | Loss: 1.6665 | Test Acc: 28.40%\n",
      "Epoch 12/100 | Loss: 1.7526 | Test Acc: 34.57%\n",
      "Epoch 13/100 | Loss: 1.6855 | Test Acc: 25.93%\n",
      "Epoch 14/100 | Loss: 1.8085 | Test Acc: 33.33%\n",
      "Epoch 15/100 | Loss: 1.6921 | Test Acc: 32.10%\n",
      "Epoch 16/100 | Loss: 1.6111 | Test Acc: 40.74%\n",
      "Epoch 17/100 | Loss: 1.6625 | Test Acc: 37.04%\n",
      "Epoch 18/100 | Loss: 1.6232 | Test Acc: 38.27%\n",
      "Epoch 19/100 | Loss: 1.5634 | Test Acc: 40.74%\n",
      "Epoch 20/100 | Loss: 1.4545 | Test Acc: 38.27%\n",
      "Epoch 21/100 | Loss: 1.5093 | Test Acc: 39.51%\n",
      "Epoch 22/100 | Loss: 1.3004 | Test Acc: 43.21%\n",
      "Epoch 23/100 | Loss: 1.4361 | Test Acc: 45.68%\n",
      "Epoch 24/100 | Loss: 1.2965 | Test Acc: 51.85%\n",
      "Epoch 25/100 | Loss: 1.2215 | Test Acc: 49.38%\n",
      "Epoch 26/100 | Loss: 1.3192 | Test Acc: 41.98%\n",
      "Epoch 27/100 | Loss: 1.2474 | Test Acc: 45.68%\n",
      "Epoch 28/100 | Loss: 1.2928 | Test Acc: 41.98%\n",
      "Epoch 29/100 | Loss: 1.3732 | Test Acc: 40.74%\n",
      "Epoch 30/100 | Loss: 1.2910 | Test Acc: 53.09%\n",
      "Epoch 31/100 | Loss: 1.2633 | Test Acc: 49.38%\n",
      "Epoch 32/100 | Loss: 1.2103 | Test Acc: 54.32%\n",
      "Epoch 33/100 | Loss: 1.1888 | Test Acc: 55.56%\n",
      "Epoch 34/100 | Loss: 1.2053 | Test Acc: 50.62%\n",
      "Epoch 35/100 | Loss: 1.1805 | Test Acc: 48.15%\n",
      "Epoch 36/100 | Loss: 1.2907 | Test Acc: 49.38%\n",
      "Epoch 37/100 | Loss: 1.2805 | Test Acc: 43.21%\n",
      "Epoch 38/100 | Loss: 1.2205 | Test Acc: 43.21%\n",
      "Epoch 39/100 | Loss: 1.1591 | Test Acc: 46.91%\n",
      "Epoch 40/100 | Loss: 1.1227 | Test Acc: 46.91%\n",
      "Epoch 41/100 | Loss: 1.1063 | Test Acc: 50.62%\n",
      "Epoch 42/100 | Loss: 1.1241 | Test Acc: 51.85%\n",
      "Epoch 43/100 | Loss: 1.0217 | Test Acc: 55.56%\n",
      "Epoch 44/100 | Loss: 1.0161 | Test Acc: 50.62%\n",
      "Epoch 45/100 | Loss: 1.0040 | Test Acc: 59.26%\n",
      "Epoch 46/100 | Loss: 0.9738 | Test Acc: 51.85%\n",
      "Epoch 47/100 | Loss: 1.0112 | Test Acc: 49.38%\n",
      "Epoch 48/100 | Loss: 0.9453 | Test Acc: 58.02%\n",
      "Epoch 49/100 | Loss: 0.9055 | Test Acc: 58.02%\n",
      "Epoch 50/100 | Loss: 0.9652 | Test Acc: 44.44%\n",
      "Epoch 51/100 | Loss: 1.0266 | Test Acc: 51.85%\n",
      "Epoch 52/100 | Loss: 0.9537 | Test Acc: 54.32%\n",
      "Epoch 53/100 | Loss: 0.9033 | Test Acc: 53.09%\n",
      "Epoch 54/100 | Loss: 0.8585 | Test Acc: 58.02%\n",
      "Epoch 55/100 | Loss: 0.9182 | Test Acc: 59.26%\n",
      "Epoch 56/100 | Loss: 0.9223 | Test Acc: 59.26%\n",
      "Epoch 57/100 | Loss: 0.9186 | Test Acc: 53.09%\n",
      "Epoch 58/100 | Loss: 0.9570 | Test Acc: 58.02%\n",
      "Epoch 59/100 | Loss: 0.9090 | Test Acc: 56.79%\n",
      "Epoch 60/100 | Loss: 0.9539 | Test Acc: 51.85%\n",
      "Epoch 61/100 | Loss: 1.0569 | Test Acc: 54.32%\n",
      "Epoch 62/100 | Loss: 1.0093 | Test Acc: 55.56%\n",
      "Epoch 63/100 | Loss: 1.0146 | Test Acc: 51.85%\n",
      "Epoch 64/100 | Loss: 0.9146 | Test Acc: 60.49%\n",
      "Epoch 65/100 | Loss: 0.9219 | Test Acc: 53.09%\n",
      "Epoch 66/100 | Loss: 0.8711 | Test Acc: 62.96%\n",
      "Epoch 67/100 | Loss: 0.8793 | Test Acc: 50.62%\n",
      "Epoch 68/100 | Loss: 0.9195 | Test Acc: 58.02%\n",
      "Epoch 69/100 | Loss: 1.0381 | Test Acc: 53.09%\n",
      "Epoch 70/100 | Loss: 0.9566 | Test Acc: 54.32%\n",
      "Epoch 71/100 | Loss: 0.8986 | Test Acc: 54.32%\n",
      "Epoch 72/100 | Loss: 0.9610 | Test Acc: 56.79%\n",
      "Epoch 73/100 | Loss: 0.9751 | Test Acc: 58.02%\n",
      "Epoch 74/100 | Loss: 0.9522 | Test Acc: 55.56%\n",
      "Epoch 75/100 | Loss: 0.8639 | Test Acc: 60.49%\n",
      "Epoch 76/100 | Loss: 0.9332 | Test Acc: 58.02%\n",
      "Epoch 77/100 | Loss: 0.8479 | Test Acc: 58.02%\n",
      "Epoch 78/100 | Loss: 0.8819 | Test Acc: 51.85%\n",
      "Epoch 79/100 | Loss: 0.8445 | Test Acc: 55.56%\n",
      "Epoch 80/100 | Loss: 0.8295 | Test Acc: 56.79%\n",
      "Epoch 81/100 | Loss: 0.8732 | Test Acc: 61.73%\n",
      "Epoch 82/100 | Loss: 0.8044 | Test Acc: 61.73%\n",
      "Epoch 83/100 | Loss: 0.8150 | Test Acc: 56.79%\n",
      "Epoch 84/100 | Loss: 0.8298 | Test Acc: 56.79%\n",
      "Epoch 85/100 | Loss: 0.8197 | Test Acc: 60.49%\n",
      "Epoch 86/100 | Loss: 0.7832 | Test Acc: 59.26%\n",
      "Epoch 87/100 | Loss: 0.8624 | Test Acc: 58.02%\n",
      "Epoch 88/100 | Loss: 0.8894 | Test Acc: 58.02%\n",
      "Epoch 89/100 | Loss: 0.8874 | Test Acc: 56.79%\n",
      "Epoch 90/100 | Loss: 0.8524 | Test Acc: 55.56%\n",
      "Epoch 91/100 | Loss: 0.8539 | Test Acc: 56.79%\n",
      "Epoch 92/100 | Loss: 0.9733 | Test Acc: 50.62%\n",
      "Epoch 93/100 | Loss: 0.9553 | Test Acc: 53.09%\n",
      "Epoch 94/100 | Loss: 0.8807 | Test Acc: 59.26%\n",
      "Epoch 95/100 | Loss: 0.8167 | Test Acc: 54.32%\n",
      "Epoch 96/100 | Loss: 0.8292 | Test Acc: 49.38%\n",
      "Epoch 97/100 | Loss: 0.8212 | Test Acc: 56.79%\n",
      "Epoch 98/100 | Loss: 0.8453 | Test Acc: 59.26%\n",
      "Epoch 99/100 | Loss: 0.8190 | Test Acc: 54.32%\n",
      "Epoch 100/100 | Loss: 0.8415 | Test Acc: 55.56%\n",
      "Run 8 Best Test Accuracy: 62.96%\n",
      "\n",
      "=== Run 9/10 ===\n",
      "Epoch 1/100 | Loss: 3.4375 | Test Acc: 9.88%\n",
      "Epoch 2/100 | Loss: 2.6113 | Test Acc: 20.99%\n",
      "Epoch 3/100 | Loss: 2.1751 | Test Acc: 25.93%\n",
      "Epoch 4/100 | Loss: 1.8796 | Test Acc: 29.63%\n",
      "Epoch 5/100 | Loss: 1.6110 | Test Acc: 28.40%\n",
      "Epoch 6/100 | Loss: 1.5349 | Test Acc: 32.10%\n",
      "Epoch 7/100 | Loss: 1.4776 | Test Acc: 39.51%\n",
      "Epoch 8/100 | Loss: 1.3571 | Test Acc: 35.80%\n",
      "Epoch 9/100 | Loss: 1.3439 | Test Acc: 37.04%\n",
      "Epoch 10/100 | Loss: 1.3020 | Test Acc: 40.74%\n",
      "Epoch 11/100 | Loss: 1.2754 | Test Acc: 43.21%\n",
      "Epoch 12/100 | Loss: 1.2133 | Test Acc: 44.44%\n",
      "Epoch 13/100 | Loss: 1.1800 | Test Acc: 45.68%\n",
      "Epoch 14/100 | Loss: 1.0711 | Test Acc: 55.56%\n",
      "Epoch 15/100 | Loss: 1.0255 | Test Acc: 58.02%\n",
      "Epoch 16/100 | Loss: 0.9822 | Test Acc: 50.62%\n",
      "Epoch 17/100 | Loss: 0.9725 | Test Acc: 49.38%\n",
      "Epoch 18/100 | Loss: 0.9248 | Test Acc: 53.09%\n",
      "Epoch 19/100 | Loss: 1.0028 | Test Acc: 51.85%\n",
      "Epoch 20/100 | Loss: 0.9354 | Test Acc: 48.15%\n",
      "Epoch 21/100 | Loss: 0.8973 | Test Acc: 46.91%\n",
      "Epoch 22/100 | Loss: 0.8920 | Test Acc: 45.68%\n",
      "Epoch 23/100 | Loss: 0.9117 | Test Acc: 49.38%\n",
      "Epoch 24/100 | Loss: 0.8261 | Test Acc: 49.38%\n",
      "Epoch 25/100 | Loss: 0.8105 | Test Acc: 56.79%\n",
      "Epoch 26/100 | Loss: 0.7874 | Test Acc: 50.62%\n",
      "Epoch 27/100 | Loss: 0.8345 | Test Acc: 51.85%\n",
      "Epoch 28/100 | Loss: 0.7498 | Test Acc: 53.09%\n",
      "Epoch 29/100 | Loss: 0.8197 | Test Acc: 50.62%\n",
      "Epoch 30/100 | Loss: 0.8199 | Test Acc: 49.38%\n",
      "Epoch 31/100 | Loss: 0.7832 | Test Acc: 59.26%\n",
      "Epoch 32/100 | Loss: 0.8607 | Test Acc: 49.38%\n",
      "Epoch 33/100 | Loss: 0.8978 | Test Acc: 48.15%\n",
      "Epoch 34/100 | Loss: 0.7630 | Test Acc: 48.15%\n",
      "Epoch 35/100 | Loss: 0.6788 | Test Acc: 51.85%\n",
      "Epoch 36/100 | Loss: 0.6943 | Test Acc: 51.85%\n",
      "Epoch 37/100 | Loss: 0.8035 | Test Acc: 56.79%\n",
      "Epoch 38/100 | Loss: 0.7476 | Test Acc: 54.32%\n",
      "Epoch 39/100 | Loss: 0.6413 | Test Acc: 55.56%\n",
      "Epoch 40/100 | Loss: 0.7049 | Test Acc: 51.85%\n",
      "Epoch 41/100 | Loss: 0.6494 | Test Acc: 58.02%\n",
      "Epoch 42/100 | Loss: 0.6694 | Test Acc: 50.62%\n",
      "Epoch 43/100 | Loss: 0.6888 | Test Acc: 44.44%\n",
      "Epoch 44/100 | Loss: 0.6127 | Test Acc: 55.56%\n",
      "Epoch 45/100 | Loss: 0.6304 | Test Acc: 51.85%\n",
      "Epoch 46/100 | Loss: 0.5888 | Test Acc: 50.62%\n",
      "Epoch 47/100 | Loss: 0.6832 | Test Acc: 62.96%\n",
      "Epoch 48/100 | Loss: 0.6276 | Test Acc: 53.09%\n",
      "Epoch 49/100 | Loss: 0.6215 | Test Acc: 55.56%\n",
      "Epoch 50/100 | Loss: 0.6250 | Test Acc: 53.09%\n",
      "Epoch 51/100 | Loss: 0.6404 | Test Acc: 44.44%\n",
      "Epoch 52/100 | Loss: 0.6085 | Test Acc: 55.56%\n",
      "Epoch 53/100 | Loss: 0.5787 | Test Acc: 50.62%\n",
      "Epoch 54/100 | Loss: 0.5699 | Test Acc: 54.32%\n",
      "Epoch 55/100 | Loss: 0.5927 | Test Acc: 50.62%\n",
      "Epoch 56/100 | Loss: 0.5712 | Test Acc: 50.62%\n",
      "Epoch 57/100 | Loss: 0.5584 | Test Acc: 49.38%\n",
      "Epoch 58/100 | Loss: 0.5306 | Test Acc: 61.73%\n",
      "Epoch 59/100 | Loss: 0.5198 | Test Acc: 55.56%\n",
      "Epoch 60/100 | Loss: 0.5160 | Test Acc: 50.62%\n",
      "Epoch 61/100 | Loss: 0.5579 | Test Acc: 60.49%\n",
      "Epoch 62/100 | Loss: 0.5167 | Test Acc: 50.62%\n",
      "Epoch 63/100 | Loss: 0.6240 | Test Acc: 56.79%\n",
      "Epoch 64/100 | Loss: 0.5732 | Test Acc: 62.96%\n",
      "Epoch 65/100 | Loss: 0.5301 | Test Acc: 54.32%\n",
      "Epoch 66/100 | Loss: 0.4998 | Test Acc: 55.56%\n",
      "Epoch 67/100 | Loss: 0.5289 | Test Acc: 61.73%\n",
      "Epoch 68/100 | Loss: 0.5074 | Test Acc: 62.96%\n",
      "Epoch 69/100 | Loss: 0.5194 | Test Acc: 56.79%\n",
      "Epoch 70/100 | Loss: 0.5051 | Test Acc: 56.79%\n",
      "Epoch 71/100 | Loss: 0.5212 | Test Acc: 60.49%\n",
      "Epoch 72/100 | Loss: 0.4968 | Test Acc: 58.02%\n",
      "Epoch 73/100 | Loss: 0.4836 | Test Acc: 64.20%\n",
      "Epoch 74/100 | Loss: 0.4370 | Test Acc: 64.20%\n",
      "Epoch 75/100 | Loss: 0.4396 | Test Acc: 65.43%\n",
      "Epoch 76/100 | Loss: 0.3912 | Test Acc: 56.79%\n",
      "Epoch 77/100 | Loss: 0.4524 | Test Acc: 61.73%\n",
      "Epoch 78/100 | Loss: 0.4035 | Test Acc: 58.02%\n",
      "Epoch 79/100 | Loss: 0.4488 | Test Acc: 59.26%\n",
      "Epoch 80/100 | Loss: 0.4255 | Test Acc: 55.56%\n",
      "Epoch 81/100 | Loss: 0.4316 | Test Acc: 61.73%\n",
      "Epoch 82/100 | Loss: 0.5191 | Test Acc: 59.26%\n",
      "Epoch 83/100 | Loss: 0.5149 | Test Acc: 59.26%\n",
      "Epoch 84/100 | Loss: 0.5260 | Test Acc: 58.02%\n",
      "Epoch 85/100 | Loss: 0.4656 | Test Acc: 59.26%\n",
      "Epoch 86/100 | Loss: 0.4699 | Test Acc: 60.49%\n",
      "Epoch 87/100 | Loss: 0.5335 | Test Acc: 62.96%\n",
      "Epoch 88/100 | Loss: 0.5170 | Test Acc: 62.96%\n",
      "Epoch 89/100 | Loss: 0.5287 | Test Acc: 62.96%\n",
      "Epoch 90/100 | Loss: 0.4439 | Test Acc: 61.73%\n",
      "Epoch 91/100 | Loss: 0.4163 | Test Acc: 62.96%\n",
      "Epoch 92/100 | Loss: 0.4189 | Test Acc: 61.73%\n",
      "Epoch 93/100 | Loss: 0.4281 | Test Acc: 67.90%\n",
      "Epoch 94/100 | Loss: 0.4512 | Test Acc: 59.26%\n",
      "Epoch 95/100 | Loss: 0.4402 | Test Acc: 59.26%\n",
      "Epoch 96/100 | Loss: 0.4140 | Test Acc: 65.43%\n",
      "Epoch 97/100 | Loss: 0.5453 | Test Acc: 60.49%\n",
      "Epoch 98/100 | Loss: 0.5338 | Test Acc: 55.56%\n",
      "Epoch 99/100 | Loss: 0.4769 | Test Acc: 54.32%\n",
      "Epoch 100/100 | Loss: 0.4472 | Test Acc: 61.73%\n",
      "Run 9 Best Test Accuracy: 67.90%\n",
      "\n",
      "=== Run 10/10 ===\n",
      "Epoch 1/100 | Loss: 3.3186 | Test Acc: 13.58%\n",
      "Epoch 2/100 | Loss: 2.3515 | Test Acc: 20.99%\n",
      "Epoch 3/100 | Loss: 1.8863 | Test Acc: 37.04%\n",
      "Epoch 4/100 | Loss: 1.6142 | Test Acc: 29.63%\n",
      "Epoch 5/100 | Loss: 1.4744 | Test Acc: 33.33%\n",
      "Epoch 6/100 | Loss: 1.2786 | Test Acc: 43.21%\n",
      "Epoch 7/100 | Loss: 1.3109 | Test Acc: 45.68%\n",
      "Epoch 8/100 | Loss: 1.1845 | Test Acc: 46.91%\n",
      "Epoch 9/100 | Loss: 1.1823 | Test Acc: 40.74%\n",
      "Epoch 10/100 | Loss: 1.1146 | Test Acc: 50.62%\n",
      "Epoch 11/100 | Loss: 1.0545 | Test Acc: 53.09%\n",
      "Epoch 12/100 | Loss: 0.9592 | Test Acc: 45.68%\n",
      "Epoch 13/100 | Loss: 1.0078 | Test Acc: 48.15%\n",
      "Epoch 14/100 | Loss: 0.9271 | Test Acc: 55.56%\n",
      "Epoch 15/100 | Loss: 0.8769 | Test Acc: 58.02%\n",
      "Epoch 16/100 | Loss: 0.8750 | Test Acc: 51.85%\n",
      "Epoch 17/100 | Loss: 0.8201 | Test Acc: 55.56%\n",
      "Epoch 18/100 | Loss: 0.8295 | Test Acc: 46.91%\n",
      "Epoch 19/100 | Loss: 0.8166 | Test Acc: 55.56%\n",
      "Epoch 20/100 | Loss: 0.7286 | Test Acc: 55.56%\n",
      "Epoch 21/100 | Loss: 0.7189 | Test Acc: 53.09%\n",
      "Epoch 22/100 | Loss: 0.6649 | Test Acc: 58.02%\n",
      "Epoch 23/100 | Loss: 0.6519 | Test Acc: 55.56%\n",
      "Epoch 24/100 | Loss: 0.7449 | Test Acc: 55.56%\n",
      "Epoch 25/100 | Loss: 0.6380 | Test Acc: 61.73%\n",
      "Epoch 26/100 | Loss: 0.6169 | Test Acc: 61.73%\n",
      "Epoch 27/100 | Loss: 0.6290 | Test Acc: 56.79%\n",
      "Epoch 28/100 | Loss: 0.6641 | Test Acc: 50.62%\n",
      "Epoch 29/100 | Loss: 0.6147 | Test Acc: 66.67%\n",
      "Epoch 30/100 | Loss: 0.5754 | Test Acc: 60.49%\n",
      "Epoch 31/100 | Loss: 0.6183 | Test Acc: 60.49%\n",
      "Epoch 32/100 | Loss: 0.5923 | Test Acc: 60.49%\n",
      "Epoch 33/100 | Loss: 0.5579 | Test Acc: 59.26%\n",
      "Epoch 34/100 | Loss: 0.5902 | Test Acc: 65.43%\n",
      "Epoch 35/100 | Loss: 0.4823 | Test Acc: 60.49%\n",
      "Epoch 36/100 | Loss: 0.5444 | Test Acc: 60.49%\n",
      "Epoch 37/100 | Loss: 0.5004 | Test Acc: 58.02%\n",
      "Epoch 38/100 | Loss: 0.5079 | Test Acc: 59.26%\n",
      "Epoch 39/100 | Loss: 0.5664 | Test Acc: 55.56%\n",
      "Epoch 40/100 | Loss: 0.5682 | Test Acc: 61.73%\n",
      "Epoch 41/100 | Loss: 0.5280 | Test Acc: 61.73%\n",
      "Epoch 42/100 | Loss: 0.4282 | Test Acc: 62.96%\n",
      "Epoch 43/100 | Loss: 0.5066 | Test Acc: 60.49%\n",
      "Epoch 44/100 | Loss: 0.4257 | Test Acc: 65.43%\n",
      "Epoch 45/100 | Loss: 0.4076 | Test Acc: 60.49%\n",
      "Epoch 46/100 | Loss: 0.4408 | Test Acc: 59.26%\n",
      "Epoch 47/100 | Loss: 0.4530 | Test Acc: 58.02%\n",
      "Epoch 48/100 | Loss: 0.4442 | Test Acc: 59.26%\n",
      "Epoch 49/100 | Loss: 0.4709 | Test Acc: 60.49%\n",
      "Epoch 50/100 | Loss: 0.4449 | Test Acc: 59.26%\n",
      "Epoch 51/100 | Loss: 0.3621 | Test Acc: 70.37%\n",
      "Epoch 52/100 | Loss: 0.4253 | Test Acc: 65.43%\n",
      "Epoch 53/100 | Loss: 0.5486 | Test Acc: 51.85%\n",
      "Epoch 54/100 | Loss: 0.5006 | Test Acc: 61.73%\n",
      "Epoch 55/100 | Loss: 0.4905 | Test Acc: 58.02%\n",
      "Epoch 56/100 | Loss: 0.4476 | Test Acc: 60.49%\n",
      "Epoch 57/100 | Loss: 0.3929 | Test Acc: 59.26%\n",
      "Epoch 58/100 | Loss: 0.3959 | Test Acc: 65.43%\n",
      "Epoch 59/100 | Loss: 0.4241 | Test Acc: 64.20%\n",
      "Epoch 60/100 | Loss: 0.3591 | Test Acc: 62.96%\n",
      "Epoch 61/100 | Loss: 0.3754 | Test Acc: 60.49%\n",
      "Epoch 62/100 | Loss: 0.3285 | Test Acc: 59.26%\n",
      "Epoch 63/100 | Loss: 0.3721 | Test Acc: 66.67%\n",
      "Epoch 64/100 | Loss: 0.4725 | Test Acc: 56.79%\n",
      "Epoch 65/100 | Loss: 0.4386 | Test Acc: 64.20%\n",
      "Epoch 66/100 | Loss: 0.3667 | Test Acc: 58.02%\n",
      "Epoch 67/100 | Loss: 0.3774 | Test Acc: 62.96%\n",
      "Epoch 68/100 | Loss: 0.3570 | Test Acc: 60.49%\n",
      "Epoch 69/100 | Loss: 0.3804 | Test Acc: 62.96%\n",
      "Epoch 70/100 | Loss: 0.3603 | Test Acc: 58.02%\n",
      "Epoch 71/100 | Loss: 0.4324 | Test Acc: 62.96%\n",
      "Epoch 72/100 | Loss: 0.3868 | Test Acc: 59.26%\n",
      "Epoch 73/100 | Loss: 0.3191 | Test Acc: 58.02%\n",
      "Epoch 74/100 | Loss: 0.3575 | Test Acc: 60.49%\n",
      "Epoch 75/100 | Loss: 0.3344 | Test Acc: 64.20%\n",
      "Epoch 76/100 | Loss: 0.3304 | Test Acc: 64.20%\n",
      "Epoch 77/100 | Loss: 0.2966 | Test Acc: 62.96%\n",
      "Epoch 78/100 | Loss: 0.4371 | Test Acc: 59.26%\n",
      "Epoch 79/100 | Loss: 0.3994 | Test Acc: 62.96%\n",
      "Epoch 80/100 | Loss: 0.3558 | Test Acc: 62.96%\n",
      "Epoch 81/100 | Loss: 0.3795 | Test Acc: 59.26%\n",
      "Epoch 82/100 | Loss: 0.3660 | Test Acc: 55.56%\n",
      "Epoch 83/100 | Loss: 0.3587 | Test Acc: 64.20%\n",
      "Epoch 84/100 | Loss: 0.3551 | Test Acc: 62.96%\n",
      "Epoch 85/100 | Loss: 0.4483 | Test Acc: 65.43%\n",
      "Epoch 86/100 | Loss: 0.5930 | Test Acc: 61.73%\n",
      "Epoch 87/100 | Loss: 0.4676 | Test Acc: 65.43%\n",
      "Epoch 88/100 | Loss: 0.4420 | Test Acc: 61.73%\n",
      "Epoch 89/100 | Loss: 0.4219 | Test Acc: 67.90%\n",
      "Epoch 90/100 | Loss: 0.4663 | Test Acc: 54.32%\n",
      "Epoch 91/100 | Loss: 0.4102 | Test Acc: 61.73%\n",
      "Epoch 92/100 | Loss: 0.3562 | Test Acc: 59.26%\n",
      "Epoch 93/100 | Loss: 0.3804 | Test Acc: 58.02%\n",
      "Epoch 94/100 | Loss: 0.4345 | Test Acc: 70.37%\n",
      "Epoch 95/100 | Loss: 0.3624 | Test Acc: 59.26%\n",
      "Epoch 96/100 | Loss: 0.3593 | Test Acc: 61.73%\n",
      "Epoch 97/100 | Loss: 0.3817 | Test Acc: 64.20%\n",
      "Epoch 98/100 | Loss: 0.3843 | Test Acc: 64.20%\n",
      "Epoch 99/100 | Loss: 0.3483 | Test Acc: 62.96%\n",
      "Epoch 100/100 | Loss: 0.3397 | Test Acc: 62.96%\n",
      "Run 10 Best Test Accuracy: 70.37%\n",
      "\n",
      "Average Highest Test Accuracy over 10 runs: 61.73%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "###################################\n",
    "# 1. FASTA Parsing and Filtering\n",
    "###################################\n",
    "\n",
    "def parse_fasta_with_labels(fasta_file):\n",
    "    \"\"\"\n",
    "    Parses a FASTA file where each header line is assumed to be:\n",
    "        >ClassLabel\n",
    "        DNASEQUENCE\n",
    "    Returns:\n",
    "        list of tuples (label, sequence)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        header = record.description.strip()\n",
    "        sequence = str(record.seq).upper()\n",
    "        label = header.split()[0]\n",
    "        data.append((label, sequence))\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_train_test_split(raw_data):\n",
    "    \"\"\"\n",
    "    Given a list of (label, sequence) pairs, \n",
    "    pick 1 random sample per class for test, \n",
    "    and the rest for train.\n",
    "    \"\"\"\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, seq in raw_data:\n",
    "        label_to_samples[label].append(seq)\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for label, seqs in label_to_samples.items():\n",
    "        random.shuffle(seqs)\n",
    "        # pick one for test\n",
    "        test_seq = seqs[0]\n",
    "        # remainder for train\n",
    "        train_seqs = seqs[1:]\n",
    "        \n",
    "        test_data.append((label, test_seq))\n",
    "        for s in train_seqs:\n",
    "            train_data.append((label, s))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "###################################\n",
    "# 2. K-mer Processing\n",
    "###################################\n",
    "\n",
    "def generate_kmers(sequence, k=6):\n",
    "    \"\"\"\n",
    "    Generates overlapping K-mers of length k from a DNA sequence.\n",
    "    \"\"\"\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmers.append(sequence[i:i + k])\n",
    "    return kmers\n",
    "\n",
    "def build_kmer_vocab(dataset, k=6):\n",
    "    \"\"\"\n",
    "    dataset is a list of (label, seq) pairs.\n",
    "    Returns a dict mapping each K-mer to an integer index.\n",
    "    \"\"\"\n",
    "    kmer_set = set()\n",
    "    for _, seq in dataset:\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        kmer_set.update(kmers)\n",
    "    \n",
    "    vocab = {\"<UNK>\": 0}\n",
    "    for i, kmer in enumerate(sorted(kmer_set), start=1):\n",
    "        vocab[kmer] = i\n",
    "    return vocab\n",
    "\n",
    "def encode_sequence(sequence, vocab, k=6):\n",
    "    \"\"\"\n",
    "    Convert a DNA sequence to a list of token indices based on K-mer vocab.\n",
    "    \"\"\"\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    encoded = [vocab.get(kmer, vocab[\"<UNK>\"]) for kmer in kmers]\n",
    "    return encoded\n",
    "\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    \"\"\"\n",
    "    Keep only classes that have >= min_count samples.\n",
    "    Discard classes with fewer samples.\n",
    "    \"\"\"\n",
    "    label_counts = Counter([label for (label, _) in raw_data])\n",
    "    filtered_data = [\n",
    "        (label, seq) \n",
    "        for (label, seq) in raw_data\n",
    "        if label_counts[label] >= min_count\n",
    "    ]\n",
    "    return filtered_data\n",
    "\n",
    "###################################\n",
    "# 3. Create \"Paired\" Data\n",
    "###################################\n",
    "\"\"\"\n",
    "For demonstration, we'll pair each forward sequence with its reverse.\n",
    "\"\"\"\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    \"\"\"\n",
    "    For demonstration, simply reverse the sequence.\n",
    "    (A true reverse complement would also swap nucleotides.)\n",
    "    \"\"\"\n",
    "    return seq[::-1]\n",
    "\n",
    "def create_paired_data(data_list):\n",
    "    \"\"\"\n",
    "    For each (label, seq) in data_list, produce\n",
    "    (label, fwd_seq, rev_seq).\n",
    "    \"\"\"\n",
    "    paired = []\n",
    "    for label, seq in data_list:\n",
    "        rev_seq = reverse_complement(seq)\n",
    "        paired.append((label, seq, rev_seq))\n",
    "    return paired\n",
    "\n",
    "###################################\n",
    "# 4. PyTorch Dataset for Two Inputs\n",
    "###################################\n",
    "\n",
    "class TwoFastaKmerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: (encoded_seq_fwd, encoded_seq_rev, label_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, paired_data, vocab, k=6):\n",
    "        \"\"\"\n",
    "        paired_data: list of (label, fwd_seq, rev_seq)\n",
    "        vocab: dict for k-mers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.k = k\n",
    "        \n",
    "        # Gather labels and create mapping\n",
    "        labels = sorted(set(item[0] for item in paired_data))\n",
    "        self.label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "        \n",
    "        self.encoded_data = []\n",
    "        for label, fwd_seq, rev_seq in paired_data:\n",
    "            x1 = encode_sequence(fwd_seq, self.vocab, k=self.k)\n",
    "            x2 = encode_sequence(rev_seq, self.vocab, k=self.k)\n",
    "            y = self.label2idx[label]\n",
    "            self.encoded_data.append((x1, x2, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]  # (fwd, rev, label_idx)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label2idx)\n",
    "\n",
    "###################################\n",
    "# 5. Collate Function for Two Inputs\n",
    "###################################\n",
    "\n",
    "def collate_fn_two(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (seq_fwd, seq_rev, label)\n",
    "    Pads both forward and reverse sequences.\n",
    "    \"\"\"\n",
    "    seqs_fwd, seqs_rev, labels = zip(*batch)\n",
    "    \n",
    "    seq_fwd_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_fwd]\n",
    "    seq_rev_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_rev]\n",
    "    \n",
    "    padded_fwd = pad_sequence(seq_fwd_tensors, batch_first=True, padding_value=0)\n",
    "    padded_rev = pad_sequence(seq_rev_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    labels_tensors = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_fwd, padded_rev, labels_tensors\n",
    "\n",
    "###################################\n",
    "# 6. Two-Transformer Fusion Model\n",
    "###################################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds tokens, adds positional encoding, runs TransformerEncoder,\n",
    "    and pools the output (mean pooling by default).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, \n",
    "                 dim_feedforward=512, dropout=0.1, max_len=5000, pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        encoded = self.pos_encoder(embedded)\n",
    "        out = self.transformer_encoder(encoded)\n",
    "        \n",
    "        if self.pooling == 'mean':\n",
    "            pooled = out.mean(dim=1)\n",
    "        else:\n",
    "            pooled = out.mean(dim=1)\n",
    "        return pooled\n",
    "\n",
    "class TwoTransformerFusionDNAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Two separate Transformer encoders -> fused representation -> classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_classes, d_model=128, nhead=8, num_layers=2, \n",
    "                 dim_feedforward=512, dropout=0.1, max_len=5000, pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transformer1 = TransformerEncoderBlock(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            pooling=pooling\n",
    "        )\n",
    "        self.transformer2 = TransformerEncoderBlock(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            pooling=pooling\n",
    "        )\n",
    "        \n",
    "        fused_dim = 2 * d_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fused_dim, fused_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fused_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        f1 = self.transformer1(x1)\n",
    "        f2 = self.transformer2(x2)\n",
    "        fused = torch.cat([f1, f2], dim=1)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "###################################\n",
    "# 7. Putting It All Together\n",
    "###################################\n",
    "\n",
    "# 7A) Load & prepare raw data\n",
    "fasta_file = \"data2/fungi_ITS_cleaned.fasta\"\n",
    "raw_data = parse_fasta_with_labels(fasta_file)\n",
    "raw_data = filter_classes(raw_data, min_count=10)\n",
    "\n",
    "# Build vocabulary from the entire raw data (both forward and its reverse)\n",
    "tmp_data = []\n",
    "for (lbl, seq) in raw_data:\n",
    "    tmp_data.append((lbl, seq))\n",
    "    tmp_data.append((lbl, reverse_complement(seq)))\n",
    "\n",
    "k = 6\n",
    "vocab = build_kmer_vocab(tmp_data, k=k)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###################################\n",
    "# 8. Run the Experiment 10 Times\n",
    "###################################\n",
    "\n",
    "num_runs = 10\n",
    "epochs = 100\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\n=== Run {run+1}/{num_runs} ===\")\n",
    "    # Create a new train-test split for each run\n",
    "    train_data, test_data = create_train_test_split(raw_data)\n",
    "    paired_train = create_paired_data(train_data)\n",
    "    paired_test  = create_paired_data(test_data)\n",
    "    \n",
    "    train_dataset = TwoFastaKmerDataset(paired_train, vocab, k=k)\n",
    "    test_dataset  = TwoFastaKmerDataset(paired_test,  vocab, k=k)\n",
    "    \n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, collate_fn=collate_fn_two)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn_two)\n",
    "    \n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    vocab_size = train_dataset.get_vocab_size()\n",
    "    \n",
    "    # 7B) Create the model for this run\n",
    "    model = TwoTransformerFusionDNAClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "        pooling='mean'\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Function to evaluate accuracy\n",
    "    def evaluate_accuracy(model, data_loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for fwd, rev, labels in data_loader:\n",
    "                fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "                logits = model(fwd, rev)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "    best_run_acc = 0.0\n",
    "    # Training loop for current run\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for fwd, rev, labels in train_loader:\n",
    "            fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(fwd, rev)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        test_acc  = evaluate_accuracy(model, test_loader)\n",
    "        \n",
    "        if test_acc > best_run_acc:\n",
    "            best_run_acc = test_acc\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    best_accuracies.append(best_run_acc)\n",
    "    print(f\"Run {run+1} Best Test Accuracy: {best_run_acc:.2f}%\")\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f\"\\nAverage Highest Test Accuracy over {num_runs} runs: {avg_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 transformer to fc, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/10 ===\n",
      "Epoch 1/30 | Loss: 3.7027 | Test Acc: 3.70%\n",
      "Epoch 2/30 | Loss: 3.2629 | Test Acc: 6.17%\n",
      "Epoch 3/30 | Loss: 2.9542 | Test Acc: 4.94%\n",
      "Epoch 4/30 | Loss: 2.7968 | Test Acc: 12.35%\n",
      "Epoch 5/30 | Loss: 2.5585 | Test Acc: 12.35%\n",
      "Epoch 6/30 | Loss: 2.4029 | Test Acc: 19.75%\n",
      "Epoch 7/30 | Loss: 2.3925 | Test Acc: 13.58%\n",
      "Epoch 8/30 | Loss: 2.2509 | Test Acc: 17.28%\n",
      "Epoch 9/30 | Loss: 2.1711 | Test Acc: 18.52%\n",
      "Epoch 10/30 | Loss: 2.0855 | Test Acc: 18.52%\n",
      "Epoch 11/30 | Loss: 2.0980 | Test Acc: 19.75%\n",
      "Epoch 12/30 | Loss: 2.0085 | Test Acc: 28.40%\n",
      "Epoch 13/30 | Loss: 1.8808 | Test Acc: 29.63%\n",
      "Epoch 14/30 | Loss: 1.9075 | Test Acc: 27.16%\n",
      "Epoch 15/30 | Loss: 1.8752 | Test Acc: 23.46%\n",
      "Epoch 16/30 | Loss: 1.8938 | Test Acc: 29.63%\n",
      "Epoch 17/30 | Loss: 1.8228 | Test Acc: 38.27%\n",
      "Epoch 18/30 | Loss: 1.8177 | Test Acc: 33.33%\n",
      "Epoch 19/30 | Loss: 1.8118 | Test Acc: 35.80%\n",
      "Epoch 20/30 | Loss: 1.8189 | Test Acc: 32.10%\n",
      "Epoch 21/30 | Loss: 1.7634 | Test Acc: 29.63%\n",
      "Epoch 22/30 | Loss: 1.7127 | Test Acc: 29.63%\n",
      "Epoch 23/30 | Loss: 1.6569 | Test Acc: 32.10%\n",
      "Epoch 24/30 | Loss: 1.6487 | Test Acc: 29.63%\n",
      "Epoch 25/30 | Loss: 1.5997 | Test Acc: 33.33%\n",
      "Epoch 26/30 | Loss: 1.5924 | Test Acc: 32.10%\n",
      "Epoch 27/30 | Loss: 1.6204 | Test Acc: 30.86%\n",
      "Epoch 28/30 | Loss: 1.5376 | Test Acc: 25.93%\n",
      "Epoch 29/30 | Loss: 1.5284 | Test Acc: 30.86%\n",
      "Epoch 30/30 | Loss: 1.4811 | Test Acc: 28.40%\n",
      "Run 1 Best Test Accuracy: 38.27%\n",
      "\n",
      "=== Run 2/10 ===\n",
      "Epoch 1/30 | Loss: 3.7740 | Test Acc: 6.17%\n",
      "Epoch 2/30 | Loss: 3.1228 | Test Acc: 2.47%\n",
      "Epoch 3/30 | Loss: 2.7970 | Test Acc: 12.35%\n",
      "Epoch 4/30 | Loss: 2.5685 | Test Acc: 12.35%\n",
      "Epoch 5/30 | Loss: 2.3927 | Test Acc: 19.75%\n",
      "Epoch 6/30 | Loss: 2.3206 | Test Acc: 25.93%\n",
      "Epoch 7/30 | Loss: 2.1911 | Test Acc: 24.69%\n",
      "Epoch 8/30 | Loss: 2.1041 | Test Acc: 20.99%\n",
      "Epoch 9/30 | Loss: 2.0154 | Test Acc: 27.16%\n",
      "Epoch 10/30 | Loss: 1.8683 | Test Acc: 28.40%\n",
      "Epoch 11/30 | Loss: 1.7593 | Test Acc: 37.04%\n",
      "Epoch 12/30 | Loss: 1.6534 | Test Acc: 37.04%\n",
      "Epoch 13/30 | Loss: 1.6088 | Test Acc: 38.27%\n",
      "Epoch 14/30 | Loss: 1.5219 | Test Acc: 35.80%\n",
      "Epoch 15/30 | Loss: 1.4520 | Test Acc: 40.74%\n",
      "Epoch 16/30 | Loss: 1.5434 | Test Acc: 38.27%\n",
      "Epoch 17/30 | Loss: 1.4871 | Test Acc: 40.74%\n",
      "Epoch 18/30 | Loss: 1.3746 | Test Acc: 39.51%\n",
      "Epoch 19/30 | Loss: 1.3277 | Test Acc: 46.91%\n",
      "Epoch 20/30 | Loss: 1.3597 | Test Acc: 44.44%\n",
      "Epoch 21/30 | Loss: 1.2857 | Test Acc: 41.98%\n",
      "Epoch 22/30 | Loss: 1.2912 | Test Acc: 49.38%\n",
      "Epoch 23/30 | Loss: 1.2551 | Test Acc: 44.44%\n",
      "Epoch 24/30 | Loss: 1.2748 | Test Acc: 53.09%\n",
      "Epoch 25/30 | Loss: 1.2451 | Test Acc: 45.68%\n",
      "Epoch 26/30 | Loss: 1.1615 | Test Acc: 49.38%\n",
      "Epoch 27/30 | Loss: 1.1625 | Test Acc: 46.91%\n",
      "Epoch 28/30 | Loss: 1.2046 | Test Acc: 49.38%\n",
      "Epoch 29/30 | Loss: 1.0799 | Test Acc: 44.44%\n",
      "Epoch 30/30 | Loss: 1.1001 | Test Acc: 50.62%\n",
      "Run 2 Best Test Accuracy: 53.09%\n",
      "\n",
      "=== Run 3/10 ===\n",
      "Epoch 1/30 | Loss: 3.7522 | Test Acc: 3.70%\n",
      "Epoch 2/30 | Loss: 3.1502 | Test Acc: 6.17%\n",
      "Epoch 3/30 | Loss: 2.7571 | Test Acc: 12.35%\n",
      "Epoch 4/30 | Loss: 2.5392 | Test Acc: 12.35%\n",
      "Epoch 5/30 | Loss: 2.3447 | Test Acc: 16.05%\n",
      "Epoch 6/30 | Loss: 2.2257 | Test Acc: 25.93%\n",
      "Epoch 7/30 | Loss: 2.0143 | Test Acc: 24.69%\n",
      "Epoch 8/30 | Loss: 1.9392 | Test Acc: 25.93%\n",
      "Epoch 9/30 | Loss: 1.8432 | Test Acc: 37.04%\n",
      "Epoch 10/30 | Loss: 1.7447 | Test Acc: 32.10%\n",
      "Epoch 11/30 | Loss: 1.6552 | Test Acc: 35.80%\n",
      "Epoch 12/30 | Loss: 1.6309 | Test Acc: 28.40%\n",
      "Epoch 13/30 | Loss: 1.5691 | Test Acc: 29.63%\n",
      "Epoch 14/30 | Loss: 1.5364 | Test Acc: 39.51%\n",
      "Epoch 15/30 | Loss: 1.5485 | Test Acc: 38.27%\n",
      "Epoch 16/30 | Loss: 1.4269 | Test Acc: 38.27%\n",
      "Epoch 17/30 | Loss: 1.4886 | Test Acc: 38.27%\n",
      "Epoch 18/30 | Loss: 1.3875 | Test Acc: 33.33%\n",
      "Epoch 19/30 | Loss: 1.3556 | Test Acc: 38.27%\n",
      "Epoch 20/30 | Loss: 1.3595 | Test Acc: 35.80%\n",
      "Epoch 21/30 | Loss: 1.3411 | Test Acc: 40.74%\n",
      "Epoch 22/30 | Loss: 1.4276 | Test Acc: 35.80%\n",
      "Epoch 23/30 | Loss: 1.3438 | Test Acc: 35.80%\n",
      "Epoch 24/30 | Loss: 1.3136 | Test Acc: 40.74%\n",
      "Epoch 25/30 | Loss: 1.2959 | Test Acc: 33.33%\n",
      "Epoch 26/30 | Loss: 1.2363 | Test Acc: 41.98%\n",
      "Epoch 27/30 | Loss: 1.2098 | Test Acc: 34.57%\n",
      "Epoch 28/30 | Loss: 1.2156 | Test Acc: 39.51%\n",
      "Epoch 29/30 | Loss: 1.1758 | Test Acc: 39.51%\n",
      "Epoch 30/30 | Loss: 1.1482 | Test Acc: 39.51%\n",
      "Run 3 Best Test Accuracy: 41.98%\n",
      "\n",
      "=== Run 4/10 ===\n",
      "Epoch 1/30 | Loss: 3.7897 | Test Acc: 2.47%\n",
      "Epoch 2/30 | Loss: 3.1390 | Test Acc: 13.58%\n",
      "Epoch 3/30 | Loss: 2.9204 | Test Acc: 9.88%\n",
      "Epoch 4/30 | Loss: 2.6389 | Test Acc: 16.05%\n",
      "Epoch 5/30 | Loss: 2.3685 | Test Acc: 29.63%\n",
      "Epoch 6/30 | Loss: 2.1946 | Test Acc: 17.28%\n",
      "Epoch 7/30 | Loss: 2.0858 | Test Acc: 22.22%\n",
      "Epoch 8/30 | Loss: 1.9306 | Test Acc: 32.10%\n",
      "Epoch 9/30 | Loss: 1.8479 | Test Acc: 32.10%\n",
      "Epoch 10/30 | Loss: 1.8122 | Test Acc: 32.10%\n",
      "Epoch 11/30 | Loss: 1.7053 | Test Acc: 32.10%\n",
      "Epoch 12/30 | Loss: 1.6309 | Test Acc: 37.04%\n",
      "Epoch 13/30 | Loss: 1.6013 | Test Acc: 28.40%\n",
      "Epoch 14/30 | Loss: 1.5506 | Test Acc: 29.63%\n",
      "Epoch 15/30 | Loss: 1.5142 | Test Acc: 33.33%\n",
      "Epoch 16/30 | Loss: 1.4476 | Test Acc: 29.63%\n",
      "Epoch 17/30 | Loss: 1.3807 | Test Acc: 37.04%\n",
      "Epoch 18/30 | Loss: 1.4174 | Test Acc: 45.68%\n",
      "Epoch 19/30 | Loss: 1.2943 | Test Acc: 37.04%\n",
      "Epoch 20/30 | Loss: 1.3336 | Test Acc: 41.98%\n",
      "Epoch 21/30 | Loss: 1.3197 | Test Acc: 38.27%\n",
      "Epoch 22/30 | Loss: 1.3091 | Test Acc: 41.98%\n",
      "Epoch 23/30 | Loss: 1.2894 | Test Acc: 40.74%\n",
      "Epoch 24/30 | Loss: 1.2202 | Test Acc: 40.74%\n",
      "Epoch 25/30 | Loss: 1.1783 | Test Acc: 40.74%\n",
      "Epoch 26/30 | Loss: 1.2188 | Test Acc: 41.98%\n",
      "Epoch 27/30 | Loss: 1.2372 | Test Acc: 40.74%\n",
      "Epoch 28/30 | Loss: 1.1802 | Test Acc: 38.27%\n",
      "Epoch 29/30 | Loss: 1.1350 | Test Acc: 37.04%\n",
      "Epoch 30/30 | Loss: 1.1430 | Test Acc: 45.68%\n",
      "Run 4 Best Test Accuracy: 45.68%\n",
      "\n",
      "=== Run 5/10 ===\n",
      "Epoch 1/30 | Loss: 3.7051 | Test Acc: 2.47%\n",
      "Epoch 2/30 | Loss: 3.0915 | Test Acc: 6.17%\n",
      "Epoch 3/30 | Loss: 2.6962 | Test Acc: 18.52%\n",
      "Epoch 4/30 | Loss: 2.2924 | Test Acc: 13.58%\n",
      "Epoch 5/30 | Loss: 2.0169 | Test Acc: 20.99%\n",
      "Epoch 6/30 | Loss: 1.9162 | Test Acc: 22.22%\n",
      "Epoch 7/30 | Loss: 1.7766 | Test Acc: 28.40%\n",
      "Epoch 8/30 | Loss: 1.7277 | Test Acc: 20.99%\n",
      "Epoch 9/30 | Loss: 1.6474 | Test Acc: 32.10%\n",
      "Epoch 10/30 | Loss: 1.6447 | Test Acc: 30.86%\n",
      "Epoch 11/30 | Loss: 1.5053 | Test Acc: 37.04%\n",
      "Epoch 12/30 | Loss: 1.4949 | Test Acc: 32.10%\n",
      "Epoch 13/30 | Loss: 1.4125 | Test Acc: 40.74%\n",
      "Epoch 14/30 | Loss: 1.4087 | Test Acc: 35.80%\n",
      "Epoch 15/30 | Loss: 1.4231 | Test Acc: 43.21%\n",
      "Epoch 16/30 | Loss: 1.3316 | Test Acc: 39.51%\n",
      "Epoch 17/30 | Loss: 1.2787 | Test Acc: 40.74%\n",
      "Epoch 18/30 | Loss: 1.2459 | Test Acc: 35.80%\n",
      "Epoch 19/30 | Loss: 1.2396 | Test Acc: 49.38%\n",
      "Epoch 20/30 | Loss: 1.1949 | Test Acc: 38.27%\n",
      "Epoch 21/30 | Loss: 1.1855 | Test Acc: 39.51%\n",
      "Epoch 22/30 | Loss: 1.2114 | Test Acc: 39.51%\n",
      "Epoch 23/30 | Loss: 1.2059 | Test Acc: 38.27%\n",
      "Epoch 24/30 | Loss: 1.1452 | Test Acc: 37.04%\n",
      "Epoch 25/30 | Loss: 1.1871 | Test Acc: 39.51%\n",
      "Epoch 26/30 | Loss: 1.1212 | Test Acc: 28.40%\n",
      "Epoch 27/30 | Loss: 1.1259 | Test Acc: 37.04%\n",
      "Epoch 28/30 | Loss: 1.0855 | Test Acc: 41.98%\n",
      "Epoch 29/30 | Loss: 1.0813 | Test Acc: 39.51%\n",
      "Epoch 30/30 | Loss: 1.0732 | Test Acc: 50.62%\n",
      "Run 5 Best Test Accuracy: 50.62%\n",
      "\n",
      "=== Run 6/10 ===\n",
      "Epoch 1/30 | Loss: 3.7585 | Test Acc: 3.70%\n",
      "Epoch 2/30 | Loss: 3.3037 | Test Acc: 6.17%\n",
      "Epoch 3/30 | Loss: 3.0487 | Test Acc: 12.35%\n",
      "Epoch 4/30 | Loss: 2.7990 | Test Acc: 8.64%\n",
      "Epoch 5/30 | Loss: 2.7102 | Test Acc: 11.11%\n",
      "Epoch 6/30 | Loss: 2.5416 | Test Acc: 12.35%\n",
      "Epoch 7/30 | Loss: 2.5339 | Test Acc: 12.35%\n",
      "Epoch 8/30 | Loss: 2.4817 | Test Acc: 8.64%\n",
      "Epoch 9/30 | Loss: 2.4485 | Test Acc: 14.81%\n",
      "Epoch 10/30 | Loss: 2.4085 | Test Acc: 13.58%\n",
      "Epoch 11/30 | Loss: 2.4335 | Test Acc: 13.58%\n",
      "Epoch 12/30 | Loss: 2.4977 | Test Acc: 12.35%\n",
      "Epoch 13/30 | Loss: 2.4137 | Test Acc: 22.22%\n",
      "Epoch 14/30 | Loss: 2.4597 | Test Acc: 18.52%\n",
      "Epoch 15/30 | Loss: 2.5344 | Test Acc: 13.58%\n",
      "Epoch 16/30 | Loss: 2.6650 | Test Acc: 13.58%\n",
      "Epoch 17/30 | Loss: 2.5865 | Test Acc: 13.58%\n",
      "Epoch 18/30 | Loss: 2.6252 | Test Acc: 17.28%\n",
      "Epoch 19/30 | Loss: 2.4608 | Test Acc: 16.05%\n",
      "Epoch 20/30 | Loss: 2.4053 | Test Acc: 17.28%\n",
      "Epoch 21/30 | Loss: 2.3780 | Test Acc: 18.52%\n",
      "Epoch 22/30 | Loss: 2.3591 | Test Acc: 17.28%\n",
      "Epoch 23/30 | Loss: 2.3143 | Test Acc: 17.28%\n",
      "Epoch 24/30 | Loss: 2.3367 | Test Acc: 16.05%\n",
      "Epoch 25/30 | Loss: 2.2059 | Test Acc: 17.28%\n",
      "Epoch 26/30 | Loss: 2.1505 | Test Acc: 20.99%\n",
      "Epoch 27/30 | Loss: 2.1283 | Test Acc: 23.46%\n",
      "Epoch 28/30 | Loss: 2.0996 | Test Acc: 17.28%\n",
      "Epoch 29/30 | Loss: 2.0935 | Test Acc: 19.75%\n",
      "Epoch 30/30 | Loss: 2.0007 | Test Acc: 13.58%\n",
      "Run 6 Best Test Accuracy: 23.46%\n",
      "\n",
      "=== Run 7/10 ===\n",
      "Epoch 1/30 | Loss: 3.8585 | Test Acc: 3.70%\n",
      "Epoch 2/30 | Loss: 3.3466 | Test Acc: 4.94%\n",
      "Epoch 3/30 | Loss: 3.1478 | Test Acc: 4.94%\n",
      "Epoch 4/30 | Loss: 3.0232 | Test Acc: 6.17%\n",
      "Epoch 5/30 | Loss: 2.9579 | Test Acc: 6.17%\n",
      "Epoch 6/30 | Loss: 2.8983 | Test Acc: 4.94%\n",
      "Epoch 7/30 | Loss: 2.8273 | Test Acc: 12.35%\n",
      "Epoch 8/30 | Loss: 2.7037 | Test Acc: 6.17%\n",
      "Epoch 9/30 | Loss: 2.6348 | Test Acc: 16.05%\n",
      "Epoch 10/30 | Loss: 2.5397 | Test Acc: 17.28%\n",
      "Epoch 11/30 | Loss: 2.4574 | Test Acc: 12.35%\n",
      "Epoch 12/30 | Loss: 2.4423 | Test Acc: 14.81%\n",
      "Epoch 13/30 | Loss: 2.4865 | Test Acc: 18.52%\n",
      "Epoch 14/30 | Loss: 2.3949 | Test Acc: 18.52%\n",
      "Epoch 15/30 | Loss: 2.3290 | Test Acc: 12.35%\n",
      "Epoch 16/30 | Loss: 2.3093 | Test Acc: 19.75%\n",
      "Epoch 17/30 | Loss: 2.2211 | Test Acc: 18.52%\n",
      "Epoch 18/30 | Loss: 2.2116 | Test Acc: 17.28%\n",
      "Epoch 19/30 | Loss: 2.1835 | Test Acc: 20.99%\n",
      "Epoch 20/30 | Loss: 2.1229 | Test Acc: 19.75%\n",
      "Epoch 21/30 | Loss: 2.0979 | Test Acc: 22.22%\n",
      "Epoch 22/30 | Loss: 2.1287 | Test Acc: 19.75%\n",
      "Epoch 23/30 | Loss: 2.1120 | Test Acc: 19.75%\n",
      "Epoch 24/30 | Loss: 2.0735 | Test Acc: 22.22%\n",
      "Epoch 25/30 | Loss: 2.0654 | Test Acc: 27.16%\n",
      "Epoch 26/30 | Loss: 2.1205 | Test Acc: 23.46%\n",
      "Epoch 27/30 | Loss: 1.9868 | Test Acc: 18.52%\n",
      "Epoch 28/30 | Loss: 2.0544 | Test Acc: 33.33%\n",
      "Epoch 29/30 | Loss: 1.9775 | Test Acc: 27.16%\n",
      "Epoch 30/30 | Loss: 1.9552 | Test Acc: 25.93%\n",
      "Run 7 Best Test Accuracy: 33.33%\n",
      "\n",
      "=== Run 8/10 ===\n",
      "Epoch 1/30 | Loss: 4.2077 | Test Acc: 1.23%\n",
      "Epoch 2/30 | Loss: 4.0029 | Test Acc: 2.47%\n",
      "Epoch 3/30 | Loss: 3.7333 | Test Acc: 2.47%\n",
      "Epoch 4/30 | Loss: 3.6808 | Test Acc: 2.47%\n",
      "Epoch 5/30 | Loss: 3.7629 | Test Acc: 2.47%\n",
      "Epoch 6/30 | Loss: 3.6353 | Test Acc: 2.47%\n",
      "Epoch 7/30 | Loss: 3.6432 | Test Acc: 3.70%\n",
      "Epoch 8/30 | Loss: 3.7198 | Test Acc: 2.47%\n",
      "Epoch 9/30 | Loss: 3.7272 | Test Acc: 1.23%\n",
      "Epoch 10/30 | Loss: 3.5521 | Test Acc: 2.47%\n",
      "Epoch 11/30 | Loss: 3.4978 | Test Acc: 2.47%\n",
      "Epoch 12/30 | Loss: 3.4810 | Test Acc: 2.47%\n",
      "Epoch 13/30 | Loss: 3.4900 | Test Acc: 2.47%\n",
      "Epoch 14/30 | Loss: 3.5591 | Test Acc: 3.70%\n",
      "Epoch 15/30 | Loss: 3.4988 | Test Acc: 2.47%\n",
      "Epoch 16/30 | Loss: 3.7228 | Test Acc: 1.23%\n",
      "Epoch 17/30 | Loss: 3.8162 | Test Acc: 3.70%\n",
      "Epoch 18/30 | Loss: 3.6774 | Test Acc: 3.70%\n",
      "Epoch 19/30 | Loss: 3.6723 | Test Acc: 4.94%\n",
      "Epoch 20/30 | Loss: 3.6237 | Test Acc: 3.70%\n",
      "Epoch 21/30 | Loss: 3.5128 | Test Acc: 2.47%\n",
      "Epoch 22/30 | Loss: 3.4565 | Test Acc: 3.70%\n",
      "Epoch 23/30 | Loss: 3.4566 | Test Acc: 3.70%\n",
      "Epoch 24/30 | Loss: 3.5642 | Test Acc: 4.94%\n",
      "Epoch 25/30 | Loss: 3.4959 | Test Acc: 3.70%\n",
      "Epoch 26/30 | Loss: 3.5041 | Test Acc: 3.70%\n",
      "Epoch 27/30 | Loss: 3.5120 | Test Acc: 3.70%\n",
      "Epoch 28/30 | Loss: 3.5324 | Test Acc: 2.47%\n",
      "Epoch 29/30 | Loss: 3.5128 | Test Acc: 3.70%\n",
      "Epoch 30/30 | Loss: 3.5114 | Test Acc: 3.70%\n",
      "Run 8 Best Test Accuracy: 4.94%\n",
      "\n",
      "=== Run 9/10 ===\n",
      "Epoch 1/30 | Loss: 3.6810 | Test Acc: 4.94%\n",
      "Epoch 2/30 | Loss: 2.8624 | Test Acc: 14.81%\n",
      "Epoch 3/30 | Loss: 2.2936 | Test Acc: 22.22%\n",
      "Epoch 4/30 | Loss: 1.9854 | Test Acc: 22.22%\n",
      "Epoch 5/30 | Loss: 1.7912 | Test Acc: 32.10%\n",
      "Epoch 6/30 | Loss: 1.6590 | Test Acc: 28.40%\n",
      "Epoch 7/30 | Loss: 1.5163 | Test Acc: 32.10%\n",
      "Epoch 8/30 | Loss: 1.4476 | Test Acc: 33.33%\n",
      "Epoch 9/30 | Loss: 1.3536 | Test Acc: 39.51%\n",
      "Epoch 10/30 | Loss: 1.3039 | Test Acc: 43.21%\n",
      "Epoch 11/30 | Loss: 1.2476 | Test Acc: 46.91%\n",
      "Epoch 12/30 | Loss: 1.1616 | Test Acc: 48.15%\n",
      "Epoch 13/30 | Loss: 1.1005 | Test Acc: 39.51%\n",
      "Epoch 14/30 | Loss: 1.1385 | Test Acc: 58.02%\n",
      "Epoch 15/30 | Loss: 1.1026 | Test Acc: 51.85%\n",
      "Epoch 16/30 | Loss: 0.9881 | Test Acc: 50.62%\n",
      "Epoch 17/30 | Loss: 1.0071 | Test Acc: 48.15%\n",
      "Epoch 18/30 | Loss: 1.0026 | Test Acc: 40.74%\n",
      "Epoch 19/30 | Loss: 1.0276 | Test Acc: 49.38%\n",
      "Epoch 20/30 | Loss: 0.9517 | Test Acc: 45.68%\n",
      "Epoch 21/30 | Loss: 0.9231 | Test Acc: 50.62%\n",
      "Epoch 22/30 | Loss: 0.8512 | Test Acc: 61.73%\n",
      "Epoch 23/30 | Loss: 0.8203 | Test Acc: 53.09%\n",
      "Epoch 24/30 | Loss: 0.9009 | Test Acc: 59.26%\n",
      "Epoch 25/30 | Loss: 0.8734 | Test Acc: 60.49%\n",
      "Epoch 26/30 | Loss: 0.8050 | Test Acc: 55.56%\n",
      "Epoch 27/30 | Loss: 0.7573 | Test Acc: 50.62%\n",
      "Epoch 28/30 | Loss: 0.7323 | Test Acc: 50.62%\n",
      "Epoch 29/30 | Loss: 0.6968 | Test Acc: 55.56%\n",
      "Epoch 30/30 | Loss: 0.6629 | Test Acc: 58.02%\n",
      "Run 9 Best Test Accuracy: 61.73%\n",
      "\n",
      "=== Run 10/10 ===\n",
      "Epoch 1/30 | Loss: 3.9638 | Test Acc: 2.47%\n",
      "Epoch 2/30 | Loss: 3.5083 | Test Acc: 2.47%\n",
      "Epoch 3/30 | Loss: 3.4116 | Test Acc: 4.94%\n",
      "Epoch 4/30 | Loss: 3.4099 | Test Acc: 4.94%\n",
      "Epoch 5/30 | Loss: 3.5367 | Test Acc: 3.70%\n",
      "Epoch 6/30 | Loss: 3.5511 | Test Acc: 2.47%\n",
      "Epoch 7/30 | Loss: 3.3926 | Test Acc: 2.47%\n",
      "Epoch 8/30 | Loss: 3.4157 | Test Acc: 3.70%\n",
      "Epoch 9/30 | Loss: 3.3892 | Test Acc: 3.70%\n",
      "Epoch 10/30 | Loss: 3.3776 | Test Acc: 3.70%\n",
      "Epoch 11/30 | Loss: 3.3475 | Test Acc: 3.70%\n",
      "Epoch 12/30 | Loss: 3.3531 | Test Acc: 4.94%\n",
      "Epoch 13/30 | Loss: 3.3740 | Test Acc: 4.94%\n",
      "Epoch 14/30 | Loss: 3.4538 | Test Acc: 3.70%\n",
      "Epoch 15/30 | Loss: 3.5288 | Test Acc: 4.94%\n",
      "Epoch 16/30 | Loss: 3.4282 | Test Acc: 2.47%\n",
      "Epoch 17/30 | Loss: 3.3695 | Test Acc: 2.47%\n",
      "Epoch 18/30 | Loss: 3.4044 | Test Acc: 1.23%\n",
      "Epoch 19/30 | Loss: 3.5381 | Test Acc: 2.47%\n",
      "Epoch 20/30 | Loss: 3.4680 | Test Acc: 3.70%\n",
      "Epoch 21/30 | Loss: 3.3958 | Test Acc: 4.94%\n",
      "Epoch 22/30 | Loss: 3.3675 | Test Acc: 4.94%\n",
      "Epoch 23/30 | Loss: 3.4284 | Test Acc: 1.23%\n",
      "Epoch 24/30 | Loss: 3.5406 | Test Acc: 3.70%\n",
      "Epoch 25/30 | Loss: 3.5082 | Test Acc: 3.70%\n",
      "Epoch 26/30 | Loss: 3.6467 | Test Acc: 3.70%\n",
      "Epoch 27/30 | Loss: 3.6704 | Test Acc: 3.70%\n",
      "Epoch 28/30 | Loss: 3.6002 | Test Acc: 3.70%\n",
      "Epoch 29/30 | Loss: 3.6289 | Test Acc: 3.70%\n",
      "Epoch 30/30 | Loss: 3.4998 | Test Acc: 3.70%\n",
      "Run 10 Best Test Accuracy: 4.94%\n",
      "\n",
      "Average Highest Test Accuracy over 10 runs: 35.80%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "###################################\n",
    "# 1. FASTA Parsing and Filtering\n",
    "###################################\n",
    "\n",
    "def parse_fasta_with_labels(fasta_file):\n",
    "    \"\"\"\n",
    "    Parses a FASTA file where each header line is assumed to be:\n",
    "        >ClassLabel\n",
    "        DNASEQUENCE\n",
    "    Returns:\n",
    "        list of tuples (label, sequence)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        header = record.description.strip()\n",
    "        sequence = str(record.seq).upper()\n",
    "        label = header.split()[0]\n",
    "        data.append((label, sequence))\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_train_test_split(raw_data):\n",
    "    \"\"\"\n",
    "    Given a list of (label, sequence) pairs, \n",
    "    pick 1 random sample per class for test, \n",
    "    and the rest for train.\n",
    "    \"\"\"\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, seq in raw_data:\n",
    "        label_to_samples[label].append(seq)\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for label, seqs in label_to_samples.items():\n",
    "        random.shuffle(seqs)\n",
    "        # pick one for test\n",
    "        test_seq = seqs[0]\n",
    "        # remainder for train\n",
    "        train_seqs = seqs[1:]\n",
    "        \n",
    "        test_data.append((label, test_seq))\n",
    "        for s in train_seqs:\n",
    "            train_data.append((label, s))\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "###################################\n",
    "# 2. K-mer Processing\n",
    "###################################\n",
    "\n",
    "def generate_kmers(sequence, k=6):\n",
    "    \"\"\"\n",
    "    Generates overlapping K-mers of length k from a DNA sequence.\n",
    "    \"\"\"\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmers.append(sequence[i:i + k])\n",
    "    return kmers\n",
    "\n",
    "def build_kmer_vocab(dataset, k=6):\n",
    "    \"\"\"\n",
    "    dataset is a list of (label, seq) pairs.\n",
    "    Returns a dict mapping each K-mer to an integer index.\n",
    "    \"\"\"\n",
    "    kmer_set = set()\n",
    "    for _, seq in dataset:\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        kmer_set.update(kmers)\n",
    "    \n",
    "    vocab = {\"<UNK>\": 0}\n",
    "    for i, kmer in enumerate(sorted(kmer_set), start=1):\n",
    "        vocab[kmer] = i\n",
    "    return vocab\n",
    "\n",
    "def encode_sequence(sequence, vocab, k=6):\n",
    "    \"\"\"\n",
    "    Convert a DNA sequence to a list of token indices based on K-mer vocab.\n",
    "    \"\"\"\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    encoded = [vocab.get(kmer, vocab[\"<UNK>\"]) for kmer in kmers]\n",
    "    return encoded\n",
    "\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    \"\"\"\n",
    "    Keep only classes that have >= min_count samples.\n",
    "    Discard classes with fewer samples.\n",
    "    \"\"\"\n",
    "    label_counts = Counter([label for (label, _) in raw_data])\n",
    "    filtered_data = [\n",
    "        (label, seq) \n",
    "        for (label, seq) in raw_data\n",
    "        if label_counts[label] >= min_count\n",
    "    ]\n",
    "    return filtered_data\n",
    "\n",
    "###################################\n",
    "# 3. Create \"Paired\" Data\n",
    "###################################\n",
    "\"\"\"\n",
    "For demonstration, we'll pair each forward sequence with its reverse.\n",
    "\"\"\"\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    \"\"\"\n",
    "    For demonstration, simply reverse the sequence.\n",
    "    (A true reverse complement would also swap nucleotides.)\n",
    "    \"\"\"\n",
    "    return seq[::-1]\n",
    "\n",
    "def create_paired_data(data_list):\n",
    "    \"\"\"\n",
    "    For each (label, seq) in data_list, produce\n",
    "    (label, fwd_seq, rev_seq).\n",
    "    \"\"\"\n",
    "    paired = []\n",
    "    for label, seq in data_list:\n",
    "        rev_seq = reverse_complement(seq)\n",
    "        paired.append((label, seq, rev_seq))\n",
    "    return paired\n",
    "\n",
    "###################################\n",
    "# 4. PyTorch Dataset for Two Inputs\n",
    "###################################\n",
    "\n",
    "class TwoFastaKmerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: (encoded_seq_fwd, encoded_seq_rev, label_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, paired_data, vocab, k=6):\n",
    "        \"\"\"\n",
    "        paired_data: list of (label, fwd_seq, rev_seq)\n",
    "        vocab: dict for k-mers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.k = k\n",
    "        \n",
    "        # Gather labels and create mapping\n",
    "        labels = sorted(set(item[0] for item in paired_data))\n",
    "        self.label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "        \n",
    "        self.encoded_data = []\n",
    "        for label, fwd_seq, rev_seq in paired_data:\n",
    "            x1 = encode_sequence(fwd_seq, self.vocab, k=self.k)\n",
    "            x2 = encode_sequence(rev_seq, self.vocab, k=self.k)\n",
    "            y = self.label2idx[label]\n",
    "            self.encoded_data.append((x1, x2, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]  # (fwd, rev, label_idx)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label2idx)\n",
    "\n",
    "###################################\n",
    "# 5. Collate Function for Two Inputs\n",
    "###################################\n",
    "\n",
    "def collate_fn_two(batch):\n",
    "    \"\"\"\n",
    "    batch: list of (seq_fwd, seq_rev, label)\n",
    "    Pads both forward and reverse sequences.\n",
    "    \"\"\"\n",
    "    seqs_fwd, seqs_rev, labels = zip(*batch)\n",
    "    \n",
    "    seq_fwd_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_fwd]\n",
    "    seq_rev_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_rev]\n",
    "    \n",
    "    padded_fwd = pad_sequence(seq_fwd_tensors, batch_first=True, padding_value=0)\n",
    "    padded_rev = pad_sequence(seq_rev_tensors, batch_first=True, padding_value=0)\n",
    "    \n",
    "    labels_tensors = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_fwd, padded_rev, labels_tensors\n",
    "\n",
    "###################################\n",
    "# 6. Single Transformer Model\n",
    "###################################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds tokens, adds positional encoding, runs TransformerEncoder,\n",
    "    and pools the output (mean pooling by default).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=8, num_layers=2, \n",
    "                 dim_feedforward=512, dropout=0.1, max_len=5000, pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pooling = pooling\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=max_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        encoded = self.pos_encoder(embedded)\n",
    "        out = self.transformer_encoder(encoded)\n",
    "        \n",
    "        if self.pooling == 'mean':\n",
    "            pooled = out.mean(dim=1)\n",
    "        else:\n",
    "            pooled = out.mean(dim=1)\n",
    "        return pooled\n",
    "\n",
    "class SingleTransformerDNAClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder that processes a concatenated sequence.\n",
    "    \n",
    "    The forward and reverse sequences are concatenated along the sequence dimension,\n",
    "    then passed through a single Transformer encoder. The pooled output is used for classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_classes, d_model=128, nhead=8, num_layers=2, \n",
    "                 dim_feedforward=512, dropout=0.1, max_len=5000, pooling='mean'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoderBlock(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            max_len=max_len,\n",
    "            pooling=pooling\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # x1: [B, seq_len1]\n",
    "        # x2: [B, seq_len2]\n",
    "        # Concatenate along sequence dimension\n",
    "        x = torch.cat([x1, x2], dim=1)  # [B, seq_len1+seq_len2]\n",
    "        encoded = self.encoder(x)       # [B, d_model]\n",
    "        logits = self.classifier(encoded)\n",
    "        return logits\n",
    "\n",
    "###################################\n",
    "# 7. Putting It All Together\n",
    "###################################\n",
    "\n",
    "# 7A) Load & prepare raw data\n",
    "fasta_file = \"data2/fungi_ITS_cleaned.fasta\"\n",
    "raw_data = parse_fasta_with_labels(fasta_file)\n",
    "raw_data = filter_classes(raw_data, min_count=10)\n",
    "\n",
    "# Build vocabulary from the entire raw data (both forward and its reverse)\n",
    "tmp_data = []\n",
    "for (lbl, seq) in raw_data:\n",
    "    tmp_data.append((lbl, seq))\n",
    "    tmp_data.append((lbl, reverse_complement(seq)))\n",
    "\n",
    "k = 6\n",
    "vocab = build_kmer_vocab(tmp_data, k=k)\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "###################################\n",
    "# 8. Run the Experiment 10 Times\n",
    "###################################\n",
    "\n",
    "num_runs = 10\n",
    "epochs = 30\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\n=== Run {run+1}/{num_runs} ===\")\n",
    "    # Create a new train-test split for each run\n",
    "    train_data, test_data = create_train_test_split(raw_data)\n",
    "    paired_train = create_paired_data(train_data)\n",
    "    paired_test  = create_paired_data(test_data)\n",
    "    \n",
    "    train_dataset = TwoFastaKmerDataset(paired_train, vocab, k=k)\n",
    "    test_dataset  = TwoFastaKmerDataset(paired_test,  vocab, k=k)\n",
    "    \n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, collate_fn=collate_fn_two)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=batch_size, \n",
    "                              shuffle=False, collate_fn=collate_fn_two)\n",
    "    \n",
    "    num_classes = train_dataset.get_num_classes()\n",
    "    vocab_size = train_dataset.get_vocab_size()\n",
    "    \n",
    "    # 7B) Create the single transformer model for this run\n",
    "    model = SingleTransformerDNAClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        num_classes=num_classes,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_layers=2,\n",
    "        dim_feedforward=512,\n",
    "        dropout=0.1,\n",
    "        max_len=5000,\n",
    "        pooling='mean'\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Function to evaluate accuracy\n",
    "    def evaluate_accuracy(model, data_loader):\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for fwd, rev, labels in data_loader:\n",
    "                fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "                logits = model(fwd, rev)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        return 100.0 * correct / total\n",
    "    \n",
    "    best_run_acc = 0.0\n",
    "    # Training loop for current run\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        for fwd, rev, labels in train_loader:\n",
    "            fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(fwd, rev)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        test_acc  = evaluate_accuracy(model, test_loader)\n",
    "        \n",
    "        if test_acc > best_run_acc:\n",
    "            best_run_acc = test_acc\n",
    "        \n",
    "        print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    best_accuracies.append(best_run_acc)\n",
    "    print(f\"Run {run+1} Best Test Accuracy: {best_run_acc:.2f}%\")\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f\"\\nAverage Highest Test Accuracy over {num_runs} runs: {avg_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
