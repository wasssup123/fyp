{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time comparision before and after use purning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# size comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Pruning Model: Total parameters = 17115786, Nonzero parameters = 17115562\n",
      "Layer model.0: kept 16 out of 32 filters (pruned 16)\n",
      "Layer model.4: kept 64 out of 64 filters (pruned 0)\n",
      "Layer model.8: kept 128 out of 128 filters (pruned 0)\n",
      "After Pruning Model: Total parameters = 17115786, Nonzero parameters = 17115434\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN Model (using 1D convolutions)\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim // 8) * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is assumed to be of shape (batch_size, input_dim)\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "# Function to print model size (total parameters and nonzero parameters)\n",
    "def print_model_size(model, label=\"\"):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "    print(f\"{label} Model: Total parameters = {total_params}, Nonzero parameters = {nonzero_params}\")\n",
    "\n",
    "# Pruning function: zeros out duplicate filters based on cosine similarity.\n",
    "def prune_similar_filters(model, threshold=0.99):\n",
    "    \"\"\"\n",
    "    For each Conv1d layer in the model, compute cosine similarity between filters.\n",
    "    Filters with cosine similarity > threshold are considered duplicates and are zeroed out.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            weight = module.weight.data  # shape: (out_channels, in_channels, kernel_size)\n",
    "            out_channels = weight.shape[0]\n",
    "            # Flatten each filter to a vector.\n",
    "            filters = weight.view(out_channels, -1)\n",
    "            keep_indices = []\n",
    "            pruned_indices = set()\n",
    "            for i in range(out_channels):\n",
    "                if i in pruned_indices:\n",
    "                    continue\n",
    "                keep_indices.append(i)\n",
    "                for j in range(i + 1, out_channels):\n",
    "                    if j in pruned_indices:\n",
    "                        continue\n",
    "                    # Compute cosine similarity between filter i and j.\n",
    "                    cos_sim = F.cosine_similarity(filters[i].unsqueeze(0), filters[j].unsqueeze(0)).item()\n",
    "                    if cos_sim > threshold:\n",
    "                        pruned_indices.add(j)\n",
    "            # Zero out the duplicate filters.\n",
    "            for j in pruned_indices:\n",
    "                module.weight.data[j] = 0\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data[j] = 0\n",
    "            print(f\"Layer {name}: kept {len(keep_indices)} out of {out_channels} filters (pruned {len(pruned_indices)})\")\n",
    "    return model\n",
    "\n",
    "# ---------------------\n",
    "# Example usage below:\n",
    "# ---------------------\n",
    "\n",
    "# Define input parameters for the model.\n",
    "input_dim = 1024   # Adjust this as needed.\n",
    "num_classes = 10   # Example number of classes.\n",
    "\n",
    "# Create a model instance.\n",
    "model = CNNModel(input_dim=input_dim, num_classes=num_classes)\n",
    "\n",
    "# Print model size before pruning.\n",
    "print_model_size(model, label=\"Before Pruning\")\n",
    "\n",
    "# Set the pruning threshold (can be adjusted by the user).\n",
    "prune_threshold = 0.9\n",
    "\n",
    "# Apply pruning.\n",
    "pruned_model = prune_similar_filters(model, threshold=prune_threshold)\n",
    "\n",
    "# Print model size after pruning.\n",
    "print_model_size(pruned_model, label=\"After Pruning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time without purning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Student10 (min_count >= 10) ##########\n",
      "Number of samples after filtering: 1693\n",
      "Train samples: 1612 Test samples: 81\n",
      "Number of classes: 81\n",
      "\n",
      "--- Student10 Run 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Loss: 8.4136 | Test Acc: 7.41% | Time: 2.50 sec\n",
      "Epoch 2/100 | Loss: 2.9305 | Test Acc: 32.10% | Time: 1.80 sec\n",
      "Epoch 3/100 | Loss: 1.6309 | Test Acc: 49.38% | Time: 1.80 sec\n",
      "Epoch 4/100 | Loss: 0.8817 | Test Acc: 66.67% | Time: 1.80 sec\n",
      "Epoch 5/100 | Loss: 0.4551 | Test Acc: 74.07% | Time: 1.79 sec\n",
      "Epoch 6/100 | Loss: 0.3368 | Test Acc: 74.07% | Time: 1.76 sec\n",
      "Epoch 7/100 | Loss: 0.2642 | Test Acc: 79.01% | Time: 1.74 sec\n",
      "Epoch 8/100 | Loss: 0.1706 | Test Acc: 76.54% | Time: 1.72 sec\n",
      "Epoch 9/100 | Loss: 0.2196 | Test Acc: 81.48% | Time: 1.81 sec\n",
      "Epoch 10/100 | Loss: 0.1963 | Test Acc: 80.25% | Time: 1.82 sec\n",
      "Epoch 11/100 | Loss: 0.2013 | Test Acc: 80.25% | Time: 1.84 sec\n",
      "Epoch 12/100 | Loss: 0.1261 | Test Acc: 87.65% | Time: 1.87 sec\n",
      "Epoch 13/100 | Loss: 0.1012 | Test Acc: 83.95% | Time: 1.83 sec\n",
      "Epoch 14/100 | Loss: 0.1685 | Test Acc: 79.01% | Time: 1.82 sec\n",
      "Epoch 15/100 | Loss: 0.2059 | Test Acc: 86.42% | Time: 1.82 sec\n",
      "Epoch 16/100 | Loss: 0.0940 | Test Acc: 85.19% | Time: 1.82 sec\n",
      "Epoch 17/100 | Loss: 0.0590 | Test Acc: 90.12% | Time: 1.81 sec\n",
      "Epoch 18/100 | Loss: 0.0811 | Test Acc: 88.89% | Time: 1.82 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 327\u001b[0m\n\u001b[0;32m    325\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    326\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 327\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Evaluation on test set.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "###############################\n",
    "# File paths and constants\n",
    "###############################\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# Helper functions for k-mer vectorization\n",
    "##########################################\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "##########################################\n",
    "# Step 1: Clean FASTA headers\n",
    "##########################################\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            # Write header using the second field if available.\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "##########################################\n",
    "# Step 2: k-mer vectorization\n",
    "##########################################\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "##########################################\n",
    "# Utility: Parse k-mer vector file\n",
    "##########################################\n",
    "def parse_kmer_vector_file(filepath):\n",
    "    # Each line is of the form: \">label<TAB>vec0 vec1 ...\"\n",
    "    raw_data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            label = parts[0][1:]  # remove '>' prefix\n",
    "            vector = list(map(int, parts[1].split()))\n",
    "            raw_data.append((label, vector))\n",
    "    return raw_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Filter classes by minimum count\n",
    "##########################################\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    label_counts = Counter([label for label, _ in raw_data])\n",
    "    filtered_data = [(label, vec) for (label, vec) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Train-test split (per label)\n",
    "##########################################\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in raw_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, vecs in label_to_samples.items():\n",
    "        random.shuffle(vecs)\n",
    "        test_vec = vecs[0]\n",
    "        train_vecs = vecs[1:]\n",
    "        test_data.append((label, test_vec))\n",
    "        for vec in train_vecs:\n",
    "            train_data.append((label, vec))\n",
    "    return train_data, test_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Resampling to balance classes\n",
    "##########################################\n",
    "def resample_dataset(train_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in train_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    max_count = max(len(samples) for samples in label_to_samples.values())\n",
    "    resampled_data = []\n",
    "    for label, samples in label_to_samples.items():\n",
    "        sampled_vecs = random.choices(samples, k=max_count)\n",
    "        for vec in sampled_vecs:\n",
    "            resampled_data.append((label, vec))\n",
    "    random.shuffle(resampled_data)\n",
    "    return resampled_data\n",
    "\n",
    "##########################################\n",
    "# Dataset class\n",
    "##########################################\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # list of k-mer count vectors\n",
    "        self.labels = labels  # list of integer-encoded labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return vector as float tensor and label as long tensor.\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.long))\n",
    "\n",
    "##########################################\n",
    "# CNN Model (using 1D convolutions)\n",
    "##########################################\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim // 8) * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, input_dim); add channel dimension.\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "##########################################\n",
    "# Distillation & Loss functions\n",
    "##########################################\n",
    "def get_overlapping_indices(teacher_label2idx, student_label2idx):\n",
    "    teacher_indices = []\n",
    "    student_indices = []\n",
    "    for label, t_idx in teacher_label2idx.items():\n",
    "        if label in student_label2idx:\n",
    "            teacher_indices.append(t_idx)\n",
    "            student_indices.append(student_label2idx[label])\n",
    "    return teacher_indices, student_indices\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, T, clip_threshold=0.9):\n",
    "    s_overlap = student_logits[:, student_overlap]  # [B, num_overlap]\n",
    "    t_overlap = teacher_logits[:, teacher_overlap]  # [B, num_overlap]\n",
    "    teacher_probs = F.softmax(t_overlap / T, dim=1)\n",
    "    teacher_probs = torch.clamp(teacher_probs, max=clip_threshold)\n",
    "    teacher_probs = teacher_probs / teacher_probs.sum(dim=1, keepdim=True)\n",
    "    kd_loss = F.kl_div(\n",
    "        F.log_softmax(s_overlap / T, dim=1),\n",
    "        teacher_probs,\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T * T)\n",
    "    return kd_loss\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "##########################################\n",
    "# Main multi-stage training pipeline\n",
    "##########################################\n",
    "# First, load the raw k-mer vector data.\n",
    "raw_data_all = parse_kmer_vector_file(kmer_vector_file)\n",
    "\n",
    "# We will store per-stage test accuracies (per run) for plotting.\n",
    "stage_test_accs = {\n",
    "    \"Student10\": [],\n",
    "    \"Student8\": [],\n",
    "    \"Student7\": [],\n",
    "    \"Student6\": [],\n",
    "    \"Student5\": []\n",
    "}\n",
    "\n",
    "# To hold teacher models for distillation between stages.\n",
    "teacher_model = None\n",
    "teacher_label2idx = None\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "temperature = 4.5\n",
    "alpha = 0.5\n",
    "\n",
    "# A dictionary to store best average accuracy for each stage.\n",
    "stage_avg_acc = {}\n",
    "\n",
    "# Stages and corresponding minimum count thresholds.\n",
    "stages = [(\"Student10\", 10), (\"Student8\", 8), (\"Student7\", 7), (\"Student6\", 6), (\"Student5\", 5)]\n",
    "\n",
    "for stage_name, min_count in stages:\n",
    "    print(f\"\\n########## {stage_name} (min_count >= {min_count}) ##########\")\n",
    "    # Filter data based on min_count.\n",
    "    stage_raw_data = filter_classes(raw_data_all, min_count=min_count)\n",
    "    print(\"Number of samples after filtering:\", len(stage_raw_data))\n",
    "    \n",
    "    # Create train-test split.\n",
    "    train_data, test_data = create_train_test_split(stage_raw_data)\n",
    "    print(\"Train samples:\", len(train_data), \"Test samples:\", len(test_data))\n",
    "    \n",
    "    # For Student6 and Student5, apply resampling to training data.\n",
    "    if stage_name in [\"Student6\", \"Student5\"]:\n",
    "        train_data = resample_dataset(train_data)\n",
    "        print(\"After resampling, train samples:\", len(train_data))\n",
    "    \n",
    "    # Get labels and vectors.\n",
    "    train_labels = [label for label, vec in train_data]\n",
    "    train_vectors = [vec for label, vec in train_data]\n",
    "    test_labels = [label for label, vec in test_data]\n",
    "    test_vectors = [vec for label, vec in test_data]\n",
    "    \n",
    "    # Create a label encoder and mapping.\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_enc = label_encoder.transform(train_labels)\n",
    "    test_labels_enc = label_encoder.transform(test_labels)\n",
    "    # Create a label-to-index dictionary.\n",
    "    student_label2idx = {label: idx for idx, label in enumerate(sorted(label_encoder.classes_))}\n",
    "    \n",
    "    # Create datasets and loaders.\n",
    "    train_dataset = KmerDataset(train_vectors, train_labels_enc)\n",
    "    test_dataset = KmerDataset(test_vectors, test_labels_enc)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    input_dim = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "    \n",
    "    # For multi-run experiments.\n",
    "    run_accs = []\n",
    "    stage_start_time = time.time()  # Start timer for the stage\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n--- {stage_name} Run {run+1}/{num_runs} ---\")\n",
    "        run_start_time = time.time()  # Start timer for this run\n",
    "        model = CNNModel(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        best_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            epoch_start_time = time.time()  # Start timer for the epoch\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                \n",
    "                # For Student10, use plain cross-entropy.\n",
    "                # For others, combine with distillation loss.\n",
    "                if stage_name == \"Student10\":\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(data)\n",
    "                    ce_loss = criterion(outputs, labels)\n",
    "                    teacher_overlap, student_overlap = get_overlapping_indices(teacher_label2idx, student_label2idx)\n",
    "                    kd_loss = distillation_loss(outputs, teacher_outputs, student_overlap, teacher_overlap, temperature, clip_threshold=0.9)\n",
    "                    if stage_name == \"Student7\":\n",
    "                        cls_loss = focal_loss(outputs, labels, alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
    "                    else:\n",
    "                        cls_loss = ce_loss\n",
    "                    loss = alpha * kd_loss + (1 - alpha) * cls_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluation on test set.\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, labels in test_loader:\n",
    "                    data, labels = data.to(device), labels.to(device)\n",
    "                    outputs = model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            test_acc = 100.0 * correct / total\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time  # Calculate epoch duration\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}% | Time: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best state.\n",
    "        model.load_state_dict(best_state)\n",
    "        # Freeze model parameters.\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "        run_duration = time.time() - run_start_time  # Calculate duration for the run\n",
    "        print(f\"Run {run+1} Best Test Accuracy: {best_acc:.2f}% | Run Duration: {run_duration:.2f} sec\")\n",
    "        run_accs.append(best_acc)\n",
    "    \n",
    "    stage_duration = time.time() - stage_start_time  # Calculate stage duration\n",
    "    avg_stage_acc = sum(run_accs) / len(run_accs)\n",
    "    stage_avg_acc[stage_name] = avg_stage_acc\n",
    "    stage_test_accs[stage_name] = run_accs\n",
    "    print(f\"\\n*** Average {stage_name} Test Accuracy over {num_runs} runs: {avg_stage_acc:.2f}% ***\")\n",
    "    print(f\"Stage {stage_name} took {stage_duration:.2f} seconds.\\n\")\n",
    "    \n",
    "    # Set teacher for next stage (if any)\n",
    "    teacher_model = model\n",
    "    teacher_label2idx = student_label2idx\n",
    "\n",
    "##########################################\n",
    "# Plotting the results\n",
    "##########################################\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stage_name, _ in stages:\n",
    "    plt.plot(range(1, num_runs+1), stage_test_accs[stage_name], marker='o', label=f\"{stage_name} Test Acc\")\n",
    "plt.xlabel(\"Run\")\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy per Run for Each Stage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print average accuracies per stage.\n",
    "for stage_name, acc in stage_avg_acc.items():\n",
    "    print(f\"{stage_name}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for training, reduced training cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pruning per stage, 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "###############################\n",
    "# File paths and constants\n",
    "###############################\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# Helper functions for k-mer vectorization\n",
    "##########################################\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "##########################################\n",
    "# Step 1: Clean FASTA headers\n",
    "##########################################\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "##########################################\n",
    "# Step 2: k-mer vectorization\n",
    "##########################################\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "##########################################\n",
    "# Utility: Parse k-mer vector file\n",
    "##########################################\n",
    "def parse_kmer_vector_file(filepath):\n",
    "    raw_data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            label = parts[0][1:]  # remove '>' prefix\n",
    "            vector = list(map(int, parts[1].split()))\n",
    "            raw_data.append((label, vector))\n",
    "    return raw_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Filter classes by minimum count\n",
    "##########################################\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    label_counts = Counter([label for label, _ in raw_data])\n",
    "    filtered_data = [(label, vec) for (label, vec) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Train-test split (per label)\n",
    "##########################################\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in raw_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, vecs in label_to_samples.items():\n",
    "        random.shuffle(vecs)\n",
    "        test_vec = vecs[0]\n",
    "        train_vecs = vecs[1:]\n",
    "        test_data.append((label, test_vec))\n",
    "        for vec in train_vecs:\n",
    "            train_data.append((label, vec))\n",
    "    return train_data, test_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Resampling to balance classes\n",
    "##########################################\n",
    "def resample_dataset(train_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in train_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    max_count = max(len(samples) for samples in label_to_samples.values())\n",
    "    resampled_data = []\n",
    "    for label, samples in label_to_samples.items():\n",
    "        sampled_vecs = random.choices(samples, k=max_count)\n",
    "        for vec in sampled_vecs:\n",
    "            resampled_data.append((label, vec))\n",
    "    random.shuffle(resampled_data)\n",
    "    return resampled_data\n",
    "\n",
    "##########################################\n",
    "# Dataset class\n",
    "##########################################\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  \n",
    "        self.labels = labels  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.long))\n",
    "\n",
    "##########################################\n",
    "# CNN Model (using 1D convolutions)\n",
    "##########################################\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim // 8) * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "##########################################\n",
    "# Distillation & Loss functions\n",
    "##########################################\n",
    "def get_overlapping_indices(teacher_label2idx, student_label2idx):\n",
    "    teacher_indices = []\n",
    "    student_indices = []\n",
    "    for label, t_idx in teacher_label2idx.items():\n",
    "        if label in student_label2idx:\n",
    "            teacher_indices.append(t_idx)\n",
    "            student_indices.append(student_label2idx[label])\n",
    "    return teacher_indices, student_indices\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, T, clip_threshold=0.9):\n",
    "    s_overlap = student_logits[:, student_overlap]\n",
    "    t_overlap = teacher_logits[:, teacher_overlap]\n",
    "    teacher_probs = F.softmax(t_overlap / T, dim=1)\n",
    "    teacher_probs = torch.clamp(teacher_probs, max=clip_threshold)\n",
    "    teacher_probs = teacher_probs / teacher_probs.sum(dim=1, keepdim=True)\n",
    "    kd_loss = F.kl_div(\n",
    "        F.log_softmax(s_overlap / T, dim=1),\n",
    "        teacher_probs,\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T * T)\n",
    "    return kd_loss\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "##########################################\n",
    "# Pruning functions for duplicate filters\n",
    "##########################################\n",
    "def prune_similar_filters(model, threshold=0.99):\n",
    "    \"\"\"\n",
    "    For each Conv1d layer, this function computes the cosine similarity\n",
    "    between each pair of filters and zeros out filters that are nearly identical.\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            weight = module.weight.data  # (out_channels, in_channels, kernel_size)\n",
    "            out_channels = weight.shape[0]\n",
    "            filters = weight.view(out_channels, -1)\n",
    "            keep_indices = []\n",
    "            pruned_indices = set()\n",
    "            for i in range(out_channels):\n",
    "                if i in pruned_indices:\n",
    "                    continue\n",
    "                keep_indices.append(i)\n",
    "                for j in range(i+1, out_channels):\n",
    "                    if j in pruned_indices:\n",
    "                        continue\n",
    "                    cos_sim = F.cosine_similarity(filters[i].unsqueeze(0), filters[j].unsqueeze(0)).item()\n",
    "                    if cos_sim > threshold:\n",
    "                        pruned_indices.add(j)\n",
    "            for j in pruned_indices:\n",
    "                module.weight.data[j] = 0\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data[j] = 0\n",
    "            print(f\"Layer {name}: kept {len(keep_indices)} out of {out_channels} filters (pruned {len(pruned_indices)})\")\n",
    "    return model\n",
    "\n",
    "def measure_inference_time(model, loader):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            _ = model(data)\n",
    "    return time.time() - start_time\n",
    "\n",
    "##########################################\n",
    "# Main multi-stage training pipeline\n",
    "##########################################\n",
    "raw_data_all = parse_kmer_vector_file(kmer_vector_file)\n",
    "\n",
    "# Store per-stage test accuracies for plotting.\n",
    "stage_test_accs = {\n",
    "    \"Student10\": [],\n",
    "    \"Student8\": [],\n",
    "    \"Student7\": [],\n",
    "    \"Student6\": [],\n",
    "    \"Student5\": []\n",
    "}\n",
    "\n",
    "# Initialize teacher variables.\n",
    "teacher_model = None\n",
    "teacher_label2idx = None\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "temperature = 4.5\n",
    "alpha = 0.5\n",
    "\n",
    "# Dictionary to store best average accuracy for each stage.\n",
    "stage_avg_acc = {}\n",
    "\n",
    "# Define stages: (stage_name, min_count)\n",
    "stages = [(\"Student10\", 10), (\"Student8\", 8), (\"Student7\", 7), (\"Student6\", 6), (\"Student5\", 5)]\n",
    "\n",
    "for stage_name, min_count in stages:\n",
    "    print(f\"\\n########## {stage_name} (min_count >= {min_count}) ##########\")\n",
    "    stage_raw_data = filter_classes(raw_data_all, min_count=min_count)\n",
    "    print(\"Number of samples after filtering:\", len(stage_raw_data))\n",
    "    \n",
    "    train_data, test_data = create_train_test_split(stage_raw_data)\n",
    "    print(\"Train samples:\", len(train_data), \"Test samples:\", len(test_data))\n",
    "    \n",
    "    if stage_name in [\"Student6\", \"Student5\"]:\n",
    "        train_data = resample_dataset(train_data)\n",
    "        print(\"After resampling, train samples:\", len(train_data))\n",
    "    \n",
    "    train_labels = [label for label, vec in train_data]\n",
    "    train_vectors = [vec for label, vec in train_data]\n",
    "    test_labels = [label for label, vec in test_data]\n",
    "    test_vectors = [vec for label, vec in test_data]\n",
    "    \n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_enc = label_encoder.transform(train_labels)\n",
    "    test_labels_enc = label_encoder.transform(test_labels)\n",
    "    student_label2idx = {label: idx for idx, label in enumerate(sorted(label_encoder.classes_))}\n",
    "    \n",
    "    train_dataset = KmerDataset(train_vectors, train_labels_enc)\n",
    "    test_dataset = KmerDataset(test_vectors, test_labels_enc)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    input_dim = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "    \n",
    "    run_accs = []\n",
    "    stage_start_time = time.time()\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n--- {stage_name} Run {run+1}/{num_runs} ---\")\n",
    "        run_start_time = time.time()\n",
    "        model = CNNModel(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        best_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            epoch_start_time = time.time()\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                if stage_name == \"Student10\":\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(data)\n",
    "                    ce_loss = criterion(outputs, labels)\n",
    "                    teacher_overlap, student_overlap = get_overlapping_indices(teacher_label2idx, student_label2idx)\n",
    "                    kd_loss = distillation_loss(outputs, teacher_outputs, student_overlap, teacher_overlap, temperature, clip_threshold=0.9)\n",
    "                    if stage_name == \"Student7\":\n",
    "                        cls_loss = focal_loss(outputs, labels, alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
    "                    else:\n",
    "                        cls_loss = ce_loss\n",
    "                    loss = alpha * kd_loss + (1 - alpha) * cls_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, labels in test_loader:\n",
    "                    data, labels = data.to(device), labels.to(device)\n",
    "                    outputs = model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            test_acc = 100.0 * correct / total\n",
    "            epoch_duration = time.time() - epoch_start_time\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}% | Time: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        model.load_state_dict(best_state)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "        run_duration = time.time() - run_start_time\n",
    "        print(f\"Run {run+1} Best Test Accuracy: {best_acc:.2f}% | Run Duration: {run_duration:.2f} sec\")\n",
    "        run_accs.append(best_acc)\n",
    "    \n",
    "    stage_duration = time.time() - stage_start_time\n",
    "    avg_stage_acc = sum(run_accs) / len(run_accs)\n",
    "    stage_avg_acc[stage_name] = avg_stage_acc\n",
    "    stage_test_accs[stage_name] = run_accs\n",
    "    print(f\"\\n*** Average {stage_name} Test Accuracy over {num_runs} runs: {avg_stage_acc:.2f}% ***\")\n",
    "    print(f\"Stage {stage_name} took {stage_duration:.2f} seconds.\\n\")\n",
    "    \n",
    "    # Prune the teacher model before using it for the next stage.\n",
    "    teacher_model = prune_similar_filters(model, threshold=0.99)\n",
    "    teacher_label2idx = student_label2idx\n",
    "\n",
    "##########################################\n",
    "# Final Inference Time Measurement\n",
    "##########################################\n",
    "orig_inference_time = measure_inference_time(teacher_model, test_loader)\n",
    "print(f\"Final teacher model inference time: {orig_inference_time:.4f} seconds.\")\n",
    "\n",
    "##########################################\n",
    "# Plotting the results\n",
    "##########################################\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stage_name, _ in stages:\n",
    "    plt.plot(range(1, num_runs+1), stage_test_accs[stage_name], marker='o', label=f\"{stage_name} Test Acc\")\n",
    "plt.xlabel(\"Run\")\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy per Run for Each Stage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for stage_name, acc in stage_avg_acc.items():\n",
    "    print(f\"{stage_name}: {acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for deployment, purn at the last stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Design:\n",
    "\n",
    "Multi-stage Training: Student10 → Student8 → Student7 → Student6 → Student5.\n",
    "Teacher Models: At each stage, the last model becomes the teacher for the next stage, but no pruning is applied at that moment.\n",
    "Final Pruning: Only after the last stage, the final teacher model is pruned before measuring inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now with pruning, prune when 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "###############################\n",
    "# File paths and constants\n",
    "###############################\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# Helper functions for k-mer vectorization\n",
    "##########################################\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "##########################################\n",
    "# Step 1: Clean FASTA headers\n",
    "##########################################\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            # Write header using the second field if available.\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "##########################################\n",
    "# Step 2: k-mer vectorization\n",
    "##########################################\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "##########################################\n",
    "# Utility: Parse k-mer vector file\n",
    "##########################################\n",
    "def parse_kmer_vector_file(filepath):\n",
    "    # Each line is of the form: \">label<TAB>vec0 vec1 ...\"\n",
    "    raw_data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            label = parts[0][1:]  # remove '>' prefix\n",
    "            vector = list(map(int, parts[1].split()))\n",
    "            raw_data.append((label, vector))\n",
    "    return raw_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Filter classes by minimum count\n",
    "##########################################\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    label_counts = Counter([label for label, _ in raw_data])\n",
    "    filtered_data = [(label, vec) for (label, vec) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Train-test split (per label)\n",
    "##########################################\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in raw_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, vecs in label_to_samples.items():\n",
    "        random.shuffle(vecs)\n",
    "        test_vec = vecs[0]\n",
    "        train_vecs = vecs[1:]\n",
    "        test_data.append((label, test_vec))\n",
    "        for vec in train_vecs:\n",
    "            train_data.append((label, vec))\n",
    "    return train_data, test_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Resampling to balance classes\n",
    "##########################################\n",
    "def resample_dataset(train_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in train_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    max_count = max(len(samples) for samples in label_to_samples.values())\n",
    "    resampled_data = []\n",
    "    for label, samples in label_to_samples.items():\n",
    "        sampled_vecs = random.choices(samples, k=max_count)\n",
    "        for vec in sampled_vecs:\n",
    "            resampled_data.append((label, vec))\n",
    "    random.shuffle(resampled_data)\n",
    "    return resampled_data\n",
    "\n",
    "##########################################\n",
    "# Dataset class\n",
    "##########################################\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # list of k-mer count vectors\n",
    "        self.labels = labels  # list of integer-encoded labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return vector as float tensor and label as long tensor.\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.long))\n",
    "\n",
    "##########################################\n",
    "# CNN Model (using 1D convolutions)\n",
    "##########################################\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim // 8) * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, input_dim); add channel dimension.\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "##########################################\n",
    "# Distillation & Loss functions\n",
    "##########################################\n",
    "def get_overlapping_indices(teacher_label2idx, student_label2idx):\n",
    "    teacher_indices = []\n",
    "    student_indices = []\n",
    "    for label, t_idx in teacher_label2idx.items():\n",
    "        if label in student_label2idx:\n",
    "            teacher_indices.append(t_idx)\n",
    "            student_indices.append(student_label2idx[label])\n",
    "    return teacher_indices, student_indices\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, T, clip_threshold=0.9):\n",
    "    s_overlap = student_logits[:, student_overlap]  # [B, num_overlap]\n",
    "    t_overlap = teacher_logits[:, teacher_overlap]  # [B, num_overlap]\n",
    "    teacher_probs = F.softmax(t_overlap / T, dim=1)\n",
    "    teacher_probs = torch.clamp(teacher_probs, max=clip_threshold)\n",
    "    teacher_probs = teacher_probs / teacher_probs.sum(dim=1, keepdim=True)\n",
    "    kd_loss = F.kl_div(\n",
    "        F.log_softmax(s_overlap / T, dim=1),\n",
    "        teacher_probs,\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T * T)\n",
    "    return kd_loss\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "##########################################\n",
    "# Pruning functions for duplicate filters\n",
    "##########################################\n",
    "def prune_similar_filters(model, threshold=0.99):\n",
    "    \"\"\"\n",
    "    For each Conv1d layer, this function computes the cosine similarity\n",
    "    between each pair of filters (after flattening) and zeros out filters\n",
    "    that are nearly identical (cosine similarity > threshold).\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            weight = module.weight.data  # shape: (out_channels, in_channels, kernel_size)\n",
    "            out_channels = weight.shape[0]\n",
    "            # Flatten each filter to a vector\n",
    "            filters = weight.view(out_channels, -1)\n",
    "            keep_indices = []\n",
    "            pruned_indices = set()\n",
    "            # Compare each filter with those already kept\n",
    "            for i in range(out_channels):\n",
    "                if i in pruned_indices:\n",
    "                    continue\n",
    "                keep_indices.append(i)\n",
    "                for j in range(i+1, out_channels):\n",
    "                    if j in pruned_indices:\n",
    "                        continue\n",
    "                    cos_sim = F.cosine_similarity(filters[i].unsqueeze(0), filters[j].unsqueeze(0)).item()\n",
    "                    if cos_sim > threshold:\n",
    "                        pruned_indices.add(j)\n",
    "            # Zero out the duplicate filters\n",
    "            for j in pruned_indices:\n",
    "                module.weight.data[j] = 0\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data[j] = 0\n",
    "            print(f\"Layer {name}: kept {len(keep_indices)} out of {out_channels} filters (pruned {len(pruned_indices)})\")\n",
    "    return model\n",
    "\n",
    "def measure_inference_time(model, loader):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            _ = model(data)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "##########################################\n",
    "# Main multi-stage training pipeline\n",
    "##########################################\n",
    "# First, load the raw k-mer vector data.\n",
    "raw_data_all = parse_kmer_vector_file(kmer_vector_file)\n",
    "\n",
    "# We will store per-stage test accuracies (per run) for plotting.\n",
    "stage_test_accs = {\n",
    "    \"Student10\": [],\n",
    "    \"Student8\": [],\n",
    "    \"Student7\": [],\n",
    "    \"Student6\": [],\n",
    "    \"Student5\": []\n",
    "}\n",
    "\n",
    "# To hold teacher models for distillation between stages.\n",
    "teacher_model = None\n",
    "teacher_label2idx = None\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "temperature = 4.5\n",
    "alpha = 0.5\n",
    "\n",
    "# A dictionary to store best average accuracy for each stage.\n",
    "stage_avg_acc = {}\n",
    "\n",
    "# Stages and corresponding minimum count thresholds.\n",
    "stages = [(\"Student10\", 10), (\"Student8\", 8), (\"Student7\", 7), (\"Student6\", 6), (\"Student5\", 5)]\n",
    "\n",
    "for stage_name, min_count in stages:\n",
    "    print(f\"\\n########## {stage_name} (min_count >= {min_count}) ##########\")\n",
    "    # Filter data based on min_count.\n",
    "    stage_raw_data = filter_classes(raw_data_all, min_count=min_count)\n",
    "    print(\"Number of samples after filtering:\", len(stage_raw_data))\n",
    "    \n",
    "    # Create train-test split.\n",
    "    train_data, test_data = create_train_test_split(stage_raw_data)\n",
    "    print(\"Train samples:\", len(train_data), \"Test samples:\", len(test_data))\n",
    "    \n",
    "    # For Student6 and Student5, apply resampling to training data.\n",
    "    if stage_name in [\"Student6\", \"Student5\"]:\n",
    "        train_data = resample_dataset(train_data)\n",
    "        print(\"After resampling, train samples:\", len(train_data))\n",
    "    \n",
    "    # Get labels and vectors.\n",
    "    train_labels = [label for label, vec in train_data]\n",
    "    train_vectors = [vec for label, vec in train_data]\n",
    "    test_labels = [label for label, vec in test_data]\n",
    "    test_vectors = [vec for label, vec in test_data]\n",
    "    \n",
    "    # Create a label encoder and mapping.\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_enc = label_encoder.transform(train_labels)\n",
    "    test_labels_enc = label_encoder.transform(test_labels)\n",
    "    # Create a label-to-index dictionary.\n",
    "    student_label2idx = {label: idx for idx, label in enumerate(sorted(label_encoder.classes_))}\n",
    "    \n",
    "    # Create datasets and loaders.\n",
    "    train_dataset = KmerDataset(train_vectors, train_labels_enc)\n",
    "    test_dataset = KmerDataset(test_vectors, test_labels_enc)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    input_dim = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "    \n",
    "    # For multi-run experiments.\n",
    "    run_accs = []\n",
    "    stage_start_time = time.time()  # Start timer for the stage\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n--- {stage_name} Run {run+1}/{num_runs} ---\")\n",
    "        run_start_time = time.time()  # Start timer for this run\n",
    "        model = CNNModel(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        best_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            epoch_start_time = time.time()  # Start timer for the epoch\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                \n",
    "                # For Student10, use plain cross-entropy.\n",
    "                # For others, combine with distillation loss.\n",
    "                if stage_name == \"Student10\":\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(data)\n",
    "                    ce_loss = criterion(outputs, labels)\n",
    "                    teacher_overlap, student_overlap = get_overlapping_indices(teacher_label2idx, student_label2idx)\n",
    "                    kd_loss = distillation_loss(outputs, teacher_outputs, student_overlap, teacher_overlap, temperature, clip_threshold=0.9)\n",
    "                    if stage_name == \"Student7\":\n",
    "                        cls_loss = focal_loss(outputs, labels, alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
    "                    else:\n",
    "                        cls_loss = ce_loss\n",
    "                    loss = alpha * kd_loss + (1 - alpha) * cls_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluation on test set.\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, labels in test_loader:\n",
    "                    data, labels = data.to(device), labels.to(device)\n",
    "                    outputs = model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            test_acc = 100.0 * correct / total\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time  # Calculate epoch duration\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}% | Time: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best state.\n",
    "        model.load_state_dict(best_state)\n",
    "        # Freeze model parameters.\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "        run_duration = time.time() - run_start_time  # Calculate duration for the run\n",
    "        print(f\"Run {run+1} Best Test Accuracy: {best_acc:.2f}% | Run Duration: {run_duration:.2f} sec\")\n",
    "        run_accs.append(best_acc)\n",
    "    \n",
    "    stage_duration = time.time() - stage_start_time  # Calculate stage duration\n",
    "    avg_stage_acc = sum(run_accs) / len(run_accs)\n",
    "    stage_avg_acc[stage_name] = avg_stage_acc\n",
    "    stage_test_accs[stage_name] = run_accs\n",
    "    print(f\"\\n*** Average {stage_name} Test Accuracy over {num_runs} runs: {avg_stage_acc:.2f}% ***\")\n",
    "    print(f\"Stage {stage_name} took {stage_duration:.2f} seconds.\\n\")\n",
    "    \n",
    "    # Set teacher for next stage (if any)\n",
    "    teacher_model = model\n",
    "    teacher_label2idx = student_label2idx\n",
    "\n",
    "##########################################\n",
    "# Evaluate inference time before pruning\n",
    "##########################################\n",
    "orig_inference_time = measure_inference_time(teacher_model, test_loader)\n",
    "print(f\"Inference time BEFORE pruning: {orig_inference_time:.4f} seconds.\")\n",
    "\n",
    "##########################################\n",
    "# Apply pruning to the model (prune duplicate filters)\n",
    "##########################################\n",
    "teacher_model = prune_similar_filters(teacher_model, threshold=0.99)\n",
    "\n",
    "##########################################\n",
    "# Evaluate inference time after pruning\n",
    "##########################################\n",
    "pruned_inference_time = measure_inference_time(teacher_model, test_loader)\n",
    "print(f\"Inference time AFTER pruning: {pruned_inference_time:.4f} seconds.\")\n",
    "\n",
    "##########################################\n",
    "# Plotting the results\n",
    "##########################################\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stage_name, _ in stages:\n",
    "    plt.plot(range(1, num_runs+1), stage_test_accs[stage_name], marker='o', label=f\"{stage_name} Test Acc\")\n",
    "plt.xlabel(\"Run\")\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy per Run for Each Stage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print average accuracies per stage.\n",
    "for stage_name, acc in stage_avg_acc.items():\n",
    "    print(f\"{stage_name}: {acc:.2f}%\")\n",
    "\n",
    "##########################################\n",
    "# Explanation of Compressed Time:\n",
    "##########################################\n",
    "# In this example, the prune_similar_filters function scans each Conv1d layer to find filters with \n",
    "# cosine similarity above a threshold (0.99 in this case) and zeros them out. After pruning, the \n",
    "# inference time is measured on the test dataset. If the inference time is lower after pruning, \n",
    "# it indicates that redundant computations (from duplicate filters) have been reduced.\n",
    "#\n",
    "# Note that zeroing out filters does not always yield a proportional reduction in runtime unless the \n",
    "# inference engine exploits the resulting sparsity. In a full model compression pipeline, you might \n",
    "# rebuild the network architecture with fewer filters for a significant speedup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now with prune, when 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "###############################\n",
    "# File paths and constants\n",
    "###############################\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "##########################################\n",
    "# Helper functions for k-mer vectorization\n",
    "##########################################\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "##########################################\n",
    "# Step 1: Clean FASTA headers\n",
    "##########################################\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            # Write header using the second field if available.\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "##########################################\n",
    "# Step 2: k-mer vectorization\n",
    "##########################################\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "##########################################\n",
    "# Utility: Parse k-mer vector file\n",
    "##########################################\n",
    "def parse_kmer_vector_file(filepath):\n",
    "    # Each line is of the form: \">label<TAB>vec0 vec1 ...\"\n",
    "    raw_data = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            label = parts[0][1:]  # remove '>' prefix\n",
    "            vector = list(map(int, parts[1].split()))\n",
    "            raw_data.append((label, vector))\n",
    "    return raw_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Filter classes by minimum count\n",
    "##########################################\n",
    "def filter_classes(raw_data, min_count=10):\n",
    "    label_counts = Counter([label for label, _ in raw_data])\n",
    "    filtered_data = [(label, vec) for (label, vec) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Train-test split (per label)\n",
    "##########################################\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in raw_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, vecs in label_to_samples.items():\n",
    "        random.shuffle(vecs)\n",
    "        test_vec = vecs[0]\n",
    "        train_vecs = vecs[1:]\n",
    "        test_data.append((label, test_vec))\n",
    "        for vec in train_vecs:\n",
    "            train_data.append((label, vec))\n",
    "    return train_data, test_data\n",
    "\n",
    "##########################################\n",
    "# Utility: Resampling to balance classes\n",
    "##########################################\n",
    "def resample_dataset(train_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, vec in train_data:\n",
    "        label_to_samples[label].append(vec)\n",
    "    max_count = max(len(samples) for samples in label_to_samples.values())\n",
    "    resampled_data = []\n",
    "    for label, samples in label_to_samples.items():\n",
    "        sampled_vecs = random.choices(samples, k=max_count)\n",
    "        for vec in sampled_vecs:\n",
    "            resampled_data.append((label, vec))\n",
    "    random.shuffle(resampled_data)\n",
    "    return resampled_data\n",
    "\n",
    "##########################################\n",
    "# Dataset class\n",
    "##########################################\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data  # list of k-mer count vectors\n",
    "        self.labels = labels  # list of integer-encoded labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return vector as float tensor and label as long tensor.\n",
    "        return (torch.tensor(self.data[idx], dtype=torch.float),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.long))\n",
    "\n",
    "##########################################\n",
    "# CNN Model (using 1D convolutions)\n",
    "##########################################\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim // 8) * 128, 1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x has shape (batch_size, input_dim); add channel dimension.\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "##########################################\n",
    "# Distillation & Loss functions\n",
    "##########################################\n",
    "def get_overlapping_indices(teacher_label2idx, student_label2idx):\n",
    "    teacher_indices = []\n",
    "    student_indices = []\n",
    "    for label, t_idx in teacher_label2idx.items():\n",
    "        if label in student_label2idx:\n",
    "            teacher_indices.append(t_idx)\n",
    "            student_indices.append(student_label2idx[label])\n",
    "    return teacher_indices, student_indices\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, T, clip_threshold=0.9):\n",
    "    s_overlap = student_logits[:, student_overlap]  # [B, num_overlap]\n",
    "    t_overlap = teacher_logits[:, teacher_overlap]  # [B, num_overlap]\n",
    "    teacher_probs = F.softmax(t_overlap / T, dim=1)\n",
    "    teacher_probs = torch.clamp(teacher_probs, max=clip_threshold)\n",
    "    teacher_probs = teacher_probs / teacher_probs.sum(dim=1, keepdim=True)\n",
    "    kd_loss = F.kl_div(\n",
    "        F.log_softmax(s_overlap / T, dim=1),\n",
    "        teacher_probs,\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T * T)\n",
    "    return kd_loss\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.25, gamma=2.0, reduction=\"mean\"):\n",
    "    ce_loss = F.cross_entropy(inputs, targets, reduction=\"none\")\n",
    "    pt = torch.exp(-ce_loss)\n",
    "    loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "    if reduction == \"mean\":\n",
    "        return loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        return loss.sum()\n",
    "    else:\n",
    "        return loss\n",
    "\n",
    "##########################################\n",
    "# Pruning functions for duplicate filters\n",
    "##########################################\n",
    "def prune_similar_filters(model, threshold=0.99):\n",
    "    \"\"\"\n",
    "    For each Conv1d layer, this function computes the cosine similarity\n",
    "    between each pair of filters (after flattening) and zeros out filters\n",
    "    that are nearly identical (cosine similarity > threshold).\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv1d):\n",
    "            weight = module.weight.data  # shape: (out_channels, in_channels, kernel_size)\n",
    "            out_channels = weight.shape[0]\n",
    "            # Flatten each filter to a vector\n",
    "            filters = weight.view(out_channels, -1)\n",
    "            keep_indices = []\n",
    "            pruned_indices = set()\n",
    "            # Compare each filter with those already kept\n",
    "            for i in range(out_channels):\n",
    "                if i in pruned_indices:\n",
    "                    continue\n",
    "                keep_indices.append(i)\n",
    "                for j in range(i+1, out_channels):\n",
    "                    if j in pruned_indices:\n",
    "                        continue\n",
    "                    cos_sim = F.cosine_similarity(filters[i].unsqueeze(0), filters[j].unsqueeze(0)).item()\n",
    "                    if cos_sim > threshold:\n",
    "                        pruned_indices.add(j)\n",
    "            # Zero out the duplicate filters\n",
    "            for j in pruned_indices:\n",
    "                module.weight.data[j] = 0\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data[j] = 0\n",
    "            print(f\"Layer {name}: kept {len(keep_indices)} out of {out_channels} filters (pruned {len(pruned_indices)})\")\n",
    "    return model\n",
    "\n",
    "def measure_inference_time(model, loader):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in loader:\n",
    "            data = data.to(device)\n",
    "            _ = model(data)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "##########################################\n",
    "# Main multi-stage training pipeline\n",
    "##########################################\n",
    "# First, load the raw k-mer vector data.\n",
    "raw_data_all = parse_kmer_vector_file(kmer_vector_file)\n",
    "\n",
    "# We will store per-stage test accuracies (per run) for plotting.\n",
    "stage_test_accs = {\n",
    "    \"Student10\": [],\n",
    "    \"Student8\": [],\n",
    "    \"Student7\": [],\n",
    "    \"Student6\": [],\n",
    "    \"Student5\": []\n",
    "}\n",
    "\n",
    "# To hold teacher models for distillation between stages.\n",
    "teacher_model = None\n",
    "teacher_label2idx = None\n",
    "\n",
    "# Hyperparameters for distillation\n",
    "temperature = 4.5\n",
    "alpha = 0.5\n",
    "\n",
    "# A dictionary to store best average accuracy for each stage.\n",
    "stage_avg_acc = {}\n",
    "\n",
    "# Stages and corresponding minimum count thresholds.\n",
    "stages = [(\"Student10\", 10), (\"Student8\", 8), (\"Student7\", 7), (\"Student6\", 6), (\"Student5\", 5)]\n",
    "\n",
    "for stage_name, min_count in stages:\n",
    "    print(f\"\\n########## {stage_name} (min_count >= {min_count}) ##########\")\n",
    "    # Filter data based on min_count.\n",
    "    stage_raw_data = filter_classes(raw_data_all, min_count=min_count)\n",
    "    print(\"Number of samples after filtering:\", len(stage_raw_data))\n",
    "    \n",
    "    # Create train-test split.\n",
    "    train_data, test_data = create_train_test_split(stage_raw_data)\n",
    "    print(\"Train samples:\", len(train_data), \"Test samples:\", len(test_data))\n",
    "    \n",
    "    # For Student6 and Student5, apply resampling to training data.\n",
    "    if stage_name in [\"Student6\", \"Student5\"]:\n",
    "        train_data = resample_dataset(train_data)\n",
    "        print(\"After resampling, train samples:\", len(train_data))\n",
    "    \n",
    "    # Get labels and vectors.\n",
    "    train_labels = [label for label, vec in train_data]\n",
    "    train_vectors = [vec for label, vec in train_data]\n",
    "    test_labels = [label for label, vec in test_data]\n",
    "    test_vectors = [vec for label, vec in test_data]\n",
    "    \n",
    "    # Create a label encoder and mapping.\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_enc = label_encoder.transform(train_labels)\n",
    "    test_labels_enc = label_encoder.transform(test_labels)\n",
    "    # Create a label-to-index dictionary.\n",
    "    student_label2idx = {label: idx for idx, label in enumerate(sorted(label_encoder.classes_))}\n",
    "    \n",
    "    # Create datasets and loaders.\n",
    "    train_dataset = KmerDataset(train_vectors, train_labels_enc)\n",
    "    test_dataset = KmerDataset(test_vectors, test_labels_enc)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    input_dim = len(train_vectors[0])\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(\"Number of classes:\", num_classes)\n",
    "    \n",
    "    # For multi-run experiments.\n",
    "    run_accs = []\n",
    "    stage_start_time = time.time()  # Start timer for the stage\n",
    "    for run in range(num_runs):\n",
    "        print(f\"\\n--- {stage_name} Run {run+1}/{num_runs} ---\")\n",
    "        run_start_time = time.time()  # Start timer for this run\n",
    "        model = CNNModel(input_dim=input_dim, num_classes=num_classes).to(device)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_acc = 0.0\n",
    "        best_state = None\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            epoch_start_time = time.time()  # Start timer for the epoch\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            for data, labels in train_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(data)\n",
    "                \n",
    "                # For Student10, use plain cross-entropy.\n",
    "                # For others, combine with distillation loss.\n",
    "                if stage_name == \"Student10\":\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        teacher_outputs = teacher_model(data)\n",
    "                    ce_loss = criterion(outputs, labels)\n",
    "                    teacher_overlap, student_overlap = get_overlapping_indices(teacher_label2idx, student_label2idx)\n",
    "                    kd_loss = distillation_loss(outputs, teacher_outputs, student_overlap, teacher_overlap, temperature, clip_threshold=0.9)\n",
    "                    if stage_name == \"Student7\":\n",
    "                        cls_loss = focal_loss(outputs, labels, alpha=0.25, gamma=2.0, reduction=\"mean\")\n",
    "                    else:\n",
    "                        cls_loss = ce_loss\n",
    "                    loss = alpha * kd_loss + (1 - alpha) * cls_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            \n",
    "            # Evaluation on test set.\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for data, labels in test_loader:\n",
    "                    data, labels = data.to(device), labels.to(device)\n",
    "                    outputs = model(data)\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "            test_acc = 100.0 * correct / total\n",
    "            \n",
    "            epoch_duration = time.time() - epoch_start_time  # Calculate epoch duration\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}% | Time: {epoch_duration:.2f} sec\")\n",
    "            \n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                best_state = model.state_dict()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        # Load best state.\n",
    "        model.load_state_dict(best_state)\n",
    "        # Freeze model parameters.\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        model.eval()\n",
    "        run_duration = time.time() - run_start_time  # Calculate duration for the run\n",
    "        print(f\"Run {run+1} Best Test Accuracy: {best_acc:.2f}% | Run Duration: {run_duration:.2f} sec\")\n",
    "        run_accs.append(best_acc)\n",
    "    \n",
    "    stage_duration = time.time() - stage_start_time  # Calculate stage duration\n",
    "    avg_stage_acc = sum(run_accs) / len(run_accs)\n",
    "    stage_avg_acc[stage_name] = avg_stage_acc\n",
    "    stage_test_accs[stage_name] = run_accs\n",
    "    print(f\"\\n*** Average {stage_name} Test Accuracy over {num_runs} runs: {avg_stage_acc:.2f}% ***\")\n",
    "    print(f\"Stage {stage_name} took {stage_duration:.2f} seconds.\\n\")\n",
    "    \n",
    "    # Set teacher for next stage (if any)\n",
    "    teacher_model = model\n",
    "    teacher_label2idx = student_label2idx\n",
    "\n",
    "##########################################\n",
    "# Evaluate inference time before pruning\n",
    "##########################################\n",
    "orig_inference_time = measure_inference_time(teacher_model, test_loader)\n",
    "print(f\"Inference time BEFORE pruning: {orig_inference_time:.4f} seconds.\")\n",
    "\n",
    "##########################################\n",
    "# Apply pruning to the model (prune duplicate filters)\n",
    "##########################################\n",
    "teacher_model = prune_similar_filters(teacher_model, threshold=0.95)\n",
    "\n",
    "##########################################\n",
    "# Evaluate inference time after pruning\n",
    "##########################################\n",
    "pruned_inference_time = measure_inference_time(teacher_model, test_loader)\n",
    "print(f\"Inference time AFTER pruning: {pruned_inference_time:.4f} seconds.\")\n",
    "\n",
    "##########################################\n",
    "# Plotting the results\n",
    "##########################################\n",
    "plt.figure(figsize=(10, 6))\n",
    "for stage_name, _ in stages:\n",
    "    plt.plot(range(1, num_runs+1), stage_test_accs[stage_name], marker='o', label=f\"{stage_name} Test Acc\")\n",
    "plt.xlabel(\"Run\")\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy per Run for Each Stage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print average accuracies per stage.\n",
    "for stage_name, acc in stage_avg_acc.items():\n",
    "    print(f\"{stage_name}: {acc:.2f}%\")\n",
    "\n",
    "##########################################\n",
    "# Explanation of Compressed Time:\n",
    "##########################################\n",
    "# In this example, the prune_similar_filters function scans each Conv1d layer to find filters with \n",
    "# cosine similarity above a threshold (0.99 in this case) and zeros them out. After pruning, the \n",
    "# inference time is measured on the test dataset. If the inference time is lower after pruning, \n",
    "# it indicates that redundant computations (from duplicate filters) have been reduced.\n",
    "#\n",
    "# Note that zeroing out filters does not always yield a proportional reduction in runtime unless the \n",
    "# inference engine exploits the resulting sparsity. In a full model compression pipeline, you might \n",
    "# rebuild the network architecture with fewer filters for a significant speedup.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
