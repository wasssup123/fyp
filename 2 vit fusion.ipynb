{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 247\n",
      "Number of training samples: 2500\n",
      "Epoch 1/200 | Loss: 4.6527 | Train Acc: 14.16% | Test Acc: 2.02%\n",
      "Epoch 2/200 | Loss: 3.3398 | Train Acc: 25.48% | Test Acc: 5.26%\n",
      "Epoch 3/200 | Loss: 2.3385 | Train Acc: 11.44% | Test Acc: 2.83%\n",
      "Epoch 4/200 | Loss: 1.5929 | Train Acc: 50.52% | Test Acc: 16.60%\n",
      "Epoch 5/200 | Loss: 1.0410 | Train Acc: 23.40% | Test Acc: 6.07%\n",
      "Epoch 6/200 | Loss: 0.6561 | Train Acc: 32.40% | Test Acc: 12.55%\n",
      "Epoch 7/200 | Loss: 0.3724 | Train Acc: 44.24% | Test Acc: 19.84%\n",
      "Epoch 8/200 | Loss: 0.2234 | Train Acc: 45.20% | Test Acc: 15.79%\n",
      "Epoch 9/200 | Loss: 0.1234 | Train Acc: 54.48% | Test Acc: 14.17%\n",
      "Epoch 10/200 | Loss: 0.0685 | Train Acc: 24.20% | Test Acc: 10.12%\n",
      "Epoch 11/200 | Loss: 0.0394 | Train Acc: 74.60% | Test Acc: 24.70%\n",
      "Epoch 12/200 | Loss: 0.0193 | Train Acc: 58.96% | Test Acc: 21.05%\n",
      "Epoch 13/200 | Loss: 0.0108 | Train Acc: 99.12% | Test Acc: 41.30%\n",
      "Epoch 14/200 | Loss: 0.0056 | Train Acc: 99.80% | Test Acc: 57.89%\n",
      "Epoch 15/200 | Loss: 0.0039 | Train Acc: 100.00% | Test Acc: 61.94%\n",
      "Epoch 16/200 | Loss: 0.0031 | Train Acc: 100.00% | Test Acc: 72.87%\n",
      "Epoch 17/200 | Loss: 0.0017 | Train Acc: 99.28% | Test Acc: 58.70%\n",
      "Epoch 18/200 | Loss: 0.0018 | Train Acc: 100.00% | Test Acc: 61.94%\n",
      "Epoch 19/200 | Loss: 0.0016 | Train Acc: 88.48% | Test Acc: 33.60%\n",
      "Epoch 20/200 | Loss: 0.0172 | Train Acc: 40.76% | Test Acc: 14.17%\n",
      "Epoch 21/200 | Loss: 0.0229 | Train Acc: 35.88% | Test Acc: 12.96%\n",
      "Epoch 22/200 | Loss: 0.0193 | Train Acc: 27.20% | Test Acc: 10.53%\n",
      "Epoch 23/200 | Loss: 0.0107 | Train Acc: 90.92% | Test Acc: 38.06%\n",
      "Epoch 24/200 | Loss: 0.0076 | Train Acc: 61.40% | Test Acc: 20.65%\n",
      "Epoch 25/200 | Loss: 0.0046 | Train Acc: 98.36% | Test Acc: 49.39%\n",
      "Epoch 26/200 | Loss: 0.0042 | Train Acc: 84.28% | Test Acc: 25.91%\n",
      "Epoch 27/200 | Loss: 0.0077 | Train Acc: 87.64% | Test Acc: 38.46%\n",
      "Epoch 28/200 | Loss: 0.0122 | Train Acc: 86.32% | Test Acc: 35.22%\n",
      "Epoch 29/200 | Loss: 0.0041 | Train Acc: 38.96% | Test Acc: 17.41%\n",
      "Epoch 30/200 | Loss: 0.0063 | Train Acc: 34.92% | Test Acc: 19.03%\n",
      "Epoch 31/200 | Loss: 0.0073 | Train Acc: 62.92% | Test Acc: 21.05%\n",
      "Epoch 32/200 | Loss: 0.0099 | Train Acc: 32.84% | Test Acc: 12.96%\n",
      "Epoch 33/200 | Loss: 0.0059 | Train Acc: 69.64% | Test Acc: 27.53%\n",
      "Epoch 34/200 | Loss: 0.0141 | Train Acc: 88.48% | Test Acc: 35.63%\n",
      "Epoch 35/200 | Loss: 0.0064 | Train Acc: 79.48% | Test Acc: 30.36%\n",
      "Epoch 36/200 | Loss: 0.0056 | Train Acc: 85.96% | Test Acc: 34.01%\n",
      "Epoch 37/200 | Loss: 0.0048 | Train Acc: 13.44% | Test Acc: 6.07%\n",
      "Epoch 38/200 | Loss: 0.0073 | Train Acc: 58.04% | Test Acc: 23.89%\n",
      "Epoch 39/200 | Loss: 0.0079 | Train Acc: 99.64% | Test Acc: 56.28%\n",
      "Epoch 40/200 | Loss: 0.0035 | Train Acc: 91.84% | Test Acc: 38.87%\n",
      "Epoch 41/200 | Loss: 0.0090 | Train Acc: 34.04% | Test Acc: 9.72%\n",
      "Epoch 42/200 | Loss: 0.0133 | Train Acc: 71.64% | Test Acc: 27.13%\n",
      "Epoch 43/200 | Loss: 0.0062 | Train Acc: 48.96% | Test Acc: 15.38%\n",
      "Epoch 44/200 | Loss: 0.0156 | Train Acc: 67.32% | Test Acc: 25.91%\n",
      "Epoch 45/200 | Loss: 0.0076 | Train Acc: 46.96% | Test Acc: 21.46%\n",
      "Epoch 46/200 | Loss: 0.0091 | Train Acc: 59.68% | Test Acc: 24.29%\n",
      "Epoch 47/200 | Loss: 0.0037 | Train Acc: 98.76% | Test Acc: 56.28%\n",
      "Epoch 48/200 | Loss: 0.0021 | Train Acc: 82.36% | Test Acc: 36.44%\n",
      "Epoch 49/200 | Loss: 0.0039 | Train Acc: 77.88% | Test Acc: 29.55%\n",
      "Epoch 50/200 | Loss: 0.0062 | Train Acc: 66.32% | Test Acc: 24.29%\n",
      "Epoch 51/200 | Loss: 0.0081 | Train Acc: 87.16% | Test Acc: 36.03%\n",
      "Epoch 52/200 | Loss: 0.0040 | Train Acc: 80.32% | Test Acc: 30.36%\n",
      "Epoch 53/200 | Loss: 0.0055 | Train Acc: 13.12% | Test Acc: 4.86%\n",
      "Epoch 54/200 | Loss: 0.0067 | Train Acc: 53.36% | Test Acc: 20.24%\n",
      "Epoch 55/200 | Loss: 0.0125 | Train Acc: 62.76% | Test Acc: 24.29%\n",
      "Epoch 56/200 | Loss: 0.0053 | Train Acc: 95.20% | Test Acc: 41.70%\n",
      "Epoch 57/200 | Loss: 0.0037 | Train Acc: 99.96% | Test Acc: 68.83%\n",
      "Epoch 58/200 | Loss: 0.0043 | Train Acc: 97.00% | Test Acc: 49.39%\n",
      "Epoch 59/200 | Loss: 0.0087 | Train Acc: 46.04% | Test Acc: 19.03%\n",
      "Epoch 60/200 | Loss: 0.0053 | Train Acc: 72.40% | Test Acc: 26.72%\n",
      "Epoch 61/200 | Loss: 0.0066 | Train Acc: 76.64% | Test Acc: 30.36%\n",
      "Epoch 62/200 | Loss: 0.0074 | Train Acc: 98.72% | Test Acc: 59.92%\n",
      "Epoch 63/200 | Loss: 0.0039 | Train Acc: 72.00% | Test Acc: 28.34%\n",
      "Epoch 64/200 | Loss: 0.0039 | Train Acc: 100.00% | Test Acc: 71.66%\n",
      "Epoch 65/200 | Loss: 0.0061 | Train Acc: 94.84% | Test Acc: 50.61%\n",
      "Epoch 66/200 | Loss: 0.0130 | Train Acc: 86.40% | Test Acc: 36.44%\n",
      "Epoch 67/200 | Loss: 0.0089 | Train Acc: 59.36% | Test Acc: 24.29%\n",
      "Epoch 68/200 | Loss: 0.0046 | Train Acc: 99.88% | Test Acc: 64.37%\n",
      "Epoch 69/200 | Loss: 0.0046 | Train Acc: 92.28% | Test Acc: 42.51%\n",
      "Epoch 70/200 | Loss: 0.0123 | Train Acc: 92.88% | Test Acc: 41.30%\n",
      "Epoch 71/200 | Loss: 0.0045 | Train Acc: 98.72% | Test Acc: 58.70%\n",
      "Epoch 72/200 | Loss: 0.0083 | Train Acc: 99.60% | Test Acc: 67.61%\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "###################################\n",
    "# 1. FASTA Parsing and Filtering (same as before)\n",
    "###################################\n",
    "def parse_fasta_with_labels(fasta_file):\n",
    "    data = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        header = record.description.strip()\n",
    "        sequence = str(record.seq).upper()\n",
    "        label = header.split()[0]\n",
    "        data.append((label, sequence))\n",
    "    return data\n",
    "\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, seq in raw_data:\n",
    "        label_to_samples[label].append(seq)\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, seqs in label_to_samples.items():\n",
    "        random.shuffle(seqs)\n",
    "        test_seq = seqs[0]\n",
    "        train_seqs = seqs[1:]\n",
    "        test_data.append((label, test_seq))\n",
    "        for s in train_seqs:\n",
    "            train_data.append((label, s))\n",
    "    return train_data, test_data\n",
    "\n",
    "def generate_kmers(sequence, k=6):\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmers.append(sequence[i:i + k])\n",
    "    return kmers\n",
    "\n",
    "def build_kmer_vocab(dataset, k=6):\n",
    "    kmer_set = set()\n",
    "    for _, seq in dataset:\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        kmer_set.update(kmers)\n",
    "    vocab = {\"<UNK>\": 0}\n",
    "    for i, kmer in enumerate(sorted(kmer_set), start=1):\n",
    "        vocab[kmer] = i\n",
    "    return vocab\n",
    "\n",
    "def encode_sequence(sequence, vocab, k=6):\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    encoded = [vocab.get(kmer, vocab[\"<UNK>\"]) for kmer in kmers]\n",
    "    return encoded\n",
    "\n",
    "def filter_classes(raw_data, min_count=5):\n",
    "    label_counts = Counter([label for (label, _) in raw_data])\n",
    "    filtered_data = [(label, seq) for (label, seq) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    # For simplicity, we just reverse the sequence.\n",
    "    return seq[::-1]\n",
    "\n",
    "def create_paired_data(data_list):\n",
    "    paired = []\n",
    "    for label, seq in data_list:\n",
    "        rev_seq = reverse_complement(seq)\n",
    "        paired.append((label, seq, rev_seq))\n",
    "    return paired\n",
    "\n",
    "class TwoFastaKmerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: (encoded_seq_fwd, encoded_seq_rev, label_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, paired_data, vocab, k=6):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.k = k\n",
    "        labels = sorted(set(item[0] for item in paired_data))\n",
    "        self.label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "        self.encoded_data = []\n",
    "        for label, fwd_seq, rev_seq in paired_data:\n",
    "            x1 = encode_sequence(fwd_seq, self.vocab, k=self.k)\n",
    "            x2 = encode_sequence(rev_seq, self.vocab, k=self.k)\n",
    "            y = self.label2idx[label]\n",
    "            self.encoded_data.append((x1, x2, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label2idx)\n",
    "\n",
    "def collate_fn_two(batch):\n",
    "    seqs_fwd, seqs_rev, labels = zip(*batch)\n",
    "    seq_fwd_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_fwd]\n",
    "    seq_rev_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_rev]\n",
    "    padded_fwd = pad_sequence(seq_fwd_tensors, batch_first=True, padding_value=0)\n",
    "    padded_rev = pad_sequence(seq_rev_tensors, batch_first=True, padding_value=0)\n",
    "    labels_tensors = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_fwd, padded_rev, labels_tensors\n",
    "\n",
    "###################################\n",
    "# 2. ViTDeepSEAEncoder with Normalization (Branch used in each pathway)\n",
    "###################################\n",
    "class ViTDeepSEAEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This module performs:\n",
    "      1. k-mer embedding.\n",
    "      2. A DeepSEA-style conv block.\n",
    "      3. A projection of the conv feature maps to d_model.\n",
    "      4. Addition of positional embeddings.\n",
    "      5. Transformer encoding (ViT style) over the resulting tokens.\n",
    "      6. Pooling over tokens to produce a fixed-length vector.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=128,\n",
    "        d_model=256,  # Each branch outputs a 256-D vector\n",
    "        num_conv_filters=(320, 480, 960),\n",
    "        conv_kernel_sizes=(8, 8, 8),\n",
    "        pool_kernel_sizes=(4, 4),\n",
    "        num_transformer_layers=2,\n",
    "        nhead=8,\n",
    "        dropout=0.1,\n",
    "        max_seq_len=1000  # maximum expected sequence length (in k-mer tokens)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        # DeepSEA-style conv block. Conv1d expects input shape [B, channels, L]\n",
    "        self.deepsea_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_conv_filters[0], kernel_size=conv_kernel_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_sizes[0]),\n",
    "            nn.Conv1d(in_channels=num_conv_filters[0], out_channels=num_conv_filters[1], kernel_size=conv_kernel_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_sizes[1]),\n",
    "            nn.Conv1d(in_channels=num_conv_filters[1], out_channels=num_conv_filters[2], kernel_size=conv_kernel_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # --- Normalization Layer Added Here ---\n",
    "        # BatchNorm1d is applied on the conv output with shape [B, channels, L]\n",
    "        self.bn = nn.BatchNorm1d(num_conv_filters[2])\n",
    "        # Project each conv output token (feature map column) to d_model\n",
    "        self.proj = nn.Linear(num_conv_filters[2], d_model)\n",
    "        # Define a fixed number of positional embeddings (adjust as needed)\n",
    "        self.max_tokens = 150  \n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, self.max_tokens, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, seq_len] (k-mer token indices)\n",
    "        x = self.embedding(x)            # [B, seq_len, embed_dim]\n",
    "        x = x.transpose(1, 2)            # [B, embed_dim, seq_len] for Conv1d\n",
    "        x = self.deepsea_conv(x)         # [B, num_conv_filters[-1], L_out]\n",
    "        x = self.bn(x)                   # Apply batch normalization\n",
    "        x = x.transpose(1, 2)            # [B, L_out, num_conv_filters[-1]]\n",
    "        x = self.proj(x)                 # [B, L_out, d_model]\n",
    "        B, L, _ = x.size()\n",
    "        # Use positional embeddings (truncate if necessary)\n",
    "        pos_embed = self.pos_embedding[:, :L, :]\n",
    "        x = x + pos_embed\n",
    "        x = self.transformer_encoder(x)  # [B, L, d_model]\n",
    "        # Pool over tokens (mean pooling)\n",
    "        x = x.mean(dim=1)                # [B, d_model]\n",
    "        return x\n",
    "\n",
    "###################################\n",
    "# 3. Two-Branch Fusion Model with FC Head\n",
    "###################################\n",
    "class TwoViTDeepSEAFusionDNAClassifierWithFC(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_classes,\n",
    "        embed_dim=128,\n",
    "        d_model=256,  # Each branch outputs 256-D features; concat gives 512-D\n",
    "        num_conv_filters=(320, 480, 960),\n",
    "        conv_kernel_sizes=(8, 8, 8),\n",
    "        pool_kernel_sizes=(4, 4),\n",
    "        num_transformer_layers=2,\n",
    "        nhead=8,\n",
    "        dropout=0.1,\n",
    "        max_seq_len=1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Two independent branches for forward and reverse sequences.\n",
    "        self.vit_branch1 = ViTDeepSEAEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            d_model=d_model,\n",
    "            num_conv_filters=num_conv_filters,\n",
    "            conv_kernel_sizes=conv_kernel_sizes,\n",
    "            pool_kernel_sizes=pool_kernel_sizes,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.vit_branch2 = ViTDeepSEAEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            d_model=d_model,\n",
    "            num_conv_filters=num_conv_filters,\n",
    "            conv_kernel_sizes=conv_kernel_sizes,\n",
    "            pool_kernel_sizes=pool_kernel_sizes,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        # Fully connected layer: 512 -> num_classes\n",
    "        self.fc = nn.Linear(2 * d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # x1: [B, seq_len1] (forward sequence)\n",
    "        # x2: [B, seq_len2] (reverse sequence)\n",
    "        f1 = self.vit_branch1(x1)  # [B, d_model] -> [B, 256]\n",
    "        f2 = self.vit_branch2(x2)  # [B, d_model] -> [B, 256]\n",
    "        fused = torch.cat([f1, f2], dim=1)  # [B, 512]\n",
    "        logits = self.fc(fused)             # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "###################################\n",
    "# 4. Putting It All Together\n",
    "###################################\n",
    "fasta_file = \"data2/fungi_ITS_cleaned.fasta\"\n",
    "\n",
    "raw_data = parse_fasta_with_labels(fasta_file)\n",
    "raw_data = filter_classes(raw_data, min_count=5)  # Use classes with at least 5 samples\n",
    "train_data, test_data = create_train_test_split(raw_data)\n",
    "\n",
    "paired_train = create_paired_data(train_data)\n",
    "paired_test  = create_paired_data(test_data)\n",
    "\n",
    "# Build vocabulary from both forward and reverse sequences\n",
    "combined_paired = paired_train + paired_test\n",
    "tmp_data = []\n",
    "for (lbl, fwd, rev) in combined_paired:\n",
    "    tmp_data.append((lbl, fwd))\n",
    "    tmp_data.append((lbl, rev))\n",
    "k = 6\n",
    "vocab = build_kmer_vocab(tmp_data, k=k)\n",
    "\n",
    "train_dataset = TwoFastaKmerDataset(paired_train, vocab, k=k)\n",
    "test_dataset  = TwoFastaKmerDataset(paired_test,  vocab, k=k)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_two)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "vocab_size = train_dataset.get_vocab_size()\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "\n",
    "# Create the model with the FC head.\n",
    "model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes,\n",
    "    embed_dim=128,\n",
    "    d_model=256,  # Each branch outputs 256-D features => concat gives 512-D\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.1,\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# --- Lower Learning Rate Applied Here (point 1) ---\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "###################################\n",
    "# 5. Train & Evaluate with Gradient Clipping (point 2)\n",
    "###################################\n",
    "def evaluate_accuracy(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for fwd, rev, labels in data_loader:\n",
    "            fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "            logits = model(fwd, rev)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "# Lists to store accuracy for plotting\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "epochs = 200\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(fwd, rev)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        # --- Gradient Clipping Applied Here (point 2) ---\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_acc = evaluate_accuracy(model, train_loader)\n",
    "    test_acc  = evaluate_accuracy(model, test_loader)\n",
    "    \n",
    "    # Append the accuracies for plotting\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6. Plotting the Training and Testing Accuracy\n",
    "###################################\n",
    "epochs_range = range(1, epochs + 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, train_acc_list, label=\"Train Accuracy\")\n",
    "plt.plot(epochs_range, test_acc_list, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Training and Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
