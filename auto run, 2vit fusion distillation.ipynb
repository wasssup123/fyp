{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Device Setup\n",
    "Import math, random, collections, torch, torch.nn, torch.optim, torch.nn.functional, DataLoader, pad_sequence, Bio.SeqIO, and matplotlib.pyplot. Set the device based on cuda/mps availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # For KL divergence in distillation\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from Bio import SeqIO\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# Set device\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Functions\n",
    "Define functions: parse_fasta_with_labels, create_train_test_split, generate_kmers, build_kmer_vocab, encode_sequence, filter_classes, reverse_complement, create_paired_data, and the TwoFastaKmerDataset class with collate_fn_two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation Functions\n",
    "\n",
    "def parse_fasta_with_labels(fasta_file):\n",
    "    data = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        header = record.description.strip()\n",
    "        sequence = str(record.seq).upper()\n",
    "        label = header.split()[0]\n",
    "        data.append((label, sequence))\n",
    "    return data\n",
    "\n",
    "def create_train_test_split(raw_data):\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for label, seq in raw_data:\n",
    "        label_to_samples[label].append(seq)\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    for label, seqs in label_to_samples.items():\n",
    "        random.shuffle(seqs)\n",
    "        test_seq = seqs[0]\n",
    "        train_seqs = seqs[1:]\n",
    "        test_data.append((label, test_seq))\n",
    "        for s in train_seqs:\n",
    "            train_data.append((label, s))\n",
    "    return train_data, test_data\n",
    "\n",
    "def generate_kmers(sequence, k=6):\n",
    "    kmers = []\n",
    "    for i in range(len(sequence) - k + 1):\n",
    "        kmers.append(sequence[i:i+k])\n",
    "    return kmers\n",
    "\n",
    "def build_kmer_vocab(dataset, k=6):\n",
    "    kmer_set = set()\n",
    "    for _, seq in dataset:\n",
    "        kmers = generate_kmers(seq, k)\n",
    "        kmer_set.update(kmers)\n",
    "    vocab = {\"<UNK>\": 0}\n",
    "    for i, kmer in enumerate(sorted(kmer_set), start=1):\n",
    "        vocab[kmer] = i\n",
    "    return vocab\n",
    "\n",
    "def encode_sequence(sequence, vocab, k=6):\n",
    "    kmers = generate_kmers(sequence, k)\n",
    "    encoded = [vocab.get(kmer, vocab[\"<UNK>\"]) for kmer in kmers]\n",
    "    return encoded\n",
    "\n",
    "def filter_classes(raw_data, min_count=5):\n",
    "    label_counts = Counter([label for (label, _) in raw_data])\n",
    "    filtered_data = [(label, seq) for (label, seq) in raw_data if label_counts[label] >= min_count]\n",
    "    return filtered_data\n",
    "\n",
    "def reverse_complement(seq):\n",
    "    # For simplicity, we just reverse the sequence.\n",
    "    return seq[::-1]\n",
    "\n",
    "def create_paired_data(data_list):\n",
    "    paired = []\n",
    "    for label, seq in data_list:\n",
    "        rev_seq = reverse_complement(seq)\n",
    "        paired.append((label, seq, rev_seq))\n",
    "    return paired\n",
    "\n",
    "class TwoFastaKmerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item: (encoded_seq_fwd, encoded_seq_rev, label_idx)\n",
    "    \"\"\"\n",
    "    def __init__(self, paired_data, vocab, k=6):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.k = k\n",
    "        labels = sorted(set(item[0] for item in paired_data))\n",
    "        self.label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "        self.encoded_data = []\n",
    "        for label, fwd_seq, rev_seq in paired_data:\n",
    "            x1 = encode_sequence(fwd_seq, self.vocab, k=self.k)\n",
    "            x2 = encode_sequence(rev_seq, self.vocab, k=self.k)\n",
    "            y = self.label2idx[label]\n",
    "            self.encoded_data.append((x1, x2, y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encoded_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_data[idx]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def get_num_classes(self):\n",
    "        return len(self.label2idx)\n",
    "\n",
    "def collate_fn_two(batch):\n",
    "    seqs_fwd, seqs_rev, labels = zip(*batch)\n",
    "    seqs_fwd_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_fwd]\n",
    "    seqs_rev_tensors = [torch.tensor(s, dtype=torch.long) for s in seqs_rev]\n",
    "    padded_fwd = pad_sequence(seqs_fwd_tensors, batch_first=True, padding_value=0)\n",
    "    padded_rev = pad_sequence(seqs_rev_tensors, batch_first=True, padding_value=0)\n",
    "    labels_tensors = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_fwd, padded_rev, labels_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "Define ViTDeepSEAEncoder and TwoViTDeepSEAFusionDNAClassifierWithFC classes to build the two-branch network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "class ViTDeepSEAEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    k-mer embedding, DeepSEA-style conv block, projection,\n",
    "    positional embedding, Transformer encoding, and mean pooling.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim=128,\n",
    "        d_model=256,\n",
    "        num_conv_filters=(320, 480, 960),\n",
    "        conv_kernel_sizes=(8, 8, 8),\n",
    "        pool_kernel_sizes=(4, 4),\n",
    "        num_transformer_layers=2,\n",
    "        nhead=8,\n",
    "        dropout=0.2,\n",
    "        max_seq_len=1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.deepsea_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=embed_dim, out_channels=num_conv_filters[0], kernel_size=conv_kernel_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_sizes[0]),\n",
    "            nn.Conv1d(in_channels=num_conv_filters[0], out_channels=num_conv_filters[1], kernel_size=conv_kernel_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=pool_kernel_sizes[1]),\n",
    "            nn.Conv1d(in_channels=num_conv_filters[1], out_channels=num_conv_filters[2], kernel_size=conv_kernel_sizes[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.bn = nn.BatchNorm1d(num_conv_filters[2])\n",
    "        self.proj = nn.Linear(num_conv_filters[2], d_model)\n",
    "        self.max_tokens = 150\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, self.max_tokens, d_model))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_transformer_layers)\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)            # [B, seq_len, embed_dim]\n",
    "        x = x.transpose(1, 2)            # [B, embed_dim, seq_len]\n",
    "        x = self.deepsea_conv(x)         # [B, num_conv_filters[-1], L_out]\n",
    "        x = self.bn(x)\n",
    "        x = x.transpose(1, 2)            # [B, L_out, num_conv_filters[-1]]\n",
    "        x = self.proj(x)                 # [B, L_out, d_model]\n",
    "        B, L, _ = x.size()\n",
    "        pos_embed = self.pos_embedding[:, :L, :]\n",
    "        x = x + pos_embed\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.final_norm(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return x\n",
    "\n",
    "class TwoViTDeepSEAFusionDNAClassifierWithFC(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-branch model: processes forward and reverse sequences,\n",
    "    concatenates features, and applies a fully-connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        num_classes,\n",
    "        embed_dim=128,\n",
    "        d_model=256,\n",
    "        num_conv_filters=(320, 480, 960),\n",
    "        conv_kernel_sizes=(8, 8, 8),\n",
    "        pool_kernel_sizes=(4, 4),\n",
    "        num_transformer_layers=2,\n",
    "        nhead=8,\n",
    "        dropout=0.2,\n",
    "        max_seq_len=1000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vit_branch1 = ViTDeepSEAEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            d_model=d_model,\n",
    "            num_conv_filters=num_conv_filters,\n",
    "            conv_kernel_sizes=conv_kernel_sizes,\n",
    "            pool_kernel_sizes=pool_kernel_sizes,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.vit_branch2 = ViTDeepSEAEncoder(\n",
    "            vocab_size=vocab_size,\n",
    "            embed_dim=embed_dim,\n",
    "            d_model=d_model,\n",
    "            num_conv_filters=num_conv_filters,\n",
    "            conv_kernel_sizes=conv_kernel_sizes,\n",
    "            pool_kernel_sizes=pool_kernel_sizes,\n",
    "            num_transformer_layers=num_transformer_layers,\n",
    "            nhead=nhead,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.fc = nn.Linear(2 * d_model, num_classes)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        f1 = self.vit_branch1(x1)\n",
    "        f2 = self.vit_branch2(x2)\n",
    "        fused = torch.cat([f1, f2], dim=1)\n",
    "        logits = self.fc(fused)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for Distillation\n",
    "Create get_overlapping_indices to match teacher and student label indices, and distillation_loss to compute KL divergence on overlapping classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Distillation\n",
    "\n",
    "def get_overlapping_indices(teacher_label2idx, student_label2idx):\n",
    "    \"\"\"\n",
    "    Returns two lists of indices corresponding to overlapping classes.\n",
    "    \"\"\"\n",
    "    teacher_indices = []\n",
    "    student_indices = []\n",
    "    for label, t_idx in teacher_label2idx.items():\n",
    "        if label in student_label2idx:\n",
    "            teacher_indices.append(t_idx)\n",
    "            student_indices.append(student_label2idx[label])\n",
    "    return teacher_indices, student_indices\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, T):\n",
    "    \"\"\"\n",
    "    Computes KL divergence over the overlapping classes only.\n",
    "    \"\"\"\n",
    "    s_overlap = student_logits[:, student_overlap]  # [B, num_overlap]\n",
    "    t_overlap = teacher_logits[:, teacher_overlap]  # [B, num_overlap]\n",
    "    return F.kl_div(F.log_softmax(s_overlap / T, dim=1),\n",
    "                    F.softmax(t_overlap / T, dim=1),\n",
    "                    reduction=\"batchmean\") * (T * T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocabulary and Dataset\n",
    "Load the FASTA file, generate raw_data using parse_fasta_with_labels, build the kmer vocabulary, and prepare dataset splits for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Vocabulary and Dataset\n",
    "\n",
    "# Load the FASTA file\n",
    "fasta_file = \"data2/fungi_ITS_cleaned.fasta\"\n",
    "raw_data = parse_fasta_with_labels(fasta_file)\n",
    "\n",
    "# Build the kmer vocabulary\n",
    "vocab = build_kmer_vocab(raw_data, k=6)\n",
    "\n",
    "# Prepare dataset splits for training and testing\n",
    "train_data, test_data = create_train_test_split(raw_data)\n",
    "\n",
    "# Create paired data for training and testing\n",
    "paired_train_data = create_paired_data(train_data)\n",
    "paired_test_data = create_paired_data(test_data)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TwoFastaKmerDataset(paired_train_data, vocab, k=6)\n",
    "test_dataset = TwoFastaKmerDataset(paired_test_data, vocab, k=6)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))\n",
    "print(\"Vocabulary size:\", train_dataset.get_vocab_size())\n",
    "print(\"Number of classes:\", train_dataset.get_num_classes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Student 10 Training\n",
    "Train Student 10 on the dataset with min_count >= 10, including early stopping, LR scheduling, and saving the best model state. This model will serve as the teacher for subsequent stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Student 10 Training\n",
    "\n",
    "# Filter classes with min_count >= 10\n",
    "student10_data = filter_classes(raw_data, min_count=10)\n",
    "\n",
    "# Create train-test split\n",
    "student10_train_data, student10_test_data = create_train_test_split(student10_data)\n",
    "\n",
    "# Create paired data\n",
    "student10_paired_train = create_paired_data(student10_train_data)\n",
    "student10_paired_test = create_paired_data(student10_test_data)\n",
    "\n",
    "# Create datasets\n",
    "student10_dataset = TwoFastaKmerDataset(student10_paired_train, vocab, k=6)\n",
    "student10_test_dataset = TwoFastaKmerDataset(student10_paired_test, vocab, k=6)\n",
    "\n",
    "# Print class information\n",
    "print(\"Student 10 classes (min_count>=10):\")\n",
    "for cls in student10_dataset.label2idx:\n",
    "    print(cls)\n",
    "print(\"Number of Student 10 classes:\", student10_dataset.get_num_classes())\n",
    "\n",
    "# Create data loaders\n",
    "student10_train_loader = DataLoader(student10_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn_two)\n",
    "student10_test_loader = DataLoader(student10_test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "# Model parameters\n",
    "num_classes_student10 = student10_dataset.get_num_classes()\n",
    "vocab_size = student10_dataset.get_vocab_size()\n",
    "\n",
    "# Initialize model\n",
    "student10_model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes_student10,\n",
    "    embed_dim=128,\n",
    "    d_model=256,\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.2,\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "student10_epochs = 100\n",
    "optimizer_student10 = optim.AdamW(student10_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler_student10 = optim.lr_scheduler.CosineAnnealingLR(optimizer_student10, T_max=student10_epochs)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_student10_acc = 0.0\n",
    "best_student10_state = None\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# List to store test accuracy per epoch\n",
    "teacher_test_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting Student 10 training (min_count>=10) with early stopping and LR scheduling...\")\n",
    "for epoch in range(1, student10_epochs + 1):\n",
    "    student10_model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in student10_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer_student10.zero_grad()\n",
    "        logits = student10_model(fwd, rev)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student10_model.parameters(), max_norm=1.0)\n",
    "        optimizer_student10.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(student10_train_loader)\n",
    "    \n",
    "    # Evaluate Student 10 accuracy\n",
    "    student10_model.eval()\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for fwd, rev, labels in student10_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student10_model(fwd, rev), dim=1)\n",
    "        train_correct += (preds == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    train_acc = 100.0 * train_correct / train_total\n",
    "    \n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    for fwd, rev, labels in student10_test_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student10_model(fwd, rev), dim=1)\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "    test_acc = 100.0 * test_correct / test_total\n",
    "    teacher_test_acc_list.append(test_acc)\n",
    "    \n",
    "    scheduler_student10.step(test_acc)\n",
    "    \n",
    "    print(f\"[Student 10] Epoch {epoch}/{student10_epochs} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2f}% | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    if test_acc > best_student10_acc:\n",
    "        best_student10_acc = test_acc\n",
    "        best_student10_state = student10_model.state_dict()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "# Load the best model state\n",
    "student10_model.load_state_dict(best_student10_state)\n",
    "for param in student10_model.parameters():\n",
    "    param.requires_grad = False\n",
    "student10_model.eval()\n",
    "print(f\"Best Student 10 Accuracy: {best_student10_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2: Student 8 Training with Distillation\n",
    "Filter the dataset for classes with min_count >= 8; create paired data; train Student 8 using the teacher logits from Student 10 combined with cross-entropy, applying KD with given temperature and alpha hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Student 8 Training with Distillation\n",
    "\n",
    "# Filter classes with min_count >= 8\n",
    "student8_data = filter_classes(raw_data, min_count=8)\n",
    "\n",
    "# Create train-test split\n",
    "student8_train_data, student8_test_data = create_train_test_split(student8_data)\n",
    "\n",
    "# Create paired data\n",
    "student8_paired_train = create_paired_data(student8_train_data)\n",
    "student8_paired_test = create_paired_data(student8_test_data)\n",
    "\n",
    "# Create datasets\n",
    "student8_dataset = TwoFastaKmerDataset(student8_paired_train, vocab, k=6)\n",
    "student8_test_dataset = TwoFastaKmerDataset(student8_paired_test, vocab, k=6)\n",
    "\n",
    "# Print class information\n",
    "print(\"\\nStudent 8 classes (min_count>=8):\")\n",
    "for cls in student8_dataset.label2idx:\n",
    "    print(cls)\n",
    "print(\"Number of Student 8 classes:\", student8_dataset.get_num_classes())\n",
    "\n",
    "# Create data loaders\n",
    "student8_train_loader = DataLoader(student8_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_two)\n",
    "student8_test_loader = DataLoader(student8_test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "# Model parameters\n",
    "num_classes_student8 = student8_dataset.get_num_classes()\n",
    "\n",
    "# Initialize model\n",
    "student8_model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes_student8,\n",
    "    embed_dim=128,\n",
    "    d_model=256,\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.2,\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer_student8 = optim.AdamW(student8_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Distillation hyperparameters\n",
    "temperature = 4.0\n",
    "alpha = 0.5\n",
    "\n",
    "# Get overlapping indices between Student 10 (teacher) and Student 8 (student)\n",
    "teacher_overlap, student_overlap = get_overlapping_indices(student10_dataset.label2idx, student8_dataset.label2idx)\n",
    "print(\"\\nOverlapping classes for KD between Student 10 and Student 8:\")\n",
    "for label in student10_dataset.label2idx:\n",
    "    if label in student8_dataset.label2idx:\n",
    "        print(label)\n",
    "print(\"Number of overlapping classes:\", len(teacher_overlap))\n",
    "\n",
    "# List to store test accuracy per epoch\n",
    "student8_test_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "student8_epochs = 100\n",
    "best_student8_acc = 0.0\n",
    "best_student8_state = None\n",
    "\n",
    "print(\"\\nStarting Student 8 training with distillation from Student 10...\")\n",
    "for epoch in range(1, student8_epochs + 1):\n",
    "    student8_model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in student8_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer_student8.zero_grad()\n",
    "        student_logits = student8_model(fwd, rev)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = student10_model(fwd, rev)\n",
    "        ce_loss = criterion(student_logits, labels)\n",
    "        kd_loss = distillation_loss(student_logits, teacher_logits, student_overlap, teacher_overlap, temperature)\n",
    "        loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student8_model.parameters(), max_norm=1.0)\n",
    "        optimizer_student8.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(student8_train_loader)\n",
    "    \n",
    "    # Evaluate Student 8 accuracy\n",
    "    student8_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for fwd, rev, labels in student8_test_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student8_model(fwd, rev), dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    student8_test_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"[Student 8] Epoch {epoch}/{student8_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    if test_acc > best_student8_acc:\n",
    "        best_student8_acc = test_acc\n",
    "        best_student8_state = student8_model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "student8_model.load_state_dict(best_student8_state)\n",
    "for param in student8_model.parameters():\n",
    "    param.requires_grad = False\n",
    "student8_model.eval()\n",
    "print(f\"\\nHighest Student 8 Test Accuracy: {best_student8_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 3: Student 7 Training with Distillation\n",
    "Filter the dataset for min_count >= 7, prepare paired data, and train Student 7 with KD from Student 8, including the computation of overlapping indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 3: Student 7 Training with Distillation\n",
    "\n",
    "# Filter classes with min_count >= 7\n",
    "student7_data = filter_classes(raw_data, min_count=7)\n",
    "\n",
    "# Create train-test split\n",
    "student7_train_data, student7_test_data = create_train_test_split(student7_data)\n",
    "\n",
    "# Create paired data\n",
    "student7_paired_train = create_paired_data(student7_train_data)\n",
    "student7_paired_test = create_paired_data(student7_test_data)\n",
    "\n",
    "# Create datasets\n",
    "student7_dataset = TwoFastaKmerDataset(student7_paired_train, vocab, k=6)\n",
    "student7_test_dataset = TwoFastaKmerDataset(student7_paired_test, vocab, k=6)\n",
    "\n",
    "# Print class information\n",
    "print(\"\\nStudent 7 classes (min_count>=7):\")\n",
    "for cls in student7_dataset.label2idx:\n",
    "    print(cls)\n",
    "print(\"Number of Student 7 classes:\", student7_dataset.get_num_classes())\n",
    "\n",
    "# Create data loaders\n",
    "student7_train_loader = DataLoader(student7_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_two)\n",
    "student7_test_loader = DataLoader(student7_test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "# Model parameters\n",
    "num_classes_student7 = student7_dataset.get_num_classes()\n",
    "\n",
    "# Initialize model\n",
    "student7_model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes_student7,\n",
    "    embed_dim=128,\n",
    "    d_model=256,\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.2,\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer_student7 = optim.AdamW(student7_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get overlapping indices between Student 8 (teacher) and Student 7 (student)\n",
    "teacher_overlap_7, student_overlap_7 = get_overlapping_indices(student8_dataset.label2idx, student7_dataset.label2idx)\n",
    "print(\"\\nOverlapping classes for KD between Student 8 and Student 7:\")\n",
    "for label in student8_dataset.label2idx:\n",
    "    if label in student7_dataset.label2idx:\n",
    "        print(label)\n",
    "print(\"Number of overlapping classes:\", len(teacher_overlap_7))\n",
    "\n",
    "# List to store test accuracy per epoch\n",
    "student7_test_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "student7_epochs = 100\n",
    "best_student7_acc = 0.0\n",
    "best_student7_state = None\n",
    "\n",
    "print(\"\\nStarting Student 7 training with distillation from Student 8...\")\n",
    "for epoch in range(1, student7_epochs + 1):\n",
    "    student7_model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in student7_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer_student7.zero_grad()\n",
    "        student_logits = student7_model(fwd, rev)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = student8_model(fwd, rev)\n",
    "        ce_loss = criterion(student_logits, labels)\n",
    "        kd_loss = distillation_loss(student_logits, teacher_logits, student_overlap_7, teacher_overlap_7, temperature)\n",
    "        loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student7_model.parameters(), max_norm=1.0)\n",
    "        optimizer_student7.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(student7_train_loader)\n",
    "    \n",
    "    # Evaluate Student 7 accuracy\n",
    "    student7_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for fwd, rev, labels in student7_test_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student7_model(fwd, rev), dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    student7_test_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"[Student 7] Epoch {epoch}/{student7_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    if test_acc > best_student7_acc:\n",
    "        best_student7_acc = test_acc\n",
    "        best_student7_state = student7_model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "student7_model.load_state_dict(best_student7_state)\n",
    "for param in student7_model.parameters():\n",
    "    param.requires_grad = False\n",
    "student7_model.eval()\n",
    "print(f\"\\nHighest Student 7 Test Accuracy: {best_student7_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 4: Student 6 Training with Distillation\n",
    "Using min_count >= 6, prepare the dataset and train Student 6 with distillation from Student 7. Implement similar KD loss and cross-entropy loss fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 4: Student 6 Training with Distillation\n",
    "\n",
    "# Filter classes with min_count >= 6\n",
    "student6_data = filter_classes(raw_data, min_count=6)\n",
    "\n",
    "# Create train-test split\n",
    "student6_train_data, student6_test_data = create_train_test_split(student6_data)\n",
    "\n",
    "# Create paired data\n",
    "student6_paired_train = create_paired_data(student6_train_data)\n",
    "student6_paired_test = create_paired_data(student6_test_data)\n",
    "\n",
    "# Create datasets\n",
    "student6_dataset = TwoFastaKmerDataset(student6_paired_train, vocab, k=6)\n",
    "student6_test_dataset = TwoFastaKmerDataset(student6_paired_test, vocab, k=6)\n",
    "\n",
    "# Print class information\n",
    "print(\"\\nStudent 6 classes (min_count>=6):\")\n",
    "for cls in student6_dataset.label2idx:\n",
    "    print(cls)\n",
    "print(\"Number of Student 6 classes:\", student6_dataset.get_num_classes())\n",
    "\n",
    "# Create data loaders\n",
    "student6_train_loader = DataLoader(student6_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_two)\n",
    "student6_test_loader = DataLoader(student6_test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "# Model parameters\n",
    "num_classes_student6 = student6_dataset.get_num_classes()\n",
    "\n",
    "# Initialize model\n",
    "student6_model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes_student6,\n",
    "    embed_dim=128,\n",
    "    d_model=256,\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=2,\n",
    "    nhead=8,\n",
    "    dropout=0.2,\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer_student6 = optim.AdamW(student6_model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get overlapping indices between Student 7 (teacher) and Student 6 (student)\n",
    "teacher_overlap_6, student_overlap_6 = get_overlapping_indices(student7_dataset.label2idx, student6_dataset.label2idx)\n",
    "print(\"\\nOverlapping classes for KD between Student 7 and Student 6:\")\n",
    "for label in student7_dataset.label2idx:\n",
    "    if label in student6_dataset.label2idx:\n",
    "        print(label)\n",
    "print(\"Number of overlapping classes:\", len(teacher_overlap_6))\n",
    "\n",
    "# List to store test accuracy per epoch\n",
    "student6_test_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "student6_epochs = 100\n",
    "best_student6_acc = 0.0\n",
    "best_student6_state = None\n",
    "\n",
    "print(\"\\nStarting Student 6 training with distillation from Student 7...\")\n",
    "for epoch in range(1, student6_epochs + 1):\n",
    "    student6_model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in student6_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer_student6.zero_grad()\n",
    "        student_logits = student6_model(fwd, rev)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = student7_model(fwd, rev)\n",
    "        ce_loss = criterion(student_logits, labels)\n",
    "        kd_loss = distillation_loss(student_logits, teacher_logits, student_overlap_6, teacher_overlap_6, temperature)\n",
    "        loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student6_model.parameters(), max_norm=1.0)\n",
    "        optimizer_student6.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(student6_train_loader)\n",
    "    \n",
    "    # Evaluate Student 6 accuracy\n",
    "    student6_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for fwd, rev, labels in student6_test_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student6_model(fwd, rev), dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    student6_test_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"[Student 6] Epoch {epoch}/{student6_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    if test_acc > best_student6_acc:\n",
    "        best_student6_acc = test_acc\n",
    "        best_student6_state = student6_model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "student6_model.load_state_dict(best_student6_state)\n",
    "for param in student6_model.parameters():\n",
    "    param.requires_grad = False\n",
    "student6_model.eval()\n",
    "print(f\"\\nHighest Student 6 Test Accuracy: {best_student6_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 5: Improved Student 5 Training with Enhanced KD\n",
    "For min_count >= 5, improve Student 5 training by tuning distillation hyperparameters (e.g., increasing the temperature, adjusting alpha to better balance KD and CE losses), adding additional training epochs or fine-tuning optimal learning rates, and potentially enhancing the model architecture (e.g., additional dropout adjustments or a modified final classification layer) to achieve >85% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 5: Improved Student 5 Training with Enhanced KD\n",
    "\n",
    "# Filter classes with min_count >= 5\n",
    "student5_data = filter_classes(raw_data, min_count=5)\n",
    "\n",
    "# Create train-test split\n",
    "student5_train_data, student5_test_data = create_train_test_split(student5_data)\n",
    "\n",
    "# Create paired data\n",
    "student5_paired_train = create_paired_data(student5_train_data)\n",
    "student5_paired_test = create_paired_data(student5_test_data)\n",
    "\n",
    "# Create datasets\n",
    "student5_dataset = TwoFastaKmerDataset(student5_paired_train, vocab, k=6)\n",
    "student5_test_dataset = TwoFastaKmerDataset(student5_paired_test, vocab, k=6)\n",
    "\n",
    "# Print class information\n",
    "print(\"\\nStudent 5 classes (min_count>=5):\")\n",
    "for cls in student5_dataset.label2idx:\n",
    "    print(cls)\n",
    "print(\"Number of Student 5 classes:\", student5_dataset.get_num_classes())\n",
    "\n",
    "# Create data loaders\n",
    "student5_train_loader = DataLoader(student5_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn_two)\n",
    "student5_test_loader = DataLoader(student5_test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn_two)\n",
    "\n",
    "# Model parameters\n",
    "num_classes_student5 = student5_dataset.get_num_classes()\n",
    "\n",
    "# Initialize model with enhanced architecture\n",
    "student5_model = TwoViTDeepSEAFusionDNAClassifierWithFC(\n",
    "    vocab_size=vocab_size,\n",
    "    num_classes=num_classes_student5,\n",
    "    embed_dim=128,\n",
    "    d_model=256,\n",
    "    num_conv_filters=(320, 480, 960),\n",
    "    conv_kernel_sizes=(8, 8, 8),\n",
    "    pool_kernel_sizes=(4, 4),\n",
    "    num_transformer_layers=3,  # Increased number of transformer layers\n",
    "    nhead=8,\n",
    "    dropout=0.3,  # Increased dropout\n",
    "    max_seq_len=1000\n",
    ").to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer_student5 = optim.AdamW(student5_model.parameters(), lr=5e-5, weight_decay=1e-4)  # Adjusted learning rate\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Distillation hyperparameters\n",
    "temperature = 6.0  # Increased temperature\n",
    "alpha = 0.7  # Adjusted alpha for better balance\n",
    "\n",
    "# Get overlapping indices between Student 6 (teacher) and Student 5 (student)\n",
    "teacher_overlap_5, student_overlap_5 = get_overlapping_indices(student6_dataset.label2idx, student5_dataset.label2idx)\n",
    "print(\"\\nOverlapping classes for KD between Student 6 and Student 5:\")\n",
    "for label in student6_dataset.label2idx:\n",
    "    if label in student5_dataset.label2idx:\n",
    "        print(label)\n",
    "print(\"Number of overlapping classes:\", len(teacher_overlap_5))\n",
    "\n",
    "# List to store test accuracy per epoch\n",
    "student5_acc_list = []\n",
    "\n",
    "# Training loop\n",
    "student5_epochs = 150  # Increased number of epochs\n",
    "best_student5_acc = 0.0\n",
    "best_student5_state = None\n",
    "\n",
    "print(\"\\nStarting Student 5 training with enhanced KD from Student 6...\")\n",
    "for epoch in range(1, student5_epochs + 1):\n",
    "    student5_model.train()\n",
    "    total_loss = 0.0\n",
    "    for fwd, rev, labels in student5_train_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        optimizer_student5.zero_grad()\n",
    "        student_logits = student5_model(fwd, rev)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = student6_model(fwd, rev)\n",
    "        ce_loss = criterion(student_logits, labels)\n",
    "        kd_loss = distillation_loss(student_logits, teacher_logits, student_overlap_5, teacher_overlap_5, temperature)\n",
    "        loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(student5_model.parameters(), max_norm=1.0)\n",
    "        optimizer_student5.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(student5_train_loader)\n",
    "    \n",
    "    # Evaluate Student 5 accuracy\n",
    "    student5_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for fwd, rev, labels in student5_test_loader:\n",
    "        fwd, rev, labels = fwd.to(device), rev.to(device), labels.to(device)\n",
    "        preds = torch.argmax(student5_model(fwd, rev), dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    student5_acc_list.append(test_acc)\n",
    "    \n",
    "    print(f\"[Student 5] Epoch {epoch}/{student5_epochs} | Loss: {avg_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "    \n",
    "    if test_acc > best_student5_acc:\n",
    "        best_student5_acc = test_acc\n",
    "        best_student5_state = student5_model.state_dict()\n",
    "\n",
    "# Load the best model state\n",
    "student5_model.load_state_dict(best_student5_state)\n",
    "for param in student5_model.parameters():\n",
    "    param.requires_grad = False\n",
    "student5_model.eval()\n",
    "print(f\"\\nHighest Student 5 Test Accuracy: {best_student5_acc:.2f}% at final epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Accuracy Curves\n",
    "Plot and compare test accuracy curves for Student 10 through Student 5 using matplotlib to visualize improvements and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy Curves\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(teacher_test_acc_list) + 1), teacher_test_acc_list, label=\"Student 10 Test Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, len(student8_test_acc_list) + 1), student8_test_acc_list, label=\"Student 8 Test Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, len(student7_test_acc_list) + 1), student7_test_acc_list, label=\"Student 7 Test Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, len(student6_test_acc_list) + 1), student6_test_acc_list, label=\"Student 6 Test Accuracy\", marker=\"o\")\n",
    "plt.plot(range(1, len(student5_acc_list) + 1), student5_acc_list, label=\"Student 5 Test Accuracy\", marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Test Accuracy Curves for All Stages\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
