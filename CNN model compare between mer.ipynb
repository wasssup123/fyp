{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare between different mer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k = 3, run 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 3\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# Helper functions\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'), nn.BatchNorm1d(32), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'), nn.BatchNorm1d(64), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'), nn.BatchNorm1d(128), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim//8)*128, 1024), nn.LeakyReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256), nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "# Step 1: Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Step 2: k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Step 2: Filter sequences by genus frequency\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Main Experiment Loop\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    data_by_class = defaultdict(list)\n",
    "    with open(filtered_vector_file, 'r') as f:\n",
    "        for line in f:\n",
    "            label = line.split('\\t')[0][1:]\n",
    "            data_by_class[label].append(line.strip())\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(len(train_vectors[0]), len(label_encoder.classes_)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K = 5, 10 times running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 5\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# Helper functions\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'), nn.BatchNorm1d(32), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'), nn.BatchNorm1d(64), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'), nn.BatchNorm1d(128), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim//8)*128, 1024), nn.LeakyReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256), nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "# Step 1: Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Step 2: k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Step 2: Filter sequences by genus frequency\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Main Experiment Loop\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    data_by_class = defaultdict(list)\n",
    "    with open(filtered_vector_file, 'r') as f:\n",
    "        for line in f:\n",
    "            label = line.split('\\t')[0][1:]\n",
    "            data_by_class[label].append(line.strip())\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(len(train_vectors[0]), len(label_encoder.classes_)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K = 6, 10 times running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constants\n",
    "k = 6\n",
    "input_fasta = 'data1/fungi_ITS_sorted90.fasta'\n",
    "cleaned_fasta = 'data1/fungi_ITS_cleaned.fasta'\n",
    "kmer_vector_file = 'data1/fungi_ITS_kmer_vector.txt'\n",
    "filtered_vector_file = 'data1/fungi_ITS_kmer_vector_filtered.txt'\n",
    "num_epochs = 100\n",
    "num_runs = 10\n",
    "\n",
    "# Helper functions\n",
    "def generate_kmers(k):\n",
    "    return [''.join(p) for p in product('ACGT', repeat=k)]\n",
    "\n",
    "def kmer_vector(seq, k, kmer_index):\n",
    "    vector = [0] * len(kmer_index)\n",
    "    for i in range(len(seq) - k + 1):\n",
    "        kmer = seq[i:i+k]\n",
    "        if kmer in kmer_index:\n",
    "            vector[kmer_index[kmer]] += 1\n",
    "    return vector\n",
    "\n",
    "class KmerDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), self.labels[idx]\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 7, padding='same'), nn.BatchNorm1d(32), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, 7, padding='same'), nn.BatchNorm1d(64), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, 7, padding='same'), nn.BatchNorm1d(128), nn.LeakyReLU(), nn.MaxPool1d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear((input_dim//8)*128, 1024), nn.LeakyReLU(), nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256), nn.LeakyReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.unsqueeze(1))\n",
    "\n",
    "# Step 1: Clean FASTA headers\n",
    "with open(input_fasta, 'r') as infile, open(cleaned_fasta, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            parts = line.strip().split()\n",
    "            outfile.write(f'>{parts[1]}\\n' if len(parts) > 1 else line)\n",
    "        else:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Step 2: k-mer vectorization\n",
    "kmers = generate_kmers(k)\n",
    "kmer_index = {kmer: idx for idx, kmer in enumerate(kmers)}\n",
    "\n",
    "with open(cleaned_fasta, 'r') as infile, open(kmer_vector_file, 'w') as outfile:\n",
    "    current_sequence, header = '', ''\n",
    "    for line in infile:\n",
    "        if line.startswith('>'):\n",
    "            if current_sequence:\n",
    "                vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "                outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "            header = line.strip()\n",
    "            current_sequence = ''\n",
    "        else:\n",
    "            current_sequence += line.strip()\n",
    "    if current_sequence:\n",
    "        vector = kmer_vector(current_sequence, k, kmer_index)\n",
    "        outfile.write(f\"{header}\\t{' '.join(map(str, vector))}\\n\")\n",
    "\n",
    "# Step 2: Filter sequences by genus frequency\n",
    "genera_count = defaultdict(int)\n",
    "with open(kmer_vector_file, 'r') as f:\n",
    "    for line in f:\n",
    "        genus = line.split()[0][1:]\n",
    "        genera_count[genus] += 1\n",
    "\n",
    "with open(kmer_vector_file, 'r') as infile, open(filtered_vector_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        genus = line.split()[0][1:]\n",
    "        if genera_count[genus] >= 10:\n",
    "            outfile.write(line)\n",
    "\n",
    "# Main Experiment Loop\n",
    "best_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    data_by_class = defaultdict(list)\n",
    "    with open(filtered_vector_file, 'r') as f:\n",
    "        for line in f:\n",
    "            label = line.split('\\t')[0][1:]\n",
    "            data_by_class[label].append(line.strip())\n",
    "\n",
    "    train_data, test_data = [], []\n",
    "    for samples in data_by_class.values():\n",
    "        test_sample = random.choice(samples)\n",
    "        test_data.append(test_sample)\n",
    "        train_data.extend(s for s in samples if s != test_sample)\n",
    "\n",
    "    train_vectors = [list(map(int, line.split('\\t')[1].split())) for line in train_data]\n",
    "    train_labels = [line.split('\\t')[0][1:] for line in train_data]\n",
    "\n",
    "    test_vectors = [list(map(int, line.split('\\t')[1].split())) for line in test_data]\n",
    "    test_labels = [line.split('\\t')[0][1:] for line in test_data]\n",
    "\n",
    "    label_encoder = LabelEncoder().fit(train_labels)\n",
    "    train_labels_encoded = label_encoder.transform(train_labels)\n",
    "    test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "    train_loader = DataLoader(KmerDataset(train_vectors, train_labels_encoded), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(KmerDataset(test_vectors, test_labels_encoded), batch_size=32)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = CNNModel(len(train_vectors[0]), len(label_encoder.classes_)).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "    max_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        if accuracy > max_accuracy:\n",
    "            max_accuracy = accuracy\n",
    "\n",
    "    best_accuracies.append(max_accuracy)\n",
    "\n",
    "avg_accuracy = sum(best_accuracies) / len(best_accuracies)\n",
    "print(f'Average Highest Accuracy over {num_runs} runs: {avg_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
