{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO\n",
    "\n",
    "def encode_and_save_fasta(fasta_file, data_file):\n",
    "    # Define the mapping for encoding\n",
    "    nucleotide_mapping = {'A': '1', 'C': '2', 'G': '3', 'T': '4'}\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "\n",
    "    # Extract sequences, encode, and save to the file\n",
    "    with open(data_file, 'w') as f:\n",
    "        for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "            label = record.description  # Get the label from the header\n",
    "            sequence = str(record.seq).upper()  # Ensure the sequence is uppercase\n",
    "\n",
    "            # Encode the sequence using the nucleotide mapping\n",
    "            encoded_sequence = ''.join([nucleotide_mapping.get(nuc, '0') for nuc in sequence])\n",
    "\n",
    "            # Write to the file in the format: class label encoded_data\n",
    "            f.write(f\"{label} {encoded_sequence}\\n\")\n",
    "\n",
    "# File paths\n",
    "fasta_file = \"data2/fungi_ITS_cleaned.fasta\"\n",
    "data_file = \"data2/encoded_data.txt\"\n",
    "\n",
    "# Call the function\n",
    "encode_and_save_fasta(fasta_file, data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# genera more than x samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "def filter_classes_with_more_than_5_samples(encoded_data_file, filtered_data_file):\n",
    "    # Step 1: Count occurrences of each class\n",
    "    class_counts = Counter()\n",
    "\n",
    "    # Read the encoded data and count class occurrences\n",
    "    with open(encoded_data_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            class_label = line.split()[0]  # Assume class label is the first part of each line\n",
    "            class_counts[class_label] += 1\n",
    "\n",
    "    # Step 2: Filter classes with more than 5 samples\n",
    "    valid_classes = {cls for cls, count in class_counts.items() if count >= 10}\n",
    "\n",
    "    # Step 3: Save filtered sequences to another file\n",
    "    with open(filtered_data_file, 'w') as filtered_f:\n",
    "        for line in lines:\n",
    "            class_label = line.split()[0]\n",
    "            # Write to the filtered file only if the class has more than 5 samples\n",
    "            if class_label in valid_classes:\n",
    "                filtered_f.write(line)\n",
    "\n",
    "\n",
    "encoded_data_file = \"data2/encoded_data.txt\"\n",
    "filtered_data_file = \"data2/filtered_encoded_data.txt\"\n",
    "filter_classes_with_more_than_5_samples(encoded_data_file, filtered_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_test_split(filtered_data_file, train_file, test_file):\n",
    "    # Dictionary to hold samples for each class\n",
    "    class_samples = defaultdict(list)\n",
    "\n",
    "    # Step 1: Read the filtered data file and organize samples by class\n",
    "    with open(filtered_data_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            class_label = line.split()[0]\n",
    "            class_samples[class_label].append(line)\n",
    "\n",
    "    # Step 2: Split into train and test\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    for class_label, samples in class_samples.items():\n",
    "        if len(samples) > 1:  # Only take a sample for test if there are multiple samples\n",
    "            test_sample = random.choice(samples)\n",
    "            test_samples.append(test_sample)\n",
    "            # Add the remaining samples to the train set\n",
    "            train_samples.extend([sample for sample in samples if sample != test_sample])\n",
    "        else:\n",
    "            # If only one sample, add it to the train set\n",
    "            train_samples.extend(samples)\n",
    "\n",
    "    # Step 3: Save train and test samples to respective files\n",
    "    with open(train_file, 'w') as train_f, open(test_file, 'w') as test_f:\n",
    "        train_f.writelines(train_samples)\n",
    "        test_f.writelines(test_samples)\n",
    "\n",
    "# File paths\n",
    "filtered_data_file = \"data2/filtered_encoded_data.txt\"\n",
    "train_file = \"data2/train_data.txt\"\n",
    "test_file = \"data2/test_data.txt\"\n",
    "\n",
    "# Perform train-test split\n",
    "train_test_split(filtered_data_file, train_file, test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        \"\"\"\n",
    "        Each line in data_file is like:\n",
    "            label  3411134123...\n",
    "        Where `label` is a string (like 'Trichoderma') and the rest is a sequence of digits.\n",
    "\n",
    "        This class:\n",
    "          - Maps each unique label to a numeric index\n",
    "          - Converts each sequence of digits into a list of integers\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "        self.label_mapping = {}\n",
    "        label_counter = 0\n",
    "        \n",
    "        with open(data_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue  # skip empty lines\n",
    "\n",
    "                # Split once on whitespace:\n",
    "                parts = line.split(maxsplit=1)\n",
    "                if len(parts) < 2:\n",
    "                    # If there's a malformed line that doesn't have both label and sequence\n",
    "                    continue\n",
    "\n",
    "                label_str, sequence_str = parts[0], parts[1]\n",
    "\n",
    "                # Map label to numeric index\n",
    "                if label_str not in self.label_mapping:\n",
    "                    self.label_mapping[label_str] = label_counter\n",
    "                    label_counter += 1\n",
    "\n",
    "                numeric_label = self.label_mapping[label_str]\n",
    "                # Convert each character in the sequence string to an integer\n",
    "                sequence = [int(x) for x in sequence_str]\n",
    "\n",
    "                self.samples.append((sequence, numeric_label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.samples[idx]\n",
    "        # Return torch tensors\n",
    "        # The sequence is variable-length, so we handle actual padding in a collate_fn\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2) Collate Function: Pads variable-length sequences\n",
    "# ---------------------------------------------------------------------\n",
    "def pad_collate(batch):\n",
    "    \"\"\"\n",
    "    Expects a batch of (sequence, label) pairs.\n",
    "    Finds the longest sequence, pads all others to that length with zeros.\n",
    "    Returns (padded_sequences, labels).\n",
    "    \"\"\"\n",
    "    sequences, labels = zip(*batch)\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "\n",
    "    padded_sequences = []\n",
    "    for seq in sequences:\n",
    "        # Pad up to max_length\n",
    "        length_diff = max_length - len(seq)\n",
    "        if length_diff > 0:\n",
    "            seq = torch.cat([seq, torch.zeros(length_diff)])\n",
    "        padded_sequences.append(seq)\n",
    "\n",
    "    # Stack into one tensor of shape (batch, max_length)\n",
    "    padded_sequences = torch.stack(padded_sequences, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3) Example: Create Datasets & DataLoaders\n",
    "# ---------------------------------------------------------------------\n",
    "train_file = \"data2/train_data.txt\"\n",
    "test_file  = \"data2/test_data.txt\"\n",
    "\n",
    "train_dataset = SequenceDataset(train_file)\n",
    "test_dataset  = SequenceDataset(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RRCNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1612\n",
      "Number of distinct classes: 81\n",
      "Label mapping: {'Cortinarius': 0, 'Aspergillus': 1, 'Inocybe': 2, 'Trichoderma': 3, 'Talaromyces': 4, 'Amanita': 5, 'Entoloma': 6, 'Orbilia': 7, 'Russula': 8, 'Lactarius': 9, 'Elsinoe': 10, 'Phyllosticta': 11, 'Mucor': 12, 'Candida': 13, 'Apiospora': 14, 'Exophiala': 15, 'Marasmius': 16, 'Hypoxylon': 17, 'Ogataea': 18, 'Tuber': 19, 'Pluteus': 20, 'Scolecobasidium': 21, 'Lactifluus': 22, 'Metschnikowia': 23, 'Leucoagaricus': 24, 'Gymnopus': 25, 'Xylodon': 26, 'Cladophialophora': 27, 'Tomentella': 28, 'Otidea': 29, 'Kazachstania': 30, 'Verrucaria': 31, 'Lipomyces': 32, 'Hygrophorus': 33, 'Geastrum': 34, 'Pseudosperma': 35, 'Boletus': 36, 'Cyberlindnera': 37, 'Absidia': 38, 'Sugiyamaella': 39, 'Wickerhamiella': 40, 'Mortierella': 41, 'Arthroderma': 42, 'Suhomyces': 43, 'Fomitiporia': 44, 'Tremella': 45, 'Xylaria': 46, 'Starmerella': 47, 'Trechispora': 48, 'Cyphellophora': 49, 'Mycena': 50, 'Wickerhamomyces': 51, 'Pichia': 52, 'Tricholoma': 53, 'Lepiota': 54, 'Blastobotrys': 55, 'Ramaria': 56, 'Hygrocybe': 57, 'Clavulina': 58, 'Phylloporus': 59, 'Coprinopsis': 60, 'Roussoella': 61, 'Inosperma': 62, 'Saccharomycopsis': 63, 'Hydnellum': 64, 'Spathaspora': 65, 'Crepidotus': 66, 'Hymenochaete': 67, 'Saturnispora': 68, 'Malassezia': 69, 'Bambusicola': 70, 'Perenniporia': 71, 'Clavaria': 72, 'Ophiocordyceps': 73, 'Colacogloea': 74, 'Backusella': 75, 'Lecanora': 76, 'Genea': 77, 'Cantharellus': 78, 'Scytinostroma': 79, 'Raffaelea': 80}\n",
      "Epoch [1/20] Train Loss: 4.2047, Train Acc: 10.67% | Test Loss: 4.5824, Test Acc: 1.23%\n",
      "Epoch [2/20] Train Loss: 4.0905, Train Acc: 12.34% | Test Loss: 4.6057, Test Acc: 1.23%\n",
      "Epoch [3/20] Train Loss: 4.0796, Train Acc: 12.34% | Test Loss: 4.6720, Test Acc: 1.23%\n",
      "Epoch [4/20] Train Loss: 4.0691, Train Acc: 12.34% | Test Loss: 4.5345, Test Acc: 1.23%\n",
      "Epoch [5/20] Train Loss: 4.0524, Train Acc: 12.41% | Test Loss: 4.5205, Test Acc: 1.23%\n",
      "Epoch [6/20] Train Loss: 4.0367, Train Acc: 12.34% | Test Loss: 4.4860, Test Acc: 1.23%\n",
      "Epoch [7/20] Train Loss: 3.9906, Train Acc: 12.47% | Test Loss: 4.5672, Test Acc: 1.23%\n",
      "Epoch [8/20] Train Loss: 3.9628, Train Acc: 12.53% | Test Loss: 4.4856, Test Acc: 1.23%\n",
      "Epoch [9/20] Train Loss: 3.9550, Train Acc: 12.47% | Test Loss: 4.3715, Test Acc: 1.23%\n",
      "Epoch [10/20] Train Loss: 3.9358, Train Acc: 12.41% | Test Loss: 4.6080, Test Acc: 1.23%\n",
      "Epoch [11/20] Train Loss: 3.9272, Train Acc: 12.47% | Test Loss: 4.4216, Test Acc: 1.23%\n",
      "Epoch [12/20] Train Loss: 3.9199, Train Acc: 12.47% | Test Loss: 4.3740, Test Acc: 1.23%\n",
      "Epoch [13/20] Train Loss: 3.9200, Train Acc: 12.78% | Test Loss: 4.4722, Test Acc: 1.23%\n",
      "Epoch [14/20] Train Loss: 3.9087, Train Acc: 12.53% | Test Loss: 4.4315, Test Acc: 1.23%\n",
      "Epoch [15/20] Train Loss: 3.8968, Train Acc: 12.97% | Test Loss: 4.3735, Test Acc: 1.23%\n",
      "Epoch [16/20] Train Loss: 3.8933, Train Acc: 12.59% | Test Loss: 4.4885, Test Acc: 1.23%\n",
      "Epoch [17/20] Train Loss: 3.8905, Train Acc: 12.59% | Test Loss: 4.3431, Test Acc: 1.23%\n",
      "Epoch [18/20] Train Loss: 3.8909, Train Acc: 12.78% | Test Loss: 4.5249, Test Acc: 1.23%\n",
      "Epoch [19/20] Train Loss: 3.8794, Train Acc: 12.53% | Test Loss: 4.4727, Test Acc: 1.23%\n",
      "Epoch [20/20] Train Loss: 3.8611, Train Acc: 12.90% | Test Loss: 4.3209, Test Acc: 1.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# IMPORTANT: If you have 69 distinct labels, label indices will be [0..68].\n",
    "# So we must ensure num_classes == len(train_dataset.label_mapping)\n",
    "num_classes = len(train_dataset.label_mapping)\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of distinct classes:\", num_classes)\n",
    "print(\"Label mapping:\", train_dataset.label_mapping)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_collate\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4) Define the RRCNN + LSTM for 1D Sequences\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "class RRCNNBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Recurrent Residual block for 1D sequences:\n",
    "      (Conv1d -> ReLU -> Conv1d -> ReLU), repeated `num_recurrent` times,\n",
    "      each time adding a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size=3, padding=1, num_recurrent=2):\n",
    "        super().__init__()\n",
    "        self.num_recurrent = num_recurrent\n",
    "        self.conv = nn.Conv1d(channels, channels, kernel_size=kernel_size, padding=padding)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, length)\n",
    "        out = x\n",
    "        for _ in range(self.num_recurrent):\n",
    "            residual = out\n",
    "            out = self.conv(out)   # conv1\n",
    "            out = self.relu(out)\n",
    "            out = self.conv(out)   # conv2\n",
    "            out = self.relu(out)\n",
    "            out = out + residual   # residual connection\n",
    "        return out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3) RRCNN for 1D data\n",
    "#    (Stack multiple RRCNN blocks + optional global pooling)\n",
    "# --------------------------------------------------\n",
    "class RRCNN1D(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels=4, \n",
    "                 hidden_channels=16, \n",
    "                 num_blocks=2, \n",
    "                 num_recurrent=6, \n",
    "                 kernel_size=3, \n",
    "                 dropout_p=0.2,\n",
    "                 use_global_pool=True):\n",
    "        \"\"\"\n",
    "        :param in_channels: e.g. 4 if you one-hot encode A,C,G,T as separate channels.\n",
    "                            But if you're just passing integer-coded [0..3], \n",
    "                            you usually set in_channels=1 and embed first.\n",
    "        :param hidden_channels: the number of feature maps in the CNN\n",
    "        :param num_blocks: how many RRCNNBlock1D to stack\n",
    "        :param num_recurrent: how many recurrent steps per block\n",
    "        :param kernel_size, padding: typical convolution params\n",
    "        :param use_global_pool: if True, do global average pool over sequence length\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.use_global_pool = use_global_pool\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        # Entry convolution to go from in_channels -> hidden_channels\n",
    "        self.entry_conv = nn.Conv1d(in_channels, hidden_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Stack multiple RRCNN blocks\n",
    "        blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            blocks.append(RRCNNBlock1D(hidden_channels, kernel_size=kernel_size, padding=1, num_recurrent=num_recurrent))\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "        # Exit convolution (optional)\n",
    "        self.exit_conv = nn.Conv1d(hidden_channels, hidden_channels, kernel_size=kernel_size, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch, channels, length)\n",
    "        \"\"\"\n",
    "        out = self.entry_conv(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.blocks(out)\n",
    "        out = self.exit_conv(out)\n",
    "        if self.use_global_pool:\n",
    "            # global average pooling over length dimension => (batch, hidden_channels)\n",
    "            out = out.mean(dim=-1)\n",
    "        return out\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4) RRCNN + LSTM Model for DNA classification\n",
    "# --------------------------------------------------\n",
    "class RRCNN_LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    RRCNN + LSTM + Dense classification layer\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size=4,            # e.g. A,C,G,T\n",
    "                 embed_dim=4,             # dimension to embed each nucleotide\n",
    "                 hidden_channels=16,\n",
    "                 rrcnn_blocks=4,\n",
    "                 rrcnn_recurrent=2,\n",
    "                 kernel_size=3,\n",
    "                 lstm_hidden_dim=64,\n",
    "                 lstm_layers=1,\n",
    "                 num_classes=2,\n",
    "                 use_global_pool=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # If your data is integer-coded [0..3], embed first => shape (batch, embed_dim, length)\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "\n",
    "        # RRCNN for feature extraction in 1D\n",
    "        # in_channels=embed_dim because the embedding dimension is your \"channel\" dimension\n",
    "        self.rrcnn = RRCNN1D(\n",
    "            in_channels=embed_dim,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_blocks=rrcnn_blocks,\n",
    "            num_recurrent=rrcnn_recurrent,\n",
    "            kernel_size=kernel_size,\n",
    "            use_global_pool=use_global_pool\n",
    "        )\n",
    "        \n",
    "        # LSTM: input_size = hidden_channels if use_global_pool else hidden_channels * ...\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_channels,\n",
    "            hidden_size=lstm_hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: shape (batch, seq_len) of integer-coded nucleotides\n",
    "        # 1) Embedding => shape (batch, seq_len, embed_dim)\n",
    "        x = self.embedding(x)  # => (batch, seq_len, embed_dim)\n",
    "\n",
    "        # 2) Transpose to (batch, embed_dim, seq_len) for Conv1D\n",
    "        x = x.transpose(1, 2)  # => (batch, embed_dim, seq_len)\n",
    "\n",
    "        # 3) RRCNN => if use_global_pool=True => (batch, hidden_channels)\n",
    "        #             else => (batch, hidden_channels, seq_len)\n",
    "        feats = self.rrcnn(x)\n",
    "        \n",
    "        # 4) If used global_pool, feats is (batch, hidden_channels). \n",
    "        #    We treat each entire sequence as one \"time-step\" => (batch, 1, hidden_channels).\n",
    "        feats = feats.unsqueeze(1)  # => (batch, 1, hidden_channels)\n",
    "\n",
    "        # 5) LSTM => output shape (batch, 1, lstm_hidden_dim)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(feats)\n",
    "        \n",
    "        # 6) Take last time-step => (batch, lstm_hidden_dim)\n",
    "        last_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # 7) Final classifier => (batch, num_classes)\n",
    "        logits = self.fc(last_out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5) Instantiate Model, Optimizer, Criterion\n",
    "# ---------------------------------------------------------------------\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "model = RRCNN1D_LSTM(\n",
    "    in_channels=1,\n",
    "    hidden_channels=16,\n",
    "    rrcnn_blocks=2,\n",
    "    rrcnn_recurrent=2,\n",
    "    kernel_size=3,\n",
    "    lstm_hidden_dim=64,\n",
    "    lstm_layers=1,\n",
    "    num_classes=num_classes,    # CRITICAL: matches number of distinct labels\n",
    "    use_global_pool=True\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 6) Training & Testing Loop\n",
    "# ---------------------------------------------------------------------\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    # -- Training --\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_correct_train = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(data)  # (batch, num_classes)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        batch_size = labels.size(0)\n",
    "        total_train_loss += loss.item() * batch_size\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct_train += (preds == labels).sum().item()\n",
    "        total_train_samples += batch_size\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_train_samples\n",
    "    train_accuracy = 100.0 * total_correct_train / total_train_samples\n",
    "\n",
    "    # -- Testing --\n",
    "    model.eval()\n",
    "    total_test_loss = 0.0\n",
    "    total_correct_test = 0\n",
    "    total_test_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            total_test_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct_test += (preds == labels).sum().item()\n",
    "            total_test_samples += batch_size\n",
    "\n",
    "    avg_test_loss = total_test_loss / total_test_samples\n",
    "    test_accuracy = 100.0 * total_correct_test / total_test_samples\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% | \"\n",
    "          f\"Test Loss: {avg_test_loss:.4f}, Test Acc: {test_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
